#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Progressive News Summarizer
è¯¥æ¨¡å—ç”¨äºè·å–ç‰¹å®šè‚¡ç¥¨çš„æ–°é—»ã€å…¬å‘Šã€ç ”æŠ¥å¹¶è¿›è¡Œæ€»ç»“
"""

import io
import os
import sys
import time
import json
import akshare as ak
import pandas as pd
import requests
from google import genai
from datetime import datetime, timedelta
import logging
import re
import urllib.request
from pathlib import Path
from dotenv import load_dotenv
import argparse
from google.genai.types import Tool, GenerateContentConfig, GoogleSearch
from gemini_utility import basic_convert  # å¯¼å…¥PDFè½¬Markdownå‡½æ•°
from google.genai import errors

# åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# è®¾ç½®æ—¥å¿—è®°å½•
def setup_logging(stock_code, stock_name, start_date=None, end_date=None):
    """
    è®¾ç½®æ—¥å¿—è®°å½•å™¨
    
    å‚æ•°:
        stock_code (str): è‚¡ç¥¨ä»£ç 
        stock_name (str): è‚¡ç¥¨åç§°
        start_date (str): å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYYMMDD
        end_date (str): ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYYMMDD
    """
    # åˆ›å»ºlogsç›®å½•
    logs_dir = Path("logs")
    logs_dir.mkdir(exist_ok=True)
    
    # åˆ›å»ºè‚¡ç¥¨ç‰¹å®šçš„æ—¥å¿—ç›®å½•
    stock_logs_dir = logs_dir / f"{stock_name}_{stock_code}"
    stock_logs_dir.mkdir(exist_ok=True)
    
    # æ„å»ºæ—¥å¿—æ–‡ä»¶å
    if start_date and end_date:
        log_filename = f"progressive_news_summarizer_{start_date}_{end_date}.log"
    else:
        current_date = datetime.now().strftime("%Y%m%d")
        log_filename = f"progressive_news_summarizer_{current_date}.log"
    
    log_file_path = stock_logs_dir / log_filename
    
    # è·å–loggerå®ä¾‹ - ä½¿ç”¨å”¯ä¸€åç§°ä»¥é¿å…é‡ç”¨
    logger_name = f"{__name__}.{stock_code}.{start_date or 'default'}.{end_date or 'default'}"
    logger = logging.getLogger(logger_name)
    
    # é‡ç½®loggerï¼Œæ¸…é™¤å·²æœ‰çš„å¤„ç†å™¨
    if logger.handlers:
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logger.setLevel(logging.INFO)
    
    # åˆ›å»ºå¤„ç†å™¨
    file_handler = logging.FileHandler(log_file_path, encoding='utf-8')
    console_handler = logging.StreamHandler(sys.stdout)
    
    # è®¾ç½®æ ¼å¼
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    # æ·»åŠ å¤„ç†å™¨
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    # ç¡®ä¿æ¶ˆæ¯ä¼ é€’åˆ°çˆ¶çº§logger
    logger.propagate = False
    
    logger.info(f"æ—¥å¿—æ–‡ä»¶ä¿å­˜åœ¨: {log_file_path}")
    return logger

# è·å–Gemini APIå¯†é’¥
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY", "")

# æ£€æŸ¥æ˜¯å¦ä¸ºæµ‹è¯•æ¨¡å¼
is_test_mode = len(sys.argv) > 1 and sys.argv[1] == "test"

# é…ç½®Gemini
if GOOGLE_API_KEY:
    # genai.configure(api_key=GOOGLE_API_KEY)
    pass
elif not is_test_mode:  # åªæœ‰åœ¨éæµ‹è¯•æ¨¡å¼ä¸‹æ‰å¯¹APIå¯†é’¥ç¼ºå¤±æŠ¥é”™
    logger.error("æœªè®¾ç½®GOOGLE_API_KEYç¯å¢ƒå˜é‡ï¼Œè¯·è®¾ç½®åå†è¿è¡Œã€‚")
    sys.exit(1)

class ProgressiveNewsSummarizer:
    """Progressive News Summarizerç±»ï¼Œç”¨äºæ”¶é›†å’Œæ€»ç»“è‚¡ç¥¨ç›¸å…³æ–°é—»"""
    
    def __init__(self, stock_code, stock_name, market="Aè‚¡", days=30, start_date=None, end_date=None):
        """
        åˆå§‹åŒ–Progressive News Summarizer
        
        å‚æ•°:
            stock_code (str): è‚¡ç¥¨ä»£ç 
            stock_name (str): è‚¡ç¥¨åç§°
            market (str): å¸‚åœºç±»å‹ï¼Œé»˜è®¤"Aè‚¡"ï¼Œå¯é€‰["Aè‚¡", "æ¸¯è‚¡"]
            days (int): è¦æŸ¥è¯¢çš„å¤©æ•°ï¼Œé»˜è®¤30å¤©
            start_date (str): å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ä¸º"YYYYMMDD"ï¼Œå¦‚æœæŒ‡å®šåˆ™ä¼˜å…ˆä½¿ç”¨ï¼Œè€Œä¸ä½¿ç”¨days
            end_date (str): ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸º"YYYYMMDD"ï¼Œå¦‚æœæŒ‡å®šåˆ™ä¼˜å…ˆä½¿ç”¨ï¼Œè€Œä¸ä½¿ç”¨days
        """
        self.stock_code = stock_code
        self.stock_name = stock_name
        self.market = market
        self.days = days
        # ä½¿ç”¨ç¯å¢ƒå˜é‡çš„APIå¯†é’¥ï¼Œå¹¶è®¾ç½®HTTPè¶…æ—¶é€‰é¡¹
        self.client = genai.Client(
            api_key=GOOGLE_API_KEY,
            http_options={"timeout": 600000}
        )
        
        # è®¾ç½®æ—¥å¿—è®°å½•å™¨
        self.logger = setup_logging(stock_code, stock_name, start_date, end_date)
        
        # åˆ›å»ºå­˜å‚¨ç›®å½•
        self.base_dir = Path(f"data/{self.stock_name}_{self.stock_code}")
        self.reports_dir = self.base_dir / "reports"  # ä¿ç•™ç›®å½•ä½†å·²ç¦ç”¨ç ”æŠ¥åŠŸèƒ½
        self.news_dir = self.base_dir / "news"
        self.announcements_dir = self.base_dir / "announcements"
        self.short_term_summary_dir = self.base_dir / "short_term_summary"
        self.long_term_summary_dir = self.base_dir / "long_term_summary"
        
        self._create_directories()
        
        # è®¾ç½®æ—¥æœŸèŒƒå›´
        self.end_date = datetime.now()
        self.start_date = self.end_date - timedelta(days=self.days)
        
        # å¦‚æœæŒ‡å®šäº†æ—¥æœŸèŒƒå›´ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨æŒ‡å®šæ—¥æœŸ
        if start_date and end_date:
            try:
                self.start_date = datetime.strptime(start_date, "%Y%m%d")
                self.end_date = datetime.strptime(end_date, "%Y%m%d")
                self.logger.info(f"ä½¿ç”¨æŒ‡å®šæ—¥æœŸèŒƒå›´: {start_date} è‡³ {end_date}")
            except ValueError as e:
                self.logger.error(f"æ—¥æœŸæ ¼å¼é”™è¯¯: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤æ—¥æœŸèŒƒå›´")
                
        self.start_date_str = self.start_date.strftime("%Y%m%d")
        self.end_date_str = self.end_date.strftime("%Y%m%d")
        
        # æ ‡è®°æ˜¯å¦ä¸ºæŒ‡å®šæ—¥æœŸèŒƒå›´æ¨¡å¼
        self.is_date_range_mode = bool(start_date and end_date)
        # æ ‡è®°æ˜¯å¦ä¸ºå¤šçŸ­æœŸæ€»ç»“åˆå¹¶æ¨¡å¼
        self.is_multiple_summary_mode = False
        
    def _create_directories(self):
        """åˆ›å»ºå­˜å‚¨ç›®å½•ç»“æ„"""
        self.announcement_summary_dir = self.base_dir / "announcement_summary"
        for directory in [self.reports_dir, self.news_dir, self.announcements_dir, 
                          self.short_term_summary_dir, self.long_term_summary_dir, 
                          self.announcement_summary_dir]:
            directory.mkdir(parents=True, exist_ok=True)
        self.logger.info(f"ä¸º{self.stock_name}({self.stock_code})åˆ›å»ºç›®å½•ç»“æ„")

    def collect_stock_research_reports(self):
        """
        æ”¶é›†ä¸ªè‚¡ç ”æŠ¥ - è¯¥åŠŸèƒ½å·²è¢«ç¦ç”¨
        è¿”å›ç©ºåˆ—è¡¨
        """
        self.logger.info(f"ç ”æŠ¥åŠŸèƒ½å·²è¢«ç¦ç”¨ï¼Œä¸å†æ”¶é›†ç ”æŠ¥")
        return []

    def collect_stock_announcements(self):
        """
        æ”¶é›†è‚¡ç¥¨å…¬å‘Š
        ä½¿ç”¨akshareè·å–å…¬å‘Šåˆ—è¡¨ï¼Œå¹¶ä¸‹è½½å¯¹åº”çš„PDFæ–‡ä»¶
        """
        self.logger.info(f"æ­£åœ¨è·å–{self.stock_name}({self.stock_code})çš„å…¬å‘Š...")
        try:
            announcement_files = []
            
            if self.market == "æ¸¯è‚¡":
                market = "æ¸¯è‚¡"
            elif self.market == "Aè‚¡":
                market = "æ²ªæ·±äº¬"
            else:
                self.logger.warning(f"æš‚ä¸æ”¯æŒ{self.market}çš„å…¬å‘Šè·å–")
                return []
            
            # ä½¿ç”¨å·¨æ½®èµ„è®¯æ¥å£è·å–å…¬å‘Š
            start_date = self.start_date.strftime("%Y%m%d")
            end_date = self.end_date.strftime("%Y%m%d")
            
            # ç¡®ä¿å­˜åœ¨chromedriver
            chromedriver_path = "C:\\Windows\\System32\\chromedriver.exe"
            if not os.path.exists(chromedriver_path):
                self.logger.error(f"æœªæ‰¾åˆ°chromedriverï¼Œè¯·ç¡®ä¿chromedriver.exeä½äºC:\\Windows\\System32\\ç›®å½•ä¸‹")
                return []
            
            # åˆ›å»ºç¼“å­˜ç›®å½•
            cache_dir = self.base_dir / "cache" / "announcements"
            cache_dir.mkdir(parents=True, exist_ok=True)

            # æ„å»ºç¼“å­˜æ–‡ä»¶è·¯å¾„
            cache_file = cache_dir / f"{self.stock_code}_{start_date}_{end_date}_å…¬å‘Š.csv"
            
            try:
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç¼“å­˜æ–‡ä»¶
                if cache_file.exists():
                    self.logger.info(f"ä½¿ç”¨ç¼“å­˜çš„å…¬å‘Šæ•°æ®: {cache_file}")
                    try:
                        df = pd.read_csv(cache_file, encoding='utf-8')
                        if df.empty:
                            self.logger.info(f"ç¼“å­˜æ–‡ä»¶ä¸ºç©ºï¼Œæœªæ‰¾åˆ°å…¬å‘Š")
                    except Exception as cache_err:
                        self.logger.error(f"è¯»å–ç¼“å­˜æ–‡ä»¶å‡ºé”™: {cache_err}ï¼Œå°†é‡æ–°è·å–æ•°æ®")
                        # ç¼“å­˜æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œé‡æ–°è·å–æ•°æ®
                        df = ak.stock_zh_a_disclosure_report_cninfo(
                            symbol=self.stock_code,
                            market=market,
                            tabList="å…¬å‘Š",
                            start_date=start_date,
                            end_date=end_date
                        )
                        
                        # ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶
                        if not df.empty:
                            df.to_csv(cache_file, encoding='utf-8', index=False)
                            self.logger.info(f"å·²å°†å…¬å‘Šæ•°æ®ä¿å­˜åˆ°ç¼“å­˜: {cache_file}")
                else:
                    # è·å–å…¬å‘Šæ•°æ®
                    self.logger.info(f"ä»APIè·å–å…¬å‘Šæ•°æ®...")
                    df = ak.stock_zh_a_disclosure_report_cninfo(
                        symbol=self.stock_code,
                        market=market,
                        category="",
                        start_date=start_date,
                        end_date=end_date
                    )
                    
                    # ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶
                    if not df.empty:
                        df.to_csv(cache_file, encoding='utf-8', index=False)
                        self.logger.info(f"å·²å°†å…¬å‘Šæ•°æ®ä¿å­˜åˆ°ç¼“å­˜: {cache_file}")
                    else:
                        # åˆ›å»ºç©ºæ–‡ä»¶ä½œä¸ºæ ‡è®°ï¼Œé¿å…ä¸‹æ¬¡ä»ç„¶è°ƒç”¨API
                        with open(cache_file, 'w', encoding='utf-8') as f:
                            f.write("")
                        self.logger.info(f"æœªæ‰¾åˆ°å…¬å‘Šï¼Œåˆ›å»ºç©ºç¼“å­˜æ–‡ä»¶")
                
                # å¤„ç†æŸ¥è¯¢ç»“æœ
                if df.empty:
                    self.logger.info(f"æœªæ‰¾åˆ°å…¬å‘Š")
                    
                # å¤„ç†æ¯æ¡å…¬å‘Š
                for _, row in df.iterrows():
                    title = re.sub(r'[\\/:*?"<>|]', '_', row['å…¬å‘Šæ ‡é¢˜'])
                    
                    # è¿‡æ»¤ä¸è´¢æŠ¥ç›¸å…³çš„å…¬å‘Š
                    # if any(keyword in row['å…¬å‘Šæ ‡é¢˜'] for keyword in [
                    #     "å¹´åº¦æŠ¥å‘Š", "åŠå¹´åº¦æŠ¥å‘Š", "å­£åº¦æŠ¥å‘Š", "è´¢åŠ¡æŠ¥å‘Š", "è´¢åŠ¡ä¼šè®¡æŠ¥å‘Š",
                    #     "è´¢åŠ¡æŠ¥è¡¨", "å®¡è®¡æŠ¥å‘Š", "å¹´æŠ¥", "åŠå¹´æŠ¥", "å­£æŠ¥", "è´¢æŠ¥",
                    #     "ä¸šç»©æŠ¥å‘Š", "å­£åº¦è´¢åŠ¡","å…¬å¸èµ„æ–™æŠ¥è¡¨", "ç¿Œæ—¥æŠ«éœ²æŠ¥è¡¨", "ç”³è¯·è¡¨æ ¼","ç”³è¯·ç‰ˆæœ¬", "è†è®¯å¾Œèµ„æ–™é›†"
                    # ]):
                    #     self.logger.info(f"è·³è¿‡è´¢æŠ¥ç›¸å…³å…¬å‘Š: {row['å…¬å‘Šæ ‡é¢˜']}")
                    #     continue

                    # è¿‡æ»¤ä¸è´¢æŠ¥ç›¸å…³çš„å…¬å‘Š
                    if any(keyword in row['å…¬å‘Šæ ‡é¢˜'] for keyword in [
                        "ç¿Œæ—¥æŠ«éœ²æŠ¥è¡¨", "ç”³è¯·è¡¨æ ¼","ç”³è¯·ç‰ˆæœ¬", "è†è®¯å¾Œèµ„æ–™é›†", "æ³•å¾‹æ„è§ä¹¦", " æ ¸æŸ¥æ„è§", "è‚¡ä¸œå¤§ä¼šçš„é€šçŸ¥", "è‚¡ä¸œå¤§ä¼šé€šçŸ¥"
                    ]):
                        self.logger.info(f"è·³è¿‡è´¢æŠ¥ç›¸å…³å…¬å‘Š: {row['å…¬å‘Šæ ‡é¢˜']}")
                        continue
                    
                    # å¤„ç†æ—¥æœŸï¼Œç§»é™¤å¯èƒ½å­˜åœ¨çš„æ—¶é—´éƒ¨åˆ†
                    announcement_date = row['å…¬å‘Šæ—¶é—´']
                    if ' ' in announcement_date:  # æ£€æŸ¥æ˜¯å¦åŒ…å«æ—¶é—´
                        announcement_date = announcement_date.split(' ')[0]  # åªä¿ç•™æ—¥æœŸéƒ¨åˆ†
                    date = announcement_date.replace('-', '')
                    
                    link = row['å…¬å‘Šé“¾æ¥']
                    
                    # è®¾ç½®æ–‡ä»¶è·¯å¾„
                    pdf_file_path = self.announcements_dir / f"{date}_{title}.pdf"
                    
                    # å¦‚æœPDFå·²å­˜åœ¨ï¼Œæ£€æŸ¥å…¶å¤§å°å¹¶è·³è¿‡ä¸‹è½½
                    if pdf_file_path.exists():
                        # æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦è¶…è¿‡3MB
                        file_size = os.path.getsize(pdf_file_path)
                        if file_size > 3 * 1024 * 1024:  # 3MB = 3 * 1024 * 1024 bytes
                            self.logger.info(f"è·³è¿‡å¤§æ–‡ä»¶å…¬å‘Š(å¤§å°: {file_size/1024/1024:.2f}MB): {pdf_file_path}")
                            continue
                        
                        self.logger.info(f"å…¬å‘ŠPDFå·²å­˜åœ¨: {pdf_file_path}")
                        announcement_files.append(str(pdf_file_path))
                        continue
                    
                    # å…ˆåˆ›å»ºä¸´æ—¶TXTæ–‡ä»¶è®°å½•å…¬å‘Šä¿¡æ¯
                    txt_file_path = self.announcements_dir / f"{date}_{title}.txt"
                    try:
                        with open(txt_file_path, 'w', encoding='utf-8') as f:
                            f.write(f"æ ‡é¢˜: {title}\n")
                            f.write(f"æ—¥æœŸ: {row['å…¬å‘Šæ—¶é—´']}\n")
                            f.write(f"é“¾æ¥: {link}\n\n")
                        self.logger.info(f"å·²åˆ›å»ºä¸´æ—¶è®°å½•: {txt_file_path}")
                    except Exception as txt_err:
                        self.logger.error(f"åˆ›å»ºä¸´æ—¶è®°å½•å¤±è´¥: {txt_err}")
                    
                    # ä¸‹è½½PDFæ–‡ä»¶
                    self.logger.info(f"ä¸‹è½½å…¬å‘ŠPDF: {title}")
                    try:
                        # å°è¯•ä¸‹è½½PDF
                        self._download_announcement_pdf(link, pdf_file_path)
                        
                        # æ£€æŸ¥ä¸‹è½½æ–‡ä»¶çš„å¤§å°æ˜¯å¦è¶…è¿‡3MB
                        if os.path.exists(pdf_file_path):
                            file_size = os.path.getsize(pdf_file_path)
                            if file_size > 3 * 1024 * 1024:  # 3MB = 3 * 1024 * 1024 bytes
                                self.logger.info(f"åˆ é™¤å¤§æ–‡ä»¶å…¬å‘Š(å¤§å°: {file_size/1024/1024:.2f}MB): {pdf_file_path}")
                                os.remove(pdf_file_path)
                                continue
                        
                        self.logger.info(f"PDFä¸‹è½½æˆåŠŸ: {pdf_file_path}")
                        announcement_files.append(str(pdf_file_path))
                        
                        # åˆ é™¤ä¸´æ—¶TXTæ–‡ä»¶
                        if os.path.exists(txt_file_path):
                            try:
                                os.remove(txt_file_path)
                                self.logger.info(f"å·²åˆ é™¤ä¸´æ—¶è®°å½•: {txt_file_path}")
                            except Exception as rm_err:
                                self.logger.error(f"åˆ é™¤ä¸´æ—¶è®°å½•å¤±è´¥: {rm_err}")
                    except Exception as pdf_err:
                        self.logger.error(f"ä¸‹è½½PDFå¤±è´¥: {title}, é”™è¯¯: {pdf_err}")
                        # ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¿ç•™TXTæ–‡ä»¶ä½œä¸ºè®°å½•
                        if os.path.exists(txt_file_path):
                            announcement_files.append(str(txt_file_path))
                
                # é˜²æ­¢è¯·æ±‚è¿‡å¿«
                time.sleep(1)
                
            except Exception as cat_err:
                self.logger.error(f"è·å–å…¬å‘Šå¤±è´¥: {cat_err}")
            
            # æ¸…ç†å¤šä½™çš„TXTæ–‡ä»¶
            self._clean_announcement_txt_files(announcement_files)
            
            # è¿”å›ç»“æœ
            pdf_files = [f for f in announcement_files if f.lower().endswith('.pdf')]
            self.logger.info(f"å…±æ”¶é›†{len(pdf_files)}ä¸ªPDFå…¬å‘Š")
            return pdf_files

                
        except Exception as e:
            self.logger.error(f"è·å–å…¬å‘Šæ—¶å‡ºé”™: {e}")
            return []

    def _clean_announcement_txt_files(self, announcement_files):
        """
        æ¸…ç†announcementsç›®å½•ä¸­çš„TXTæ–‡ä»¶
        åªä¿ç•™æ²¡æœ‰å¯¹åº”PDFæ–‡ä»¶çš„TXTè®°å½•
        
        Args:
            announcement_files: å·²ä¸‹è½½çš„å…¬å‘Šæ–‡ä»¶åˆ—è¡¨
        """
        try:
            # è·å–æ‰€æœ‰å·²ä¸‹è½½çš„PDFæ–‡ä»¶å
            pdf_filenames = [os.path.basename(f) for f in announcement_files if f.lower().endswith('.pdf')]
            
            # éå†ç›®å½•ä¸­çš„æ‰€æœ‰TXTæ–‡ä»¶
            for file in os.listdir(self.announcements_dir):
                if not file.lower().endswith('.txt'):
                    continue
                    
                txt_path = os.path.join(self.announcements_dir, file)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„PDFæ–‡ä»¶
                pdf_name = file.replace('.txt', '.pdf')
                if pdf_name in pdf_filenames:
                    # å¦‚æœæœ‰å¯¹åº”çš„PDFï¼Œåˆ é™¤TXT
                    try:
                        os.remove(txt_path)
                        self.logger.info(f"æ¸…ç†: åˆ é™¤ä¸´æ—¶TXTæ–‡ä»¶ {txt_path}")
                    except Exception as e:
                        self.logger.error(f"åˆ é™¤TXTæ–‡ä»¶å¤±è´¥: {e}")
                        
            self.logger.info("ä¸´æ—¶TXTæ–‡ä»¶æ¸…ç†å®Œæˆ")
        except Exception as e:
            self.logger.error(f"æ¸…ç†TXTæ–‡ä»¶æ—¶å‡ºé”™: {e}")

    
    def _download_announcement_pdf(self, url, output_path):
        """ä¸‹è½½å·¨æ½®èµ„è®¯ç½‘çš„å…¬å‘ŠPDF
        
        Args:
            url: å…¬å‘Šé¡µé¢URL
            output_path: PDFä¿å­˜è·¯å¾„
        """
        import urllib.request
        from bs4 import BeautifulSoup
        import requests
        import time
        import os
        
        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # å°è¯•ç›´æ¥è·å–PDFé“¾æ¥
        success = False
        
        try:
            # 1. å°è¯•é€šè¿‡è§„åˆ™æ¨å¯¼PDF URL
            # ç¤ºä¾‹ï¼šä»http://www.cninfo.com.cn/new/disclosure/detail?stockCode=002415&announcementId=1221835007
            # æ¨å¯¼ä¸ºhttp://static.cninfo.com.cn/finalpage/2024-11-26/1221835007.PDF
            
            # ä»URLä¸­æå–announcementId
            import re
            announcement_id = re.search(r"announcementId=(\d+)", url)
            announcement_time = re.search(r"announcementTime=(\d{4}-\d{2}-\d{2})", url)
            
            if announcement_id and announcement_time:
                ann_id = announcement_id.group(1)
                ann_date = announcement_time.group(1).replace("-", "")
                
                # æ„å»ºé™æ€PDFé“¾æ¥
                pdf_url = f"http://static.cninfo.com.cn/finalpage/{announcement_time.group(1)}/{ann_id}.PDF"
                self.logger.info(f"é€šè¿‡è§„åˆ™æ¨å¯¼å¾—åˆ°PDFé“¾æ¥: {pdf_url}")
                
                # è®¾ç½®è¯·æ±‚å¤´
                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                    "Referer": url
                }
                
                # åˆ›å»ºè¯·æ±‚å¯¹è±¡
                req = urllib.request.Request(pdf_url, headers=headers)
                
                try:
                    # ä¸‹è½½æ–‡ä»¶
                    self.logger.info(f"å¼€å§‹ä¸‹è½½PDF: {pdf_url}")
                    with urllib.request.urlopen(req) as response, open(output_path, "wb") as out_file:
                        out_file.write(response.read())
                    
                    # éªŒè¯ä¸‹è½½ç»“æœ
                    if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
                        self.logger.info(f"ä¸‹è½½æˆåŠŸï¼Œæ–‡ä»¶å¤§å°: {os.path.getsize(output_path)} å­—èŠ‚")
                        success = True
                        return True
                except Exception as direct_err:
                    self.logger.error(f"ç›´æ¥ä¸‹è½½PDFå¤±è´¥: {direct_err}")
            
            # 2. å¦‚æœæ¨å¯¼å¤±è´¥ï¼Œå°è¯•è§£æé¡µé¢è·å–PDFé“¾æ¥
            if not success:
                self.logger.info("å°è¯•è§£æé¡µé¢è·å–PDFé“¾æ¥")
                
                response = requests.get(url, headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
                })
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, "html.parser")
                    
                    # å°è¯•æŸ¥æ‰¾é¡µé¢ä¸­çš„PDFé“¾æ¥
                    pdf_links = []
                    for a_tag in soup.find_all("a", href=True):
                        if ".pdf" in a_tag["href"].lower():
                            pdf_links.append(a_tag["href"])
                    
                    # ä¹ŸæŸ¥æ‰¾embedæ ‡ç­¾çš„srcå±æ€§
                    for embed_tag in soup.find_all("embed"):
                        src = embed_tag.get("src", "")
                        if ".pdf" in src.lower():
                            pdf_links.append(src)
                    
                    self.logger.info(f"é€šè¿‡è§£æé¡µé¢æ‰¾åˆ°{len(pdf_links)}ä¸ªPDFé“¾æ¥")
                    
                    if pdf_links:
                        # å¤„ç†ç›¸å¯¹URL
                        pdf_url = pdf_links[0]
                        if pdf_url.startswith("/"):
                            pdf_url = f"http://www.cninfo.com.cn{pdf_url}"
                        elif not pdf_url.startswith("http"):
                            pdf_url = f"http://www.cninfo.com.cn/{pdf_url}"
                        
                        self.logger.info(f"å°è¯•ä¸‹è½½PDFé“¾æ¥: {pdf_url}")
                        pdf_response = requests.get(pdf_url, headers={
                            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                            "Referer": url
                        })
                        
                        if pdf_response.status_code == 200:
                            with open(output_path, "wb") as f:
                                f.write(pdf_response.content)
                            self.logger.info(f"é€šè¿‡ç›´æ¥ä¸‹è½½é“¾æ¥æˆåŠŸä¿å­˜PDF: {output_path}")
                            success = True
                            return True
            
            # æ¸…ç†æ ¹ç›®å½•æ®‹ç•™çš„.crdownloadæ–‡ä»¶
            try:
                root_dir = os.getcwd()
                for file in os.listdir(root_dir):
                    if file.lower().endswith(".crdownload"):
                        file_path = os.path.join(root_dir, file)
                        try:
                            os.remove(file_path)
                            self.logger.info(f"å·²åˆ é™¤æ®‹ç•™çš„ä¸‹è½½æ–‡ä»¶: {file_path}")
                        except Exception as e:
                            self.logger.error(f"åˆ é™¤æ®‹ç•™æ–‡ä»¶å¤±è´¥: {e}")
            except Exception as e:
                self.logger.error(f"æ¸…ç†æ®‹ç•™ä¸‹è½½æ–‡ä»¶å‡ºé”™: {e}")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æˆåŠŸä¸‹è½½
            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
                self.logger.info(f"PDFæ–‡ä»¶æˆåŠŸä¸‹è½½ï¼Œå¤§å°: {os.path.getsize(output_path)} å­—èŠ‚")
                return True
            else:
                raise Exception("ä¸‹è½½PDFå¤±è´¥")
                
        except Exception as e:
            self.logger.error(f"ä¸‹è½½PDFå‡ºé”™: {e}")
            raise
    
    def search_stock_news_with_gemini(self):
        """
        ä½¿ç”¨Geminiçš„Googleæœç´¢åŠŸèƒ½è·å–è‚¡ç¥¨ç›¸å…³æ–°é—»
        å¦‚æœå·²å­˜åœ¨ç›¸åŒæ—¥æœŸèŒƒå›´çš„æ–°é—»æ–‡ä»¶ï¼Œåˆ™ç›´æ¥å¤ç”¨
        """

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ—¥æœŸèŒƒå›´çš„æ–°é—»æ–‡ä»¶
        news_file = self.news_dir / f"{self.stock_code}_news_{self.start_date_str}_{self.end_date_str}.md"
        if news_file.exists():
            self.logger.info(f"å·²å­˜åœ¨æ—¥æœŸèŒƒå›´å†…çš„æ–°é—»æ–‡ä»¶ï¼Œç›´æ¥å¤ç”¨: {news_file}")
            return str(news_file)

        MODEL = "gemini-2.5-pro"
        self.logger.info(f"æ­£åœ¨ä½¿ç”¨Geminiæœç´¢{self.stock_name}({self.stock_code})çš„æ–°é—»...")

        google_search_tool = Tool(
            google_search=GoogleSearch()
        )

        # æ„å»ºæœç´¢æç¤º
        start_date_formatted = self.start_date.strftime("%Yå¹´%mæœˆ%dæ—¥")
        end_date_formatted = self.end_date.strftime("%Yå¹´%mæœˆ%dæ—¥")
        
        prompt = f"""
                    è¯·å¸®æˆ‘æœé›†å¹¶æ±‡æ€»å…³äº{self.stock_name}åœ¨{start_date_formatted}è‡³{end_date_formatted}æœŸé—´çš„æ‰€æœ‰å¯èƒ½å½±å“å…¬å¸è‚¡ä»·çš„æ–°é—»å’Œä¼ é—»ï¼Œä¸¥æ ¼æŒ‰ä»¥ä¸‹è¦æ±‚æ‰§è¡Œï¼š

                    ã€æ•°æ®æ¥æºè¦æ±‚ã€‘
                    è¦†ç›–ä»¥ä¸‹æ¸ é“ï¼šä¸»æµè´¢ç»åª’ä½“ã€è¡Œä¸šå‚ç›´å¹³å°ã€ç¤¾äº¤åª’ä½“ï¼ˆè‚¡å§/é›ªçƒ/å¾®åšï¼‰ã€ç›‘ç®¡æ–‡ä»¶ã€å…¬å¸å…¬å‘Šã€ä¾›åº”é“¾ä¿¡æºã€‚å¯¹äºç¤¾äº¤åª’ä½“ä¼ é—»ï¼Œéœ€æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ä¹‹ä¸€æ‰æ”¶å½•ï¼š
                    - ç›¸å…³è¯é¢˜é˜…è¯»é‡ï¼10ä¸‡æ¬¡ 
                    - è¢«5ä¸ªä»¥ä¸Šè´¢ç»é¢†åŸŸå¤§Vè½¬å‘
                    - ä¸è¿‘æœŸè‚¡ä»·å¼‚åŠ¨æ—¶é—´å»åˆ

                    ã€æ ¸å¿ƒä¿¡æ¯ç»´åº¦ã€‘
                    æŒ‰é¡ºåºå¤„ç†ä»¥ä¸‹å†…å®¹ï¼ˆæ— ç»“æœåˆ™è·³è¿‡è¯¥éƒ¨åˆ†ï¼‰ï¼š
                    1. é‡å¤§åˆåŒ/è®¢å•å˜åŠ¨ï¼ˆéœ€å¯¹æ¯”åˆåŒé‡‘é¢ä¸ä¸Šå­£åº¦è¥æ”¶ï¼‰
                    2. äº§å“ä¸æœåŠ¡åŠ¨æ€ï¼ˆæ³¨æ˜æ˜¯å¦çªç ´ç°æœ‰æŠ€æœ¯è·¯çº¿ï¼‰
                    3. ä¸šåŠ¡å¢é•¿ä¸æ‰©å¼ ï¼ˆåŒºåˆ†æœ‰æœºå¢é•¿ä¸å¹¶è´­ï¼‰
                    4. æ²»ç†ä¸äººäº‹ï¼ˆé«˜ç®¡å˜åŠ¨éœ€å¯¹æ¯”ä»»æœŸå‰©ä½™æ—¶é—´ï¼‰
                    5. æ³•å¾‹ç›‘ç®¡äº‹ä»¶ï¼ˆæ ‡æ³¨å¤„ç½šé‡‘é¢/æ•´æ”¹æˆæœ¬é¢„ä¼°ï¼‰
                    6. èµ„æœ¬è¿ä½œï¼ˆå¦‚è‚¡ç¥¨å‘è¡Œä¸å›è´­ã€å¤§è‚¡ä¸œå¥—ç°ã€å¤§è‚¡ä¸œå˜æ›´ã€ä¿¡ç”¨è¯„çº§è°ƒæ•´ç­‰ï¼Œéœ€å…³æ³¨å¤§è‚¡ä¸œè¡Œä¸ºä¸€è‡´æ€§ï¼‰
                    7. çªå‘äº‹ä»¶ï¼ˆæ ‡æ³¨æ˜¯å¦æ¶‰åŠæ ¸å¿ƒä¸šåŠ¡ï¼‰
                    8. å›½é™…åˆ¶è£/æ”¿ç­–ï¼ˆåŒºåˆ†ç›´æ¥å½±å“ä¸æƒ…ç»ªå½±å“ï¼‰
                    9. è¡Œä¸šç”Ÿæ€å˜åŒ–ï¼ˆæŠ€æœ¯çªç ´/æ›¿ä»£å“å¨èƒï¼‰
                    10. æ”¿ç­–åŠ¨æ€ï¼ˆè‰æ¡ˆ/è¯•ç‚¹/å›½é™…è”ç›Ÿï¼‰

                    ã€ä¼ é—»å¤„ç†è§„åˆ™ã€‘
                    å¯¹éå®˜æ–¹æ¶ˆæ¯å¿…é¡»ï¼š
                    - æ·»åŠ ã€å¾…æ ¸å®ã€‘å‰ç¼€ 
                    - æ ‡æ³¨ä¼ æ’­è·¯å¾„ï¼ˆå¦‚ï¼šå¾®åšâ†’é›ªçƒâ†’è´¢ç»åª’ä½“ï¼‰
                    - è®°å½•æœ€æ—©å‡ºç°æ—¶é—´ä¸ä¼ æ’­å³°å€¼æ—¶é—´
                    - æ³¨æ˜ï¼š"è¯¥ä¿¡æ¯å°šæœªè¯å®ï¼Œè¯·è°¨æ…å‚è€ƒ"

                    ã€è´¢åŠ¡å…³è”è§„åˆ™ã€‘
                    å½“æ¶‰åŠä»¥ä¸‹æ–°é—»ç±»å‹æ—¶ï¼Œå…³è”æœ€è¿‘å­£åº¦è´¢æŠ¥æ•°æ®ï¼š
                    â–  æŠ•èµ„/å¹¶è´­ â†’ å¯¹æ¯”ç°é‡‘æŒæœ‰é‡ä¸æŠ•èµ„æ€»é¢
                    â–  ä»·æ ¼è°ƒæ•´ â†’ æ³¨æ˜å†å²æ¯›åˆ©ç‡æ³¢åŠ¨èŒƒå›´
                    â–  è¯‰è®¼/å¤„ç½š â†’ è®¡ç®—å å‡€åˆ©æ¶¦æ¯”ä¾‹
                    ï¼ˆå…·ä½“æ•°å€¼è®¡ç®—ç”±å…¶ä»–æ¨¡å—å¤„ç†ï¼‰

                    ã€å¯ä¿¡åº¦æ ‡æ³¨ç³»ç»Ÿã€‘
                    æ¯æ¡ä¿¡æ¯å¤´éƒ¨æ·»åŠ ï¼š
                    âœ… å®˜æ–¹è¯å® - å…¬å¸/ç›‘ç®¡æ­£å¼æ–‡ä»¶
                    ğŸ…°ï¸ å¤šæ–¹å°è¯ - â‰¥3å®¶æƒå¨åª’ä½“ç‹¬ç«‹æŠ¥é“
                    ğŸ…±ï¸ å•æ–¹ä¿¡æº - æœªè·å…¬å¸å›åº”çš„åª’ä½“æŠ¥é“
                    âš ï¸ ä¼ é—»é¢„è­¦ - ç¤¾äº¤å¹³å°ä¼ æ’­æœªéªŒè¯

                    ã€å‹åˆ¶ä¿¡æ¯ç›‘æµ‹ã€‘ 
                    é‡ç‚¹æ•æ‰ï¼š
                    â€¢ çªå‘å¯†é›†è´Ÿé¢åå¿«é€Ÿåˆ é™¤ï¼ˆè®°å½•ç½‘é¡µå­˜æ¡£é“¾æ¥ï¼‰
                    â€¢ é«˜ç®¡å¼‚å¸¸ç¦»èŒï¼ˆä»»æœŸå†…+æ— ç»§ä»»è€…+æœªå‘æ„Ÿè°¢ä¿¡ï¼‰
                    â€¢ ä¾›åº”é“¾å¼‚åŠ¨ï¼ˆå¤šä¸ªåˆä½œæ–¹åŒæ—¶å˜æ›´ä¿¡æ¯ï¼‰
                    â€¢ è´¢æŠ¥å…³é”®æ¨¡ç³Šè¡¨è¿°ï¼ˆå¯¹æ¯”å¾€æœŸåŒç±»è¡¨è¿°å˜åŒ–ï¼‰

                    ã€è¾“å‡ºæ ¼å¼ã€‘
                    â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡
                    æ ‡é¢˜ï¼šã€å¯ä¿¡åº¦å›¾æ ‡ã€‘æ ‡é¢˜æ–‡æœ¬
                    æ—¥æœŸï¼šYYYY-MM-DD HH:MM
                    æ¥æºï¼šåª’ä½“åç§°/ç¤¾äº¤å¹³å°+ä¼ æ’­çƒ­åº¦
                    è´¢åŠ¡å…³è”ï¼šå¯èƒ½å½±å“çš„è´¢æŠ¥ç§‘ç›®/æŒ‡æ ‡
                    æ‘˜è¦ï¼šäº‹ä»¶æ ¸å¿ƒäº‹å®+æ½œåœ¨å½±å“é€»è¾‘
                    å‹åˆ¶è¿¹è±¡ï¼šï¼»è‹¥æœ‰åˆ™å¡«ï¼½åˆ é™¤æ—¶é—´/é™æµèŒƒå›´
                    æ—¶é—´è½´æ ‡è®°ï¼šï¼»äº‹ä»¶é˜¶æ®µï¼½å‘é…µæœŸ/æ¶ˆé€€æœŸ/åå¤æœŸ
                    â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡â‰¡

                    ã€ç‰¹åˆ«è§„èŒƒã€‘
                    1. ä¸æ”¶å½•æ˜æ˜¾è¯½è°¤æˆ–è¿æ³•ä¿¡æ¯
                    2. åŒä¸€äº‹ä»¶å¤šä¿¡æºæŠ¥é“éœ€åˆå¹¶å¤„ç†
                    3. æ¶‰åŠæ”¿ç­–è‰æ¡ˆéœ€æ³¨æ˜ç«‹æ³•æ¦‚ç‡è¯„ä¼°
                    4. æ¯é¡µæœ€å¤šå‘ˆç°25æ¡å…³é”®ä¿¡æ¯
                    4. ç”¨ä¸­æ–‡è¾“å‡ºï¼Œæ— éœ€è§£é‡Šåˆ†æé€»è¾‘
                """
        
        # è®¾ç½®é‡è¯•å‚æ•°
        max_retries = 5
        retry_delay = 5  # åˆå§‹å»¶è¿Ÿ5ç§’
        
        # é‡è¯•å¾ªç¯
        for attempt in range(1, max_retries + 1):
            try:
                self.logger.info(f"å°è¯•æœç´¢æ–°é—» (å°è¯• {attempt}/{max_retries})...")
                
                # å‘é€è¯·æ±‚å¹¶ç­‰å¾…å“åº”
                response_text = ""
                for chunk in self.client.models.generate_content_stream(
                    model=MODEL,
                    contents=prompt,
                    config=GenerateContentConfig(
                        tools=[google_search_tool],
                        http_options = {"timeout": 600000},
                    ),
                ):
                    if chunk.text:
                        response_text += chunk.text

                # ä¿å­˜æœç´¢ç»“æœ
                with open(news_file, "w", encoding="utf-8") as f:
                    f.write(f"# {self.stock_name}({self.stock_code}) æ–°é—»æŠ¥é“\n\n")
                    f.write(f"æ—¶é—´èŒƒå›´: {start_date_formatted} è‡³ {end_date_formatted}\n\n")
                    f.write(response_text)
                    
                self.logger.info(f"æ–°é—»æœç´¢ç»“æœå·²ä¿å­˜è‡³: {news_file}")
                return str(news_file)
                
            except Exception as e:
                error_msg = str(e)
                self.logger.warning(f"æœç´¢æ–°é—»æ—¶å‡ºé”™ (å°è¯• {attempt}/{max_retries}): {error_msg}")
                
                # å¦‚æœå·²ç»æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
                if attempt == max_retries:
                    self.logger.error(f"ä½¿ç”¨Geminiæœç´¢æ–°é—»å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§å°è¯•æ¬¡æ•° ({max_retries}æ¬¡)")
                    raise
                
                # æŒ‡æ•°é€€é¿ï¼šæ¯æ¬¡å¤±è´¥åå¢åŠ ç­‰å¾…æ—¶é—´
                wait_time = retry_delay * (2 ** (attempt - 1))
                self.logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)

    def generate_progressive_summary(self, news_content, announcement_pdfs=None, research_pdfs=None):
        """
        ç”ŸæˆçŸ­æœŸæ¸è¿›å¼æ€»ç»“(Ns,Ï„)
        
        å‚æ•°:
            news_content: æ–°é—»å†…å®¹æ–‡æœ¬
            announcement_pdfs: å…¬å‘ŠPDFæ–‡ä»¶è·¯å¾„åˆ—è¡¨
            research_pdfs: ç ”ç©¶æŠ¥å‘ŠPDFæ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆå·²ä¸ä½¿ç”¨ï¼‰
            
        è¿”å›:
            ç”Ÿæˆçš„çŸ­æœŸæ¸è¿›å¼æ€»ç»“
        """
        # éªŒè¯å‚æ•°
        if self.start_date is None or self.end_date is None:
            raise ValueError("è¯·å…ˆè®¾ç½®æ—¥æœŸèŒƒå›´")
            
        # åˆ›å»ºMarkdownè½¬æ¢è¾“å‡ºæ–‡ä»¶å¤¹
        markdown_dir = self.base_dir / "markdown_files"
        markdown_dir.mkdir(parents=True, exist_ok=True)
        
        # å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ–‡æœ¬
        all_pdfs_markdown = ""
        pdf_files_count = 0
        
        try:
            # å¤„ç†å…¬å‘ŠPDF
            if announcement_pdfs:
                self.logger.info(f"å¤„ç†å…¬å‘ŠPDFæ–‡ä»¶ï¼Œå…±{len(announcement_pdfs)}ä¸ª")
                all_pdfs_markdown += "\n## å…¬å¸å…¬å‘Š\n\n"
                
                for pdf_path in announcement_pdfs:
                    if os.path.exists(pdf_path):
                        pdf_name = Path(pdf_path).name
                        # æ„å»ºMarkdownç¼“å­˜æ–‡ä»¶è·¯å¾„
                        markdown_file_path = markdown_dir / f"{Path(pdf_path).stem}.md"
                        
                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç¼“å­˜çš„Markdownæ–‡ä»¶
                        if markdown_file_path.exists():
                            self.logger.info(f"ä½¿ç”¨ç¼“å­˜çš„Markdown: {markdown_file_path}")
                            with open(markdown_file_path, 'r', encoding='utf-8') as f:
                                markdown_text = f.read()
                        else:
                            # è½¬æ¢PDFä¸ºMarkdown
                            self.logger.info(f"å°†PDFè½¬æ¢ä¸ºMarkdown: {pdf_path}")
                            try:
                                markdown_text = basic_convert(pdf_path, output_dir=str(markdown_dir))
                                if not markdown_text:
                                    self.logger.error(f"PDFè½¬Markdownå¤±è´¥: {pdf_path}")
                                    continue
                            except Exception as e:
                                self.logger.error(f"PDFè½¬Markdownè¿‡ç¨‹å‡ºé”™: {pdf_path}, {e}")
                                continue
                        
                        # æ·»åŠ å…¬å‘Šæ ‡é¢˜
                        all_pdfs_markdown += f"### {pdf_name}\n\n"
                        # æ·»åŠ æ‘˜è¦ç‰ˆæœ¬çš„Markdownå†…å®¹ï¼ˆæœ€å¤š5000å­—ç¬¦ï¼‰
                        truncated_text = markdown_text[:5000]
                        if len(markdown_text) > 5000:
                            truncated_text += "...(å†…å®¹å·²æˆªæ–­)"
                        all_pdfs_markdown += truncated_text + "\n\n---\n\n"
                        pdf_files_count += 1
            
            # å¦‚æœæœ‰PDFæ–‡ä»¶ï¼Œæ·»åŠ æç¤ºè¯´æ˜
            if pdf_files_count > 0:
                all_pdfs_markdown = f"# {self.stock_name}({self.stock_code}) PDFæ–‡æ¡£æ‘˜è¦\n\n" + \
                                   f"å…±{pdf_files_count}ä¸ªPDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼\n\n" + \
                                   all_pdfs_markdown
            
            # æ„å»ºä¸åŒæ¨¡å¼ä¸‹çš„æç¤ºè¯
            if self.is_date_range_mode:
                # æŒ‡å®šæ—¥æœŸèŒƒå›´æ¨¡å¼
                time_description = f"åœ¨{self.start_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}è‡³{self.end_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}æœŸé—´"
                date_span = (self.end_date - self.start_date).days
                prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è‚¡ç¥¨åˆ†æå¸ˆï¼Œéœ€è¦å¯¹{self.stock_name}({self.stock_code}){time_description}çš„ä¿¡æ¯è¿›è¡Œæ€»ç»“åˆ†æã€‚

æˆ‘å°†æä¾›è¿™æ®µæ—¶é—´å†…ä¸è¯¥å…¬å¸ç›¸å…³çš„å…¬å¸å…¬å‘Šå’Œæ–°é—»ä¿¡æ¯ã€‚è¯·è¯¦ç»†åˆ†æè¿™äº›ä¿¡æ¯ã€‚

è¿™ä»½æ€»ç»“é‡ç‚¹æ˜¯æ•æ‰è¿™{date_span}å¤©å†…çš„å…³é”®ä¿¡æ¯ï¼š

1. å…¬å¸å…¬å‘Šåˆ†æï¼šæå–å…¬å¸å…¬å‘Šä¸­çš„å…³é”®ä¿¡æ¯ï¼Œå¦‚è´¢åŠ¡æ•°æ®ã€é‡å¤§äº‹é¡¹ã€ç®¡ç†å±‚å˜åŠ¨ã€é£é™©æç¤ºç­‰ï¼›
2. æ–°é—»èˆ†æƒ…åˆ†æï¼šæ€»ç»“å¸‚åœºæ–°é—»å¯¹å…¬å¸çš„æŠ¥é“å’Œè¯„ä»·ï¼Œä»¥åŠå¯èƒ½å¯¹è‚¡ä»·äº§ç”Ÿçš„å½±å“ï¼›
3. æ—¶é—´çº¿åˆ†æï¼šæŒ‰æ—¶é—´é¡ºåºæ ‡æ³¨é‡è¦äº‹ä»¶ï¼Œçªå‡ºå…¶å¯¹å…¬å¸å‘å±•è·¯å¾„çš„å½±å“ï¼›
4. å…³é”®æŒ‡æ ‡åˆ†æï¼šåˆ†æè¿™æ®µæ—¶é—´å†…å…³é”®æŒ‡æ ‡çš„çŠ¶å†µï¼›
5. ç»¼åˆè¯„ä¼°ï¼šåŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œå¯¹å…¬å¸åœ¨è¿™æ®µæ—¶é—´å†…çš„è¡¨ç°è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚

è¯·æ³¨æ„ï¼š
- ä¿æŒå®¢è§‚ï¼Œçªå‡ºè¿™æ®µæ—¶é—´å†…çš„å…³é”®ä¿¡æ¯ï¼›
- æä¾›æœ‰æ®å¯ä¾çš„åˆ†æï¼Œç‰¹åˆ«å…³æ³¨æ—¶é—´åºåˆ—ä¸Šçš„å˜åŒ–ï¼›
- çªå‡ºé‡ç‚¹ä¿¡æ¯å’Œæ•°æ®ï¼Œå‰”é™¤å†—ä½™å†…å®¹ï¼›
- é€‚å½“å¼•ç”¨åŸæ–‡ä¸­çš„å…³é”®æ•°æ®å’Œè§‚ç‚¹ï¼›
- ä½¿ç”¨ä¸“ä¸šçš„é‡‘èæœ¯è¯­ï¼›
- è¯¦ç»†åˆ†ææ‰€æœ‰æ–‡æ¡£ä¸­çš„é‡è¦æ•°æ®ã€‚

ä»¥ä¸‹æ˜¯PDFæ–‡æ¡£æ‘˜è¦:
{all_pdfs_markdown}

ä»¥ä¸‹æ˜¯æ–°é—»å†…å®¹:
{news_content}

æœ€ç»ˆå½¢æˆä¸€ä»½ä¸“ä¸šã€å…¨é¢çš„æ€»ç»“ï¼ŒåŒæ—¶ä½ éœ€è¦ä¿æŒä¸­æ–‡å›å¤ã€‚
"""
            else:
                # é»˜è®¤æ¨¡å¼ - çŸ­æœŸæ¸è¿›å¼æ€»ç»“
                prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è‚¡ç¥¨åˆ†æå¸ˆï¼Œéœ€è¦å¯¹{self.stock_name}({self.stock_code})åœ¨è¿‡å»{self.days}å¤©å†…ï¼ˆ{self.start_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}è‡³{self.end_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}ï¼‰çš„ä¿¡æ¯è¿›è¡ŒçŸ­æœŸæ¸è¿›å¼æ€»ç»“(Ns,Ï„)ã€‚

æˆ‘å°†æä¾›è¿™æ®µæ—¶é—´å†…ä¸è¯¥å…¬å¸ç›¸å…³çš„å…¬å¸å…¬å‘Šå’Œæ–°é—»ä¿¡æ¯ã€‚è¯·è¯¦ç»†åˆ†æè¿™äº›ä¿¡æ¯ã€‚

è¿™ä»½çŸ­æœŸæ€»ç»“(Ns,Ï„)å°†ä½œä¸ºæ¸è¿›å¼æ€»ç»“ç³»ç»Ÿçš„ç¬¬ä¸€æ­¥ï¼Œé‡ç‚¹æ˜¯æ•æ‰è¿™{self.days}å¤©å†…çš„å…³é”®ä¿¡æ¯å˜åŒ–ï¼š

1. å…¬å¸å…¬å‘Šåˆ†æï¼šæå–å…¬å¸å…¬å‘Šä¸­çš„å…³é”®ä¿¡æ¯ï¼Œå¦‚è´¢åŠ¡æ•°æ®ã€é‡å¤§äº‹é¡¹ã€ç®¡ç†å±‚å˜åŠ¨ã€é£é™©æç¤ºç­‰ï¼›
2. æ–°é—»èˆ†æƒ…åˆ†æï¼šæ€»ç»“å¸‚åœºæ–°é—»å¯¹å…¬å¸çš„æŠ¥é“å’Œè¯„ä»·ï¼Œä»¥åŠå¯èƒ½å¯¹è‚¡ä»·äº§ç”Ÿçš„å½±å“ï¼›
3. æ—¶é—´çº¿åˆ†æï¼šæŒ‰æ—¶é—´é¡ºåºæ ‡æ³¨é‡è¦äº‹ä»¶ï¼Œçªå‡ºå…¶å¯¹å…¬å¸å‘å±•è·¯å¾„çš„å½±å“ï¼›
4. çŸ­æœŸå…³é”®æŒ‡æ ‡å˜åŒ–ï¼šå¯¹æ¯”åˆ†æè¿™{self.days}å¤©å†…å…³é”®æŒ‡æ ‡çš„å˜åŒ–è¶‹åŠ¿ï¼›
5. ç»¼åˆè¯„ä¼°ï¼šåŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œå¯¹å…¬å¸çŸ­æœŸè¡¨ç°è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚

è¯·æ³¨æ„ï¼š
- ä¿æŒå®¢è§‚ï¼Œçªå‡ºè¿™æ®µæ—¶é—´å†…çš„æ–°å˜åŒ–å’Œæ–°ä¿¡æ¯ï¼›
- æä¾›æœ‰æ®å¯ä¾çš„åˆ†æï¼Œç‰¹åˆ«å…³æ³¨æ—¶é—´åºåˆ—ä¸Šçš„å˜åŒ–ï¼›
- çªå‡ºé‡ç‚¹ä¿¡æ¯å’Œæ•°æ®ï¼Œå‰”é™¤å†—ä½™å†…å®¹ï¼›
- é€‚å½“å¼•ç”¨åŸæ–‡ä¸­çš„å…³é”®æ•°æ®å’Œè§‚ç‚¹ï¼›
- ä½¿ç”¨ä¸“ä¸šçš„é‡‘èæœ¯è¯­ï¼›
- è¯¦ç»†åˆ†ææ‰€æœ‰æ–‡æ¡£ä¸­çš„é‡è¦æ•°æ®ã€‚

ä»¥ä¸‹æ˜¯PDFæ–‡æ¡£æ‘˜è¦:
{all_pdfs_markdown}

ä»¥ä¸‹æ˜¯æ–°é—»å†…å®¹:
{news_content}

æœ€ç»ˆå½¢æˆä¸€ä»½ä¸“ä¸šã€å…¨é¢çš„"çŸ­æœŸæ¸è¿›å¼æ–°é—»æ€»ç»“(Ns,Ï„)ï¼ŒåŒæ—¶ä½ éœ€è¦ä¿æŒä¸­æ–‡å›å¤"ã€‚
"""
            
            MODEL = "gemini-2.5-pro"
            
            # ä¿å­˜ç”Ÿæˆçš„promptåˆ°æ–‡ä»¶
            prompt_file = self.base_dir / "prompts" / f"summary_prompt_{self.start_date_str}_{self.end_date_str}.txt"
            prompt_file.parent.mkdir(parents=True, exist_ok=True)
            with open(prompt_file, 'w', encoding='utf-8') as f:
                f.write(prompt)
            self.logger.info(f"æ€»ç»“æç¤ºå·²ä¿å­˜åˆ°: {prompt_file}")
            
            # ä½¿ç”¨Gemini APIç”Ÿæˆæ€»ç»“ï¼Œå¸¦é‡è¯•æœºåˆ¶
            max_retries = 3
            retry_count = 0
            summary = ""
            
            while retry_count < max_retries and not summary:
                try:
                    self.logger.info(f"è°ƒç”¨Gemini APIç”Ÿæˆæ€»ç»“ (å°è¯• {retry_count+1}/{max_retries})...")
                    # ä½¿ç”¨æ–‡æœ¬æç¤º
                    for chunk in self.client.models.generate_content_stream(
                        model=MODEL,
                        contents=prompt
                    ):
                        if chunk.text:
                            summary += chunk.text
                    
                    self.logger.info(f"æˆåŠŸç”Ÿæˆæ€»ç»“ï¼Œé•¿åº¦: {len(summary)} å­—ç¬¦")
                    
                except errors.APIError as e:
                    retry_count += 1
                    error_msg = e.message
                    self.logger.warning(f"ç”Ÿæˆæ€»ç»“å¤±è´¥ (å°è¯• {retry_count}/{max_retries}): {error_msg}:{e.code}")
                    
                    if ("timeout" in error_msg.lower() or 
                        "timed out" in error_msg.lower() or 
                        "server disconnected" in error_msg.lower()):
                        # è¶…æ—¶é”™è¯¯ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´å†é‡è¯•
                        wait_time = 15 * retry_count
                        self.logger.info(f"è¶…æ—¶é”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    else:
                        # å…¶ä»–é”™è¯¯ï¼Œç­‰å¾…æ ‡å‡†æ—¶é—´
                        wait_time = 8 * retry_count
                        self.logger.info(f"é”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    
                    if retry_count < max_retries:
                        time.sleep(wait_time)
                    else:
                        # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè®°å½•é”™è¯¯
                        self.logger.error(f"ç”Ÿæˆæ€»ç»“å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                        raise ValueError(f"ç”Ÿæˆæ€»ç»“å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
            
            return summary
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæ€»ç»“æ—¶å‡ºé”™: {e}")
            raise e
    
    def fusion_progressive_summary(self, current_summary, previous_monthly_summary=None):
        """
        å°†å½“å‰çŸ­æœŸæ€»ç»“(Ns,Ï„)ä¸ä¸Šä¸ªæœˆçš„æ¸è¿›å¼æ€»ç»“(PNs,t-1)èåˆï¼Œç”Ÿæˆå½“å‰æœˆçš„æ¸è¿›å¼æ€»ç»“(PNs,t)
        
        å‚æ•°:
            current_summary: å½“å‰çŸ­æœŸæ€»ç»“(Ns,Ï„)
            previous_monthly_summary: ä¸Šä¸ªæœˆçš„æ¸è¿›å¼æ€»ç»“(PNs,t-1)ï¼Œé»˜è®¤ä¸ºNone
            
        è¿”å›:
            ç”Ÿæˆçš„å½“å‰æœˆæ¸è¿›å¼æ€»ç»“(PNs,t)
        """
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä¸Šä¸ªæœˆçš„æ¸è¿›å¼æ€»ç»“
        if previous_monthly_summary is None:
            self.logger.info("æœªæ‰¾åˆ°ä¸Šä¸ªæœˆçš„æ¸è¿›å¼æ€»ç»“ï¼Œå°†ç›´æ¥ä½¿ç”¨å½“å‰çŸ­æœŸæ€»ç»“ä½œä¸ºæœ¬æœˆæ¸è¿›å¼æ€»ç»“")
            return current_summary
        
        MODEL = "gemini-2.5-pro"
        # æ„å»ºèåˆæç¤º
        fusion_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è‚¡ç¥¨åˆ†æå¸ˆï¼Œéœ€è¦å°†{self.stock_name}({self.stock_code})çš„ä¸¤éƒ¨åˆ†ä¿¡æ¯èåˆä¸ºä¸€ä»½å®Œæ•´çš„é•¿æœŸå†å²æ¸è¿›å¼æ€»ç»“ï¼š

1. ä¸Šä¸ªæœˆçš„æ¸è¿›å¼æ€»ç»“ï¼šåŒ…å«æˆªè‡³ä¸Šä¸ªæœˆæœ«çš„å†å²ç´¯ç§¯ä¿¡æ¯
2. å½“å‰çŸ­æœŸæ€»ç»“ï¼šåŒ…å«è¿‘{self.days}å¤©å†…({self.start_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}è‡³{self.end_date.strftime('%Yå¹´%mæœˆ%dæ—¥')})çš„æœ€æ–°ä¿¡æ¯

è¯·æ‰§è¡Œä»¥ä¸‹æ¸è¿›å¼èåˆä»»åŠ¡ï¼š

1. æ—¶é—´åºåˆ—é›†æˆï¼šå°†ä¸åŒæ—¶é—´æ®µçš„ä¿¡æ¯æŒ‰ç…§æ—¶é—´é¡ºåºç»„ç»‡ï¼Œå½¢æˆå®Œæ•´çš„å†å²æ—¶é—´çº¿
2. è¶‹åŠ¿åˆ†æï¼šè¯†åˆ«å…³é”®æŒ‡æ ‡å’Œäº‹ä»¶åœ¨æ•´ä¸ªæ—¶é—´èŒƒå›´å†…çš„é•¿æœŸå˜åŒ–è¶‹åŠ¿å’Œé‡è¦è½¬æŠ˜ç‚¹
3. ä¿¡æ¯å»é‡ä¸æ•´åˆï¼šç§»é™¤é‡å¤ä¿¡æ¯ï¼Œåˆå¹¶ç›¸ä¼¼å†…å®¹ï¼Œç”¨æœ€æ–°ä¿¡æ¯æ›´æ–°è¿‡æ—¶å†…å®¹ï¼Œä¹Ÿè¦é‡ç‚¹åˆ†æä¸€ä¸‹è¿‘æœŸå‘ç”Ÿçš„äº‹ä»¶çš„æ•´ä½“å½±å“ã€‚é™¤ç¡®å®šåˆ¤æ–­å·²è¿‡æ—¶ä¿¡æ¯å¤–ï¼Œè¦å°½é‡ä¿è¯äº‹ä»¶æè¿°è¯¦ç»†å®Œæ•´ï¼Œä¸è¦çœç•¥ä»»ä½•ç»†èŠ‚ã€‚
4. äº‹ä»¶åˆ†æï¼šå¯¹å…³é”®äº‹ä»¶è¿›è¡Œæ·±åº¦åˆ†æï¼ŒåŒ…æ‹¬äº‹ä»¶å‘ç”Ÿçš„æ—¶é—´ã€åŸå› ã€å½±å“ã€å½±å“èŒƒå›´ã€å½±å“ç¨‹åº¦ã€å½±å“åæœ
5. å†å²å…³è”æ€§åˆ†æï¼šåˆ†æä¸åŒæ—¶æœŸäº‹ä»¶ä¹‹é—´çš„å…³è”å’Œå½±å“
6. å®Œæ•´å‘å±•è½¨è¿¹ï¼šå±•ç°å…¬å¸ä»å†å²ä¿¡æ¯åˆ°è¿‘æœŸçš„å®Œæ•´å‘å±•è½¨è¿¹
7. ç»¼åˆè¯„ä¼°ï¼šåŸºäºå®Œæ•´å†å²ä¿¡æ¯ï¼Œå¯¹å…¬å¸çš„é•¿æœŸè¡¨ç°å’ŒæŠ•èµ„ä»·å€¼è¿›è¡Œå…¨é¢è¯„ä¼°

**===== ä¸Šä¸ªæœˆçš„æ¸è¿›å¼æ€»ç»“ï¼š===== **
{previous_monthly_summary}

**===== è¿‘æœŸçŸ­æœŸæ€»ç»“ï¼š===== **
{current_summary}


**ä½ éœ€è¦æ ¹æ®ä¸Šé¢çš„2ä»½æ€»ç»“ï¼Œæœ€ç»ˆè¾“å‡ºä¸€ä»½å®Œæ•´çš„æ¸è¿›å¼æœˆåº¦æ€»ç»“æŠ¥å‘Šï¼Œæ—¢ä¿ç•™å†å²ä¿¡æ¯çš„æ·±åº¦ï¼Œåˆçªå‡ºæœ€æ–°åŠ¨æ€çš„å½±å“ã€‚**
"""

        # ä¿å­˜èåˆæç¤ºåˆ°æ–‡ä»¶
        current_month = self.end_date.strftime('%Y%m')
        prompt_dir = self.base_dir / "prompts"
        prompt_dir.mkdir(parents=True, exist_ok=True)
        
        fusion_prompt_file = prompt_dir / f"fusion_prompt_{self.stock_code}_{current_month}.txt"
        with open(fusion_prompt_file, 'w', encoding='utf-8') as f:
            f.write(fusion_prompt)
        self.logger.info(f"èåˆæç¤ºå·²ä¿å­˜åˆ°: {fusion_prompt_file}")

        # ä½¿ç”¨Gemini APIç”Ÿæˆèåˆæ€»ç»“ï¼Œå¸¦é‡è¯•æœºåˆ¶
        max_retries = 3
        retry_count = 0
        fusion_result = ""
        
        while retry_count < max_retries and not fusion_result:
            try:
                self.logger.info(f"è°ƒç”¨Gemini APIç”Ÿæˆèåˆæ€»ç»“ (å°è¯• {retry_count+1}/{max_retries})...")
                # ç”Ÿæˆå†…å®¹
                for chunk in self.client.models.generate_content_stream(
                    model=MODEL,
                    contents=fusion_prompt
                ):
                    if chunk.text:
                        fusion_result += chunk.text
                self.logger.info(f"æˆåŠŸç”Ÿæˆèåˆæ€»ç»“ï¼Œé•¿åº¦: {len(fusion_result)} å­—ç¬¦")
                
            except Exception as e:
                retry_count += 1
                error_msg = str(e)
                self.logger.warning(f"ç”Ÿæˆèåˆæ€»ç»“å¤±è´¥ (å°è¯• {retry_count}/{max_retries}): {error_msg}")
                
                if ("timeout" in error_msg.lower() or 
                    "timed out" in error_msg.lower() or 
                    "server disconnected" in error_msg.lower()):
                    # è¶…æ—¶é”™è¯¯ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´å†é‡è¯•
                    wait_time = 15 * retry_count
                    self.logger.info(f"è¶…æ—¶é”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                else:
                    # å…¶ä»–é”™è¯¯ï¼Œç­‰å¾…æ ‡å‡†æ—¶é—´
                    wait_time = 8 * retry_count
                    self.logger.info(f"é”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                
                if retry_count < max_retries:
                    time.sleep(wait_time)
                else:
                    # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè®°å½•é”™è¯¯
                    self.logger.error(f"ç”Ÿæˆèåˆæ€»ç»“å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                    # å‡ºé”™æ—¶è¿”å›å½“å‰çŸ­æœŸæ€»ç»“
                    return current_summary
            
        return fusion_result
    
    def get_previous_monthly_summary(self):
        """
        è·å–ä¸Šä¸ªæœˆçš„æ¸è¿›å¼æ€»ç»“
        
        è¿”å›:
            ä¸Šä¸ªæœˆçš„æ¸è¿›å¼æ€»ç»“æ–‡æœ¬ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        # è®¡ç®—ä¸Šä¸ªæœˆçš„å¹´æœˆ
        if self.end_date is None:
            raise ValueError("è¯·å…ˆè®¾ç½®æ—¥æœŸèŒƒå›´")
            
        # è®¡ç®—ä¸Šä¸ªæœˆçš„æ—¥æœŸï¼ˆå½“å‰æœˆ1å·å‡ä¸€å¤©ï¼Œå†å–å½“æœˆ1å·ï¼‰
        current_month_first_day = self.end_date.replace(day=1)
        last_day_of_prev_month = current_month_first_day - timedelta(days=1)
        previous_month = last_day_of_prev_month.replace(day=1)
        
        # æ„å»ºä¸Šä¸ªæœˆæ€»ç»“æ–‡ä»¶è·¯å¾„
        previous_monthly_summary_file = self.long_term_summary_dir / f"progressive_summary_{self.stock_code}_{previous_month.strftime('%Y%m')}.md"
        
        # å°è¯•è¯»å–ä¸Šä¸ªæœˆæ€»ç»“
        try:
            with open(previous_monthly_summary_file, 'r', encoding='utf-8') as f:
                previous_monthly_summary = f.read()
            print(f"è¯»å–ä¸Šä¸ªæœˆ({previous_month.strftime('%Yå¹´%mæœˆ')})æ¸è¿›å¼æ€»ç»“æˆåŠŸ")
            return previous_monthly_summary
        except FileNotFoundError:
            print(f"æœªæ‰¾åˆ°ä¸Šä¸ªæœˆ({previous_month.strftime('%Yå¹´%mæœˆ')})çš„æ¸è¿›å¼æ€»ç»“")
            return None
    
    def save_monthly_progressive_summary(self, monthly_summary):
        """
        ä¿å­˜æœˆåº¦æ¸è¿›å¼æ€»ç»“
        
        å‚æ•°:
            monthly_summary: æœˆåº¦æ¸è¿›å¼æ€»ç»“æ–‡æœ¬
        """
        if self.end_date is None:
            raise ValueError("è¯·å…ˆè®¾ç½®æ—¥æœŸèŒƒå›´")
            
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.long_term_summary_dir.mkdir(parents=True, exist_ok=True)
        
        # è·å–å½“å‰æœˆä»½
        current_month = self.end_date.strftime('%Y%m')
        
        # æ„å»ºä¿å­˜è·¯å¾„
        summary_file = self.long_term_summary_dir / f"progressive_summary_{self.stock_code}_{current_month}.md"
        
        # å‡†å¤‡è¦ä¿å­˜çš„å†…å®¹ï¼ŒåŒ…æ‹¬èåˆæç¤ºä¿¡æ¯
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨èåˆæç¤ºæ–‡ä»¶
        fusion_prompt_file = self.base_dir / "prompts" / f"fusion_prompt_{self.stock_code}_{current_month}.txt"
        fusion_prompt_info = ""
        
        if fusion_prompt_file.exists():
            try:
                with open(fusion_prompt_file, 'r', encoding='utf-8') as f:
                    fusion_prompt = f.read()
                
                # æå–èåˆæç¤ºä¸­çš„å…³é”®ä¿¡æ¯ï¼ˆä¸åŒ…å«é•¿æ–‡æœ¬å†…å®¹ï¼‰
                prompt_lines = fusion_prompt.split('\n')
                # æå–å‰10è¡Œå’ŒåŒ…å«"ä¸Šä¸ªæœˆçš„æ¸è¿›å¼æ€»ç»“"å’Œ"è¿‘æœŸçŸ­æœŸæ€»ç»“"è¡Œä¹‹å‰çš„å†…å®¹
                cutoff_line = 0
                for i, line in enumerate(prompt_lines):
                    if "ä¸Šä¸ªæœˆçš„æ¸è¿›å¼æ€»ç»“(PNs,t-1)ï¼š" in line:
                        cutoff_line = i
                        break
                
                # åªä¿ç•™å…³é”®éƒ¨åˆ†
                if cutoff_line > 0:
                    fusion_prompt_info = "\n\n## èåˆæç¤ºä¿¡æ¯\n\n```\n" + "\n".join(prompt_lines[:cutoff_line]) + "\n```\n"
                    
            except Exception as e:
                self.logger.error(f"è¯»å–èåˆæç¤ºæ–‡ä»¶å¤±è´¥: {e}")
        
        # æ‹¼æ¥å®Œæ•´å†…å®¹
        formatted_summary = f"# {self.stock_name}({self.stock_code}) æœˆåº¦æ¸è¿›å¼æ€»ç»“\n\n"
        formatted_summary += f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        formatted_summary += f"æ—¶é—´èŒƒå›´: æˆªè‡³{self.end_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}\n"
        
        # æ·»åŠ èåˆæç¤ºä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if fusion_prompt_info:
            formatted_summary += fusion_prompt_info
        
        # æ·»åŠ æ­£æ–‡å†…å®¹
        formatted_summary += "\n\n## æ¸è¿›å¼æ€»ç»“å†…å®¹\n\n"
        formatted_summary += monthly_summary
        
        # ä¿å­˜æ€»ç»“
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(formatted_summary)
            
        self.logger.info(f"ä¿å­˜{self.end_date.strftime('%Yå¹´%mæœˆ')}æ¸è¿›å¼æ€»ç»“åˆ° {summary_file}")
        return summary_file
    
    def save_short_term_summary(self, short_term_summary):
        """
        ä¿å­˜çŸ­æœŸæ€»ç»“
        
        å‚æ•°:
            short_term_summary: çŸ­æœŸæ€»ç»“æ–‡æœ¬
        """
        if self.start_date is None or self.end_date is None:
            raise ValueError("è¯·å…ˆè®¾ç½®æ—¥æœŸèŒƒå›´")
            
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.short_term_summary_dir.mkdir(parents=True, exist_ok=True)
        
        # æ„å»ºä¿å­˜è·¯å¾„
        start_date_str = self.start_date.strftime('%Y%m%d')
        end_date_str = self.end_date.strftime('%Y%m%d')
        summary_file = self.short_term_summary_dir / f"short_term_summary_{self.stock_code}_{start_date_str}_{end_date_str}.md"
        
        # ä¿å­˜æ€»ç»“
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(short_term_summary)
            
        print(f"ä¿å­˜çŸ­æœŸæ€»ç»“åˆ° {summary_file}")
    
    def process_full_pipeline(self, news_content, announcement_pdfs=None, research_pdfs=None):
        """
        æ‰§è¡Œå®Œæ•´çš„æ¸è¿›å¼æ€»ç»“æµç¨‹
        
        å‚æ•°:
            news_content: æ–°é—»å†…å®¹æ–‡æœ¬
            announcement_pdfs: å…¬å‘ŠPDFæ–‡ä»¶è·¯å¾„åˆ—è¡¨
            research_pdfs: ç ”ç©¶æŠ¥å‘ŠPDFæ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        è¿”å›:
            (çŸ­æœŸæ€»ç»“, æœˆåº¦æ¸è¿›å¼æ€»ç»“) æˆ–ä»…çŸ­æœŸæ€»ç»“
        """
        # 1. ç”ŸæˆçŸ­æœŸæ€»ç»“ Ns,Ï„
        if self.is_date_range_mode:
            print(f"å¼€å§‹ç”ŸæˆæŒ‡å®šæ—¥æœŸèŒƒå›´({self.start_date.strftime('%Y-%m-%d')}è‡³{self.end_date.strftime('%Y-%m-%d')})çš„æ€»ç»“...")
        else:
            print(f"å¼€å§‹ç”ŸæˆçŸ­æœŸæ€»ç»“(Ns,Ï„)...")
            
        short_term_summary = self.generate_progressive_summary(
            news_content, 
            announcement_pdfs, 
            research_pdfs
        )
        
        # ä¿å­˜çŸ­æœŸæ€»ç»“
        self.save_short_term_summary(short_term_summary)
        
        # å¦‚æœæ˜¯æŒ‡å®šæ—¥æœŸèŒƒå›´æ¨¡å¼ï¼Œåˆ™ä¸éœ€è¦ç”Ÿæˆæœˆåº¦æ¸è¿›å¼æ€»ç»“
        if self.is_date_range_mode:
            print(f"æŒ‡å®šæ—¥æœŸèŒƒå›´æ¨¡å¼ï¼Œä¸ç”Ÿæˆæœˆåº¦æ¸è¿›å¼æ€»ç»“")
            return short_term_summary, None
        
        # 2. è·å–ä¸Šä¸ªæœˆçš„æ¸è¿›å¼æ€»ç»“ PNs,t-1
        previous_monthly_summary = self.get_previous_monthly_summary()
        
        # 3. ç”Ÿæˆæœ¬æœˆçš„æ¸è¿›å¼æ€»ç»“ PNs,t
        print(f"å¼€å§‹ç”Ÿæˆæœˆåº¦æ¸è¿›å¼æ€»ç»“(PNs,t)...")
        monthly_progressive_summary = self.fusion_progressive_summary(
            short_term_summary, 
            previous_monthly_summary
        )
        
        # 4. ä¿å­˜æœ¬æœˆçš„æ¸è¿›å¼æ€»ç»“
        self.save_monthly_progressive_summary(monthly_progressive_summary)
        
        return short_term_summary, monthly_progressive_summary
    
    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„Progressive News Summarizeræµç¨‹"""
        self.logger.info(f"å¼€å§‹ä¸º{self.stock_name}({self.stock_code})ç”Ÿæˆæ¸è¿›å¼æ–°é—»æ€»ç»“...")
        
        # 1. æ”¶é›†ç ”æŠ¥ - å·²ç¦ç”¨ï¼Œåªè®°å½•æ—¥å¿—
        self.logger.info("ç ”æŠ¥åŠŸèƒ½å·²ç¦ç”¨")
        report_files = []
        
        # 2. æ”¶é›†å…¬å‘Š
        self.logger.info("æ­£åœ¨æ”¶é›†å…¬å‘Š...")
        announcement_files = self.collect_stock_announcements()
        self.logger.info(f"å…±æ”¶é›†åˆ°{len(announcement_files)}ä»½å…¬å‘Š")
        
        # 3. æœç´¢æ–°é—»
        self.logger.info("æ­£åœ¨æœç´¢æ–°é—»...")
        news_file = self.search_stock_news_with_gemini()
        
        # è¯»å–æ–°é—»å†…å®¹
        news_content = ""
        if news_file and os.path.exists(news_file):
            try:
                with open(news_file, 'r', encoding='utf-8') as f:
                    news_content = f.read()
                self.logger.info(f"æˆåŠŸè¯»å–æ–°é—»å†…å®¹ï¼Œé•¿åº¦: {len(news_content)} å­—ç¬¦")
            except Exception as e:
                self.logger.error(f"è¯»å–æ–°é—»æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        
        # 4. æ‰§è¡Œå®Œæ•´çš„æ¸è¿›å¼æ€»ç»“æµç¨‹
        try:
            self.logger.info("å¼€å§‹ç”Ÿæˆæ¸è¿›å¼æ€»ç»“...")
            short_term_summary, monthly_summary = self.process_full_pipeline(
                news_content=news_content,
                announcement_pdfs=announcement_files,
                research_pdfs=report_files  # ä¼ é€’ç©ºåˆ—è¡¨
            )
            
            # 5. æ‰“å°ç»“æœä¿¡æ¯
            self.logger.info(f"å¤„ç†å®Œæˆ!")
            self.logger.info(f"çŸ­æœŸæ€»ç»“ä¿å­˜åœ¨: {self.short_term_summary_dir}")
            self.logger.info(f"æœˆåº¦æ¸è¿›å¼æ€»ç»“ä¿å­˜åœ¨: {self.long_term_summary_dir}")
            
            return {
                "short_term_summary": short_term_summary,
                "monthly_summary": monthly_summary
            }
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæ¸è¿›å¼æ€»ç»“æ—¶å‡ºé”™: {e}")
            return None

    def merge_multiple_summaries(self, summary_file_paths):
        """
        å°†å¤šä¸ªçŸ­æœŸæ€»ç»“åˆå¹¶ä¸ºä¸€ä»½é•¿æœŸæ¸è¿›å¼æ€»ç»“
        
        å‚æ•°:
            summary_file_paths (list): çŸ­æœŸæ€»ç»“æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ŒæŒ‰æ—¶é—´é¡ºåºæ’åˆ—
        
        è¿”å›:
            åˆå¹¶åçš„é•¿æœŸæ¸è¿›å¼æ€»ç»“
        """
        if not summary_file_paths or len(summary_file_paths) == 0:
            self.logger.error("æœªæä¾›ä»»ä½•çŸ­æœŸæ€»ç»“æ–‡ä»¶")
            return None
        
        # æ ‡è®°ä¸ºå¤šçŸ­æœŸæ€»ç»“åˆå¹¶æ¨¡å¼    
        self.is_multiple_summary_mode = True
        
        # è¯»å–æ‰€æœ‰çŸ­æœŸæ€»ç»“æ–‡ä»¶
        summaries = []
        date_ranges = []
        
        for file_path in summary_file_paths:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    summaries.append(content)
                    
                    # å°è¯•ä»æ–‡ä»¶åè·å–æ—¥æœŸèŒƒå›´
                    file_name = os.path.basename(file_path)
                    # ä»æ–‡ä»¶åä¸­æå–æ—¥æœŸä¿¡æ¯ï¼Œé¢„æœŸæ ¼å¼å¦‚ï¼šshort_term_summary_000001_20240101_20240131.md
                    match = re.search(r'(\d{8})_(\d{8})', file_name)
                    if match:
                        start_date = match.group(1)
                        end_date = match.group(2)
                        date_ranges.append((start_date, end_date))
                    else:
                        # å¦‚æœæ–‡ä»¶åä¸­æ²¡æœ‰æ—¥æœŸä¿¡æ¯ï¼Œå°è¯•ä»å†…å®¹ä¸­æå–
                        date_match = re.search(r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥).*?è‡³.*?(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)', content)
                        if date_match:
                            date_ranges.append((date_match.group(1), date_match.group(2)))
                        else:
                            date_ranges.append(("æœªçŸ¥æ—¥æœŸ", "æœªçŸ¥æ—¥æœŸ"))
                            
                self.logger.info(f"å·²è¯»å–çŸ­æœŸæ€»ç»“: {file_path}")
            except Exception as e:
                self.logger.error(f"è¯»å–çŸ­æœŸæ€»ç»“æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
                return None
        
        self.logger.info(f"å…±è¯»å– {len(summaries)} ä»½çŸ­æœŸæ€»ç»“")
        
        # è·å–æ•´ä½“æ—¶é—´èŒƒå›´
        overall_start_date = "æœ€æ—©æ—¥æœŸ"
        overall_end_date = "æœ€è¿‘æ—¥æœŸ"
        if date_ranges and all(isinstance(d[0], str) and len(d[0]) == 8 and d[0].isdigit() for d in date_ranges):
            # å¦‚æœæ—¥æœŸæ˜¯YYYYMMDDæ ¼å¼çš„å­—ç¬¦ä¸²
            overall_start_date = min([d[0] for d in date_ranges])
            overall_end_date = max([d[1] for d in date_ranges])
            try:
                start_dt = datetime.strptime(overall_start_date, "%Y%m%d")
                end_dt = datetime.strptime(overall_end_date, "%Y%m%d")
                overall_start_date = start_dt.strftime("%Yå¹´%mæœˆ%dæ—¥")
                overall_end_date = end_dt.strftime("%Yå¹´%mæœˆ%dæ—¥")
            except ValueError:
                pass
        
        # æ„å»ºåˆå¹¶æ€»ç»“çš„æç¤º
        MODEL = "gemini-2.5-pro"
        
        merge_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è‚¡ç¥¨åˆ†æå¸ˆï¼Œéœ€è¦å°†{self.stock_name}({self.stock_code})çš„å¤šä»½çŸ­æœŸæ€»ç»“åˆå¹¶ä¸ºä¸€ä»½é•¿æœŸæ¸è¿›å¼æ€»ç»“ã€‚

è¿™äº›çŸ­æœŸæ€»ç»“æ¶µç›–äº†ä¸åŒæ—¶é—´æ®µå†…çš„ä¿¡æ¯ï¼Œä½ éœ€è¦å°†å®ƒä»¬æ•´åˆæˆä¸€ä»½å®Œæ•´çš„é•¿æœŸå†å²æ¸è¿›å¼æ€»ç»“(PNs,t-1)ã€‚

è¯·æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š

1. æ—¶é—´åºåˆ—é›†æˆï¼šå°†ä¸åŒæ—¶é—´æ®µçš„ä¿¡æ¯æŒ‰ç…§æ—¶é—´é¡ºåºç»„ç»‡ï¼Œå½¢æˆå®Œæ•´çš„å†å²æ—¶é—´çº¿
2. è¶‹åŠ¿åˆ†æï¼šè¯†åˆ«å…³é”®æŒ‡æ ‡å’Œäº‹ä»¶åœ¨æ•´ä¸ªæ—¶é—´èŒƒå›´å†…çš„é•¿æœŸå˜åŒ–è¶‹åŠ¿å’Œé‡è¦è½¬æŠ˜ç‚¹
3. ä¿¡æ¯å»é‡ä¸æ•´åˆï¼šç§»é™¤é‡å¤ä¿¡æ¯ï¼Œåˆå¹¶ç›¸ä¼¼å†…å®¹ï¼Œä¿æŒå™è¿°çš„ç®€æ´æ€§
4. å†å²å…³è”æ€§åˆ†æï¼šåˆ†æä¸åŒæ—¶æœŸäº‹ä»¶ä¹‹é—´çš„å…³è”å’Œå½±å“
5. å®Œæ•´å‘å±•è½¨è¿¹ï¼šå±•ç°å…¬å¸ä»{overall_start_date}è‡³{overall_end_date}çš„å®Œæ•´å‘å±•è½¨è¿¹
6. ç»¼åˆè¯„ä¼°ï¼šåŸºäºå®Œæ•´å†å²ä¿¡æ¯ï¼Œå¯¹å…¬å¸çš„é•¿æœŸè¡¨ç°å’ŒæŠ•èµ„ä»·å€¼è¿›è¡Œå…¨é¢è¯„ä¼°

ä»¥ä¸‹æ˜¯éœ€è¦åˆå¹¶çš„{len(summaries)}ä»½çŸ­æœŸæ€»ç»“ï¼š

"""
        
        # æ·»åŠ æ¯ä»½çŸ­æœŸæ€»ç»“çš„å†…å®¹
        for i, (summary, date_range) in enumerate(zip(summaries, date_ranges)):
            merge_prompt += f"\n===== ç¬¬{i+1}ä»½çŸ­æœŸæ€»ç»“ï¼ˆ{date_range[0]}è‡³{date_range[1]}ï¼‰ =====\n\n"
            merge_prompt += summary + "\n\n"
        
        merge_prompt += f"""

è¯·åŸºäºä»¥ä¸Šæ‰€æœ‰çŸ­æœŸæ€»ç»“ï¼Œç”Ÿæˆä¸€ä»½å…¨é¢ã€ä¸“ä¸šçš„é•¿æœŸæ¸è¿›å¼æ€»ç»“(PNs,t-1)ï¼Œæ—¢ä¿ç•™å…³é”®å†å²ä¿¡æ¯ï¼Œåˆçªå‡ºæ•´ä½“å‘å±•è¶‹åŠ¿ã€‚
è¿™ä»½æ€»ç»“å°†ä½œä¸ºæœªæ¥ç»§ç»­æ¸è¿›å¼åˆ†æçš„å†å²åŸºç¡€ã€‚è¯·ä¿æŒä¸­æ–‡å›å¤ï¼Œå¹¶ä½¿ç”¨ä¸“ä¸šçš„é‡‘èåˆ†ææœ¯è¯­ã€‚
"""

        # ä¿å­˜åˆå¹¶æç¤ºåˆ°æ–‡ä»¶
        timestamp = datetime.now().strftime('%Y%m%d')
        prompt_dir = self.base_dir / "prompts"
        prompt_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜å®Œæ•´æç¤ºæ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        full_prompt_file = prompt_dir / f"merge_prompt_full_{self.stock_code}_{timestamp}.txt"
        with open(full_prompt_file, 'w', encoding='utf-8') as f:
            f.write(merge_prompt)
        self.logger.info(f"å®Œæ•´åˆå¹¶æç¤ºå·²ä¿å­˜åˆ°: {full_prompt_file}")
        
        # ä¿å­˜ç®€åŒ–ç‰ˆæç¤ºæ–‡ä»¶ï¼ˆä¸åŒ…å«çŸ­æœŸæ€»ç»“å†…å®¹ï¼Œç”¨äºè®°å½•ï¼‰
        simple_prompt_parts = merge_prompt.split("ä»¥ä¸‹æ˜¯éœ€è¦åˆå¹¶çš„")
        if len(simple_prompt_parts) > 1:
            simple_prompt = simple_prompt_parts[0] + f"ä»¥ä¸‹æ˜¯éœ€è¦åˆå¹¶çš„{len(summaries)}ä»½çŸ­æœŸæ€»ç»“...\n\nè¯·åŸºäºä»¥ä¸Šæ‰€æœ‰çŸ­æœŸæ€»ç»“ï¼Œç”Ÿæˆä¸€ä»½å…¨é¢ã€ä¸“ä¸šçš„é•¿æœŸæ¸è¿›å¼æ€»ç»“..."
            simple_prompt_file = prompt_dir / f"merge_prompt_{self.stock_code}_{timestamp}.txt"
            with open(simple_prompt_file, 'w', encoding='utf-8') as f:
                f.write(simple_prompt)
            self.logger.info(f"ç®€åŒ–åˆå¹¶æç¤ºå·²ä¿å­˜åˆ°: {simple_prompt_file}")
        
        try:
            # è®°å½•æ—¶é—´èŒƒå›´
            self.start_date_str = overall_start_date.replace("å¹´", "").replace("æœˆ", "").replace("æ—¥", "") if "å¹´" in overall_start_date else overall_start_date
            self.end_date_str = overall_end_date.replace("å¹´", "").replace("æœˆ", "").replace("æ—¥", "") if "å¹´" in overall_end_date else overall_end_date
            
            # ç”Ÿæˆåˆå¹¶æ€»ç»“ï¼Œä½¿ç”¨é‡è¯•æœºåˆ¶
            max_retries = 3
            retry_count = 0
            merged_summary = ""
            
            while retry_count < max_retries and not merged_summary:
                try:
                    self.logger.info(f"è°ƒç”¨Gemini APIåˆå¹¶å¤šä»½çŸ­æœŸæ€»ç»“ (å°è¯• {retry_count+1}/{max_retries})...")
                    # ç”Ÿæˆå†…å®¹
                    response = self.client.models.generate_content_stream(
                        model=MODEL,
                        contents=merge_prompt
                    )
                    
                    # è·å–å“åº”æ–‡æœ¬
                    for chunk in response:
                        if chunk.candidates and chunk.candidates[0].content and chunk.candidates[0].content.parts:
                            for part in chunk.candidates[0].content.parts:
                                if hasattr(part, 'text') and part.text:
                                    merged_summary += part.text
                    
                    self.logger.info(f"æˆåŠŸç”Ÿæˆåˆå¹¶æ€»ç»“ï¼Œé•¿åº¦: {len(merged_summary)} å­—ç¬¦")
                    
                except Exception as e:
                    retry_count += 1
                    error_msg = str(e)
                    self.logger.warning(f"åˆå¹¶æ€»ç»“å¤±è´¥ (å°è¯• {retry_count}/{max_retries}): {error_msg}")
                    
                    if ("timeout" in error_msg.lower() or 
                        "timed out" in error_msg.lower() or 
                        "server disconnected" in error_msg.lower()):
                        # è¶…æ—¶é”™è¯¯ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´å†é‡è¯•
                        wait_time = 15 * retry_count
                        self.logger.info(f"è¶…æ—¶é”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    else:
                        # å…¶ä»–é”™è¯¯ï¼Œç­‰å¾…æ ‡å‡†æ—¶é—´
                        wait_time = 8 * retry_count
                        self.logger.info(f"é”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    
                    if retry_count < max_retries:
                        time.sleep(wait_time)
                    else:
                        # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè®°å½•é”™è¯¯
                        self.logger.error(f"åˆå¹¶æ€»ç»“å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                        return None
            
            # å‡†å¤‡è¦ä¿å­˜çš„å†…å®¹
            formatted_summary = f"# {self.stock_name}({self.stock_code}) å¤šçŸ­æœŸæ€»ç»“åˆå¹¶æŠ¥å‘Š\n\n"
            formatted_summary += f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            formatted_summary += f"æ—¶é—´èŒƒå›´: {overall_start_date}è‡³{overall_end_date}\n"
            formatted_summary += f"åˆå¹¶æ–‡ä»¶æ•°: {len(summaries)}\n\n"
            
            # æ·»åŠ ç®€åŒ–çš„æç¤ºä¿¡æ¯
            formatted_summary += "## åˆå¹¶ä»»åŠ¡è¯´æ˜\n\n"
            formatted_summary += simple_prompt if 'simple_prompt' in locals() else "å°†å¤šä»½çŸ­æœŸæ€»ç»“åˆå¹¶ä¸ºä¸€ä»½é•¿æœŸæ¸è¿›å¼æ€»ç»“\n\n"
            
            # æ·»åŠ æ­£æ–‡å†…å®¹
            formatted_summary += "\n\n## åˆå¹¶æ€»ç»“å†…å®¹\n\n"
            formatted_summary += merged_summary
            
            # ä¿å­˜åˆå¹¶æ€»ç»“
            merged_file_path = self.long_term_summary_dir / f"merged_summary_{self.stock_code}_{timestamp}.md"
            self.long_term_summary_dir.mkdir(parents=True, exist_ok=True)
            with open(merged_file_path, 'w', encoding='utf-8') as f:
                f.write(formatted_summary)
            
            self.logger.info(f"å¤šä»½çŸ­æœŸæ€»ç»“åˆå¹¶å®Œæˆï¼Œå·²ä¿å­˜è‡³: {merged_file_path}")
            
            return formatted_summary
            
        except Exception as e:
            self.logger.error(f"åˆå¹¶å¤šä»½çŸ­æœŸæ€»ç»“æ—¶å‡ºé”™: {e}")
            return None

    def process_long_period(self, interval_days=90):
        """
        å°†é•¿æ—¶æœŸåˆ†æˆå¤šä¸ªé—´éš”ï¼Œç”Ÿæˆå¤šä¸ªçŸ­æœŸæ€»ç»“ååˆå¹¶ä¸ºä¸€ä¸ªé•¿æœŸæ¸è¿›å¼æ€»ç»“
        
        å‚æ•°:
            interval_days (int): é—´éš”å¤©æ•°ï¼Œé»˜è®¤90å¤©
            
        è¿”å›:
            ç”Ÿæˆçš„é•¿æœŸæ¸è¿›å¼æ€»ç»“
        """
        if self.start_date is None or self.end_date is None:
            raise ValueError("è¯·å…ˆè®¾ç½®æ—¥æœŸèŒƒå›´")
            
        # è®¡ç®—æ€»å¤©æ•°
        total_days = (self.end_date - self.start_date).days
        self.logger.info(f"é•¿æ—¶æœŸæ¨¡å¼å¯åŠ¨: ä»{self.start_date.strftime('%Y-%m-%d')}åˆ°{self.end_date.strftime('%Y-%m-%d')}, å…±{total_days}å¤©")
        self.logger.info(f"å°†æŒ‰ç…§{interval_days}å¤©çš„é—´éš”è¿›è¡Œåˆ†æ®µå¤„ç†")
        
        # åˆ†å‰²æ—¶é—´æ®µ
        current_start = self.start_date
        segments = []
        
        while current_start < self.end_date:
            # è®¡ç®—å½“å‰æ®µçš„ç»“æŸæ—¥æœŸ
            current_end = min(current_start + timedelta(days=interval_days), self.end_date)
            segments.append((current_start, current_end))
            current_start = current_end
            
        self.logger.info(f"å…±åˆ†æˆ{len(segments)}ä¸ªæ—¶é—´æ®µè¿›è¡Œå¤„ç†")
        
        # ç”Ÿæˆæ¯ä¸ªæ—¶é—´æ®µçš„çŸ­æœŸæ€»ç»“
        summary_files = []
        
        for i, (seg_start, seg_end) in enumerate(segments):
            self.logger.info(f"å¤„ç†ç¬¬{i+1}/{len(segments)}ä¸ªæ—¶é—´æ®µ: {seg_start.strftime('%Y-%m-%d')}è‡³{seg_end.strftime('%Y-%m-%d')}")
            
            # ä¿å­˜åŸå§‹æ—¥æœŸè®¾ç½®
            orig_start = self.start_date
            orig_end = self.end_date
            orig_start_str = self.start_date_str
            orig_end_str = self.end_date_str
            
            try:
                # è®¾ç½®æ–°çš„æ—¥æœŸèŒƒå›´
                self.start_date = seg_start
                self.end_date = seg_end
                self.start_date_str = seg_start.strftime("%Y%m%d")
                self.end_date_str = seg_end.strftime("%Y%m%d")
                
                # æ„å»ºçŸ­æœŸæ€»ç»“æ–‡ä»¶è·¯å¾„
                summary_file = self.short_term_summary_dir / f"short_term_summary_{self.stock_code}_{self.start_date_str}_{self.end_date_str}.md"
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨çŸ­æœŸæ€»ç»“æ–‡ä»¶
                if summary_file.exists():
                    self.logger.info(f"å·²å­˜åœ¨çŸ­æœŸæ€»ç»“æ–‡ä»¶ï¼Œè·³è¿‡å¤„ç†: {summary_file}")
                    summary_files.append(str(summary_file))
                    continue
                
                # æ”¶é›†ç ”æŠ¥ - å·²ç¦ç”¨
                self.logger.info(f"ç ”æŠ¥åŠŸèƒ½å·²ç¦ç”¨")
                report_files = []
                
                # æ”¶é›†å…¬å‘Š
                self.logger.info(f"æ”¶é›†{seg_start.strftime('%Y-%m-%d')}è‡³{seg_end.strftime('%Y-%m-%d')}çš„å…¬å‘Š...")
                announcement_files = self.collect_stock_announcements()
                self.logger.info(f"å…±æ”¶é›†åˆ°{len(announcement_files)}ä»½å…¬å‘Š")
                
                # æœç´¢æ–°é—»
                self.logger.info(f"æœç´¢{seg_start.strftime('%Y-%m-%d')}è‡³{seg_end.strftime('%Y-%m-%d')}çš„æ–°é—»...")
                news_file = self.search_stock_news_with_gemini()
                
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–æ–°é—»
                if not news_file or not os.path.exists(news_file):
                    self.logger.error(f"æœªèƒ½è·å–æ–°é—»ï¼Œè·³è¿‡å½“å‰æ—¶é—´æ®µ")
                    continue
                
                # è¯»å–æ–°é—»å†…å®¹
                news_content = ""
                try:
                    with open(news_file, 'r', encoding='utf-8') as f:
                        news_content = f.read()
                    self.logger.info(f"æˆåŠŸè¯»å–æ–°é—»å†…å®¹ï¼Œé•¿åº¦: {len(news_content)} å­—ç¬¦")
                except Exception as e:
                    self.logger.error(f"è¯»å–æ–°é—»æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                    continue
                
                # ç”ŸæˆçŸ­æœŸæ€»ç»“
                self.logger.info(f"ç”Ÿæˆ{seg_start.strftime('%Y-%m-%d')}è‡³{seg_end.strftime('%Y-%m-%d')}çš„çŸ­æœŸæ€»ç»“...")
                
                try:
                    short_term_summary = self.generate_progressive_summary(
                        news_content=news_content,
                        announcement_pdfs=announcement_files,
                        research_pdfs=report_files
                    )
                    
                    # ä¿å­˜çŸ­æœŸæ€»ç»“
                    with open(summary_file, 'w', encoding='utf-8') as f:
                        f.write(short_term_summary)
                    
                    self.logger.info(f"çŸ­æœŸæ€»ç»“å·²ä¿å­˜è‡³: {summary_file}")
                    summary_files.append(str(summary_file))
                    
                except Exception as e:
                    self.logger.error(f"ç”ŸæˆçŸ­æœŸæ€»ç»“æ—¶å‡ºé”™: {e}")
                    # æ£€æŸ¥æ˜¯å¦ä¸ºGemini APIé”™è¯¯æˆ–HTTPè¿æ¥é”™è¯¯
                    error_str = str(e).lower()
                    if "server disconnected" in error_str or "api" in error_str or "http" in error_str or "timeout" in error_str or "connection" in error_str or "The read operation timed out" in error_str:
                        self.logger.critical(f"é‡åˆ°Gemini APIæˆ–ç½‘ç»œè¿æ¥é”™è¯¯ï¼Œç¨‹åºç»ˆæ­¢")
                        sys.exit(1)  # é‡åˆ°APIæˆ–ç½‘ç»œé”™è¯¯æ—¶ç›´æ¥é€€å‡º
                    continue
                    
            finally:
                # æ¢å¤åŸå§‹æ—¥æœŸè®¾ç½®
                self.start_date = orig_start
                self.end_date = orig_end
                self.start_date_str = orig_start_str
                self.end_date_str = orig_end_str
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„çŸ­æœŸæ€»ç»“
        if len(summary_files) == 0:
            self.logger.error("æœªèƒ½ç”Ÿæˆä»»ä½•çŸ­æœŸæ€»ç»“ï¼Œæ— æ³•ç»§ç»­")
            return None
            
        self.logger.info(f"å·²ç”Ÿæˆ{len(summary_files)}/{len(segments)}ä¸ªçŸ­æœŸæ€»ç»“")
        
        # åˆå¹¶æ‰€æœ‰çŸ­æœŸæ€»ç»“
        self.logger.info("å¼€å§‹åˆå¹¶æ‰€æœ‰çŸ­æœŸæ€»ç»“...")
        merged_summary = self.merge_multiple_summaries(summary_files)
        
        if not merged_summary:
            self.logger.error("åˆå¹¶çŸ­æœŸæ€»ç»“å¤±è´¥")
            return None
            
        # ä¿å­˜æœ€ç»ˆçš„æ¸è¿›å¼æ€»ç»“ - ç”±äºmerge_multiple_summarieså·²ç»ä¿å­˜äº†æ ¼å¼åŒ–çš„æ€»ç»“æ–‡ä»¶
        # è¿™é‡Œæˆ‘ä»¬åªéœ€è¦è¿”å›åˆå¹¶ç»“æœå³å¯ï¼Œä¸éœ€è¦å†æ¬¡ä¿å­˜
        self.logger.info(f"é•¿æ—¶æœŸæ¸è¿›å¼æ€»ç»“ç”Ÿæˆå®Œæˆ")
        return merged_summary

    def generate_announcement_summary(self, announcement_pdfs=None):
        """
        ç”Ÿæˆå…¬å‘Šä¸“é¡¹æ€»ç»“
        
        å‚æ•°:
            announcement_pdfs: å…¬å‘ŠPDFæ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        è¿”å›:
            ç”Ÿæˆçš„å…¬å‘Šæ€»ç»“
        """
        # éªŒè¯å‚æ•°
        if self.start_date is None or self.end_date is None:
            raise ValueError("è¯·å…ˆè®¾ç½®æ—¥æœŸèŒƒå›´")
            
        # åˆ›å»ºMarkdownè½¬æ¢è¾“å‡ºæ–‡ä»¶å¤¹
        markdown_dir = self.base_dir / "markdown_files"
        markdown_dir.mkdir(parents=True, exist_ok=True)
        
        # å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ–‡æœ¬
        all_pdfs_markdown = ""
        pdf_files_count = 0
        
        try:
            # å¤„ç†å…¬å‘ŠPDF
            if announcement_pdfs:
                self.logger.info(f"å¤„ç†å…¬å‘ŠPDFæ–‡ä»¶ï¼Œå…±{len(announcement_pdfs)}ä¸ª")
                all_pdfs_markdown += "\n## å…¬å¸å…¬å‘Š\n\n"
                
                for pdf_path in announcement_pdfs:
                    if os.path.exists(pdf_path):
                        pdf_name = Path(pdf_path).name
                        # æ„å»ºMarkdownç¼“å­˜æ–‡ä»¶è·¯å¾„
                        markdown_file_path = markdown_dir / f"{Path(pdf_path).stem}.md"
                        
                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç¼“å­˜çš„Markdownæ–‡ä»¶
                        if markdown_file_path.exists():
                            self.logger.info(f"ä½¿ç”¨ç¼“å­˜çš„Markdown: {markdown_file_path}")
                            with open(markdown_file_path, 'r', encoding='utf-8') as f:
                                markdown_text = f.read()
                        else:
                            # è½¬æ¢PDFä¸ºMarkdown
                            self.logger.info(f"å°†PDFè½¬æ¢ä¸ºMarkdown: {pdf_path}")
                            try:
                                markdown_text = basic_convert(pdf_path, output_dir=str(markdown_dir))
                                if not markdown_text:
                                    self.logger.error(f"PDFè½¬Markdownå¤±è´¥: {pdf_path}")
                                    continue
                            except Exception as e:
                                self.logger.error(f"PDFè½¬Markdownè¿‡ç¨‹å‡ºé”™: {pdf_path}, {e}")
                                continue
                        
                        # æ·»åŠ å…¬å‘Šæ ‡é¢˜
                        all_pdfs_markdown += f"### {pdf_name}\n\n"
                        # æ·»åŠ æ‘˜è¦ç‰ˆæœ¬çš„Markdownå†…å®¹ï¼ˆæœ€å¤š5000å­—ç¬¦ï¼‰
                        truncated_text = markdown_text[:50000]
                        if len(markdown_text) > 50000:
                            truncated_text += "...(å†…å®¹å·²æˆªæ–­)"
                        all_pdfs_markdown += truncated_text + "\n\n---\n\n"
                        pdf_files_count += 1
            
            # å¦‚æœæœ‰PDFæ–‡ä»¶ï¼Œæ·»åŠ æç¤ºè¯´æ˜
            if pdf_files_count > 0:
                all_pdfs_markdown = f"# {self.stock_name}({self.stock_code}) å…¬å¸å…¬å‘Šæ‘˜è¦\n\n" + \
                                   f"å…±{pdf_files_count}ä¸ªå…¬å‘ŠPDFæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼\n\n" + \
                                   all_pdfs_markdown
            else:
                self.logger.warning(f"æœªæ‰¾åˆ°ä»»ä½•å…¬å‘ŠPDFæ–‡ä»¶")
                return "æœªæ‰¾åˆ°æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„å…¬å¸å…¬å‘Šæ–‡ä»¶ã€‚"
            
            # æ„å»ºæç¤ºè¯
            time_description = f"åœ¨{self.start_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}è‡³{self.end_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}æœŸé—´"
            # å…¬å‘Šåˆ†ç±»å¤„ç†prompt
            prompt = f"""
            ä¸‹è¿°å†…å®¹æ˜¯{self.stock_name}({self.stock_code}){time_description}å‘å¸ƒå…¬å‘ŠMarkdownå†…å®¹ï¼š
            
            ===========================================å…¬å‘Šå†…å®¹å¼€å§‹==========================================
            {all_pdfs_markdown}
            ===========================================å…¬å‘Šå†…å®¹ç»“æŸ==========================================
            
            è¯·å°†ä¸Šè¿°å…¬å‘Šå†…å®¹ï¼ŒæŒ‰ä»¥ä¸‹è§„åˆ™å¤„ç†ï¼š

            ã€åˆ†ç±»å¤„ç†è§„åˆ™ã€‘
            1. å¸¸è§„å…¬å‘Šï¼ˆåˆ†çº¢/æ—¥å¸¸äº¤æ˜“ï¼‰ï¼š
               - æå–ï¼šæ‰§è¡Œæ—¥æœŸã€é‡‘é¢åŸºå‡†ã€å¯¹æ¯”å¾€æœŸå˜åŒ–ç‡
               - æ¨¡æ¿ï¼šâ–  å¹´åº¦åˆ†çº¢é¢„æ¡ˆï¼šæ¯è‚¡Xå…ƒï¼ˆåŒæ¯”+Y%ï¼‰ï¼Œè‚¡æƒç™»è®°æ—¥date

            2. é‡å¤§äº‹é¡¹å…¬å‘Šï¼ˆå¹¶è´­/è¯‰è®¼/é‡ç»„ï¼‰ï¼š
               - å¿…é¡»è§£æï¼š
                 a. äº‹é¡¹è¿›å±•é˜¶æ®µï¼ˆç­¹åˆ’/å®æ–½/å®Œæˆï¼‰
                 b. å¯¹èµ„äº§è´Ÿå€ºè¡¨çš„å…·ä½“å½±å“ç§‘ç›®
                 c. é£é™©æç¤ºä¸­çš„å…³é”®å‚æ•°ï¼ˆå¦‚èµ”å¿ä¸Šé™ï¼‰
               - æ¨¡æ¿ï¼šâš ï¸ é‡å¤§è¯‰è®¼è¿›å±•ï¼šæ¶‰è¯‰é‡‘é¢amountï¼ˆå Q3å‡€åˆ©æ¶¦ratio%ï¼‰ï¼Œé¢„è®¡è®¡æè´Ÿå€ºç§‘ç›®"account"

            3. è´¢åŠ¡æŠ¥å‘Šå…¬å‘Šï¼š
               - è”åŠ¨è´¢æŠ¥æ¨¡å—åˆ†æç»“æœï¼Œä»…ä¿ç•™ï¼š
                 a. å…³é”®æŒ‡æ ‡è¶…é¢„æœŸå¹…åº¦ï¼ˆvs åˆ†æå¸ˆå…±è¯†é¢„æœŸï¼‰
                 b. ç®¡ç†å±‚æŒ‡å¼•å˜åŒ–ï¼ˆç”¨diffç®—æ³•æ¯”å¯¹å¾€æœŸè¡¨è¿°ï¼‰
                 c. å®¡è®¡æ„è§ç±»å‹å˜æ›´

            ã€è¾“å‡ºè¦æ±‚ã€‘
            - æŒ‰æ—¶é—´å€’åºæ’åˆ—
            - æ¯æ¡å…¬å‘Šæ·»åŠ å½±å“ç³»æ•°æ ‡ç­¾ï¼š
               ğŸ”µ çŸ­æœŸæ“ä½œå½±å“ï¼ˆæ¶‰åŠäº¤æ˜“æ—¥æœŸï¼‰ 
               ğŸŸ  ä¸­æœŸè´¢åŠ¡å½±å“ï¼ˆå½±å“æœªæ¥1-2å­£æŠ¥ï¼‰
               ğŸ”´ é•¿æœŸæˆ˜ç•¥å½±å“ï¼ˆæ”¹å˜ä¸šåŠ¡æ¨¡å¼ï¼‰
                
            ã€ç¤ºä¾‹ã€‘
            ğŸ”´ **é•¿æœŸæˆ˜ç•¥å½±å“**
                *   **å…¬å‘Šæ—¥æœŸ:** 2021-02-02
                *   **äº‹é¡¹:** å­å…¬å¸xxxIPOç”³è¯·è¿›å±•ã€‚
                *   **æ‘˜è¦:** âš ï¸ å­å…¬å¸IPOè¿›å±•ï¼šæ§è‚¡å­å…¬å¸xxé¦–å‘ç”³è¯·è·ä¸Šæµ·è¯åˆ¸äº¤æ˜“æ‰€ç§‘åˆ›æ¿ä¸Šå¸‚å§”å‘˜ä¼šå®¡è®®é€šè¿‡ï¼ˆå®æ–½é˜¶æ®µï¼‰ã€‚æ­¤ä¸¾å¯èƒ½å½±å“å…¬å¸èµ„äº§ç»“æ„å’Œä¼°å€¼ï¼Œä½†å°šéœ€è¯ç›‘ä¼šæ³¨å†Œï¼Œå­˜åœ¨ä¸ç¡®å®šæ€§ã€‚
            """
            
            MODEL = "gemini-2.5-pro"
            
            # ä¿å­˜ç”Ÿæˆçš„promptåˆ°æ–‡ä»¶
            prompt_file = self.base_dir / "prompts" / f"announcement_summary_prompt_{self.start_date_str}_{self.end_date_str}.md"
            prompt_file.parent.mkdir(parents=True, exist_ok=True)
            with open(prompt_file, 'w', encoding='utf-8') as f:
                f.write(prompt)
            self.logger.info(f"å…¬å‘Šæ€»ç»“æç¤ºå·²ä¿å­˜åˆ°: {prompt_file}")
            
        #     # ä½¿ç”¨Gemini APIç”Ÿæˆæ€»ç»“ï¼Œå¸¦é‡è¯•æœºåˆ¶
        #     max_retries = 3
        #     retry_count = 0
        #     summary = ""
            
        #     while retry_count < max_retries and not summary:
        #         try:
        #             self.logger.info(f"è°ƒç”¨Gemini APIç”Ÿæˆå…¬å‘Šæ€»ç»“ (å°è¯• {retry_count+1}/{max_retries})...")
        #             # ä½¿ç”¨æ–‡æœ¬æç¤º
        #             for chunk in self.client.models.generate_content_stream(
        #                 model=MODEL,
        #                 contents=prompt
        #             ):
        #                 if chunk.text:
        #                     summary += chunk.text
                    
        #             self.logger.info(f"æˆåŠŸç”Ÿæˆå…¬å‘Šæ€»ç»“ï¼Œé•¿åº¦: {len(summary)} å­—ç¬¦")
                    
        #         except errors.APIError as e:
        #             retry_count += 1
        #             error_msg = e.message
        #             self.logger.warning(f"ç”Ÿæˆå…¬å‘Šæ€»ç»“å¤±è´¥ (å°è¯• {retry_count}/{max_retries}): {error_msg}:{e.code}")
                    
        #             if ("timeout" in error_msg.lower() or 
        #                 "timed out" in error_msg.lower() or 
        #                 "server disconnected" in error_msg.lower()):
        #                 # è¶…æ—¶é”™è¯¯ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´å†é‡è¯•
        #                 wait_time = 15 * retry_count
        #                 self.logger.info(f"è¶…æ—¶é”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
        #             else:
        #                 # å…¶ä»–é”™è¯¯ï¼Œç­‰å¾…æ ‡å‡†æ—¶é—´
        #                 wait_time = 8 * retry_count
        #                 self.logger.info(f"é”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    
        #             if retry_count < max_retries:
        #                 time.sleep(wait_time)
        #             else:
        #                 # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè®°å½•é”™è¯¯
        #                 self.logger.error(f"ç”Ÿæˆå…¬å‘Šæ€»ç»“å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
        #                 raise ValueError(f"ç”Ÿæˆå…¬å‘Šæ€»ç»“å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
            
        #     return summary
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå…¬å‘Šæ€»ç»“æ—¶å‡ºé”™: {e}")
            raise e
            
    def save_announcement_summary(self, announcement_summary):
        """
        ä¿å­˜å…¬å‘Šä¸“é¡¹æ€»ç»“
        
        å‚æ•°:
            announcement_summary: å…¬å‘Šæ€»ç»“æ–‡æœ¬
        """
        if self.start_date is None or self.end_date is None:
            raise ValueError("è¯·å…ˆè®¾ç½®æ—¥æœŸèŒƒå›´")
            
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.announcement_summary_dir.mkdir(parents=True, exist_ok=True)
        
        # æ„å»ºä¿å­˜è·¯å¾„
        start_date_str = self.start_date.strftime('%Y%m%d')
        end_date_str = self.end_date.strftime('%Y%m%d')
        summary_file = self.announcement_summary_dir / f"announcement_summary_{self.stock_code}_{start_date_str}_{end_date_str}.md"
        
        # ä¿å­˜æ€»ç»“
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(announcement_summary)
            
        self.logger.info(f"ä¿å­˜å…¬å‘Šä¸“é¡¹æ€»ç»“åˆ° {summary_file}")
        return summary_file

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description="Progressive News Summarizer - è‚¡ç¥¨æ–°é—»æ¸è¿›å¼æ€»ç»“å·¥å…·")
    parser.add_argument("--code", type=str, help="è‚¡ç¥¨ä»£ç ï¼Œå¦‚'000001'", default="000001")
    parser.add_argument("--name", type=str, help="è‚¡ç¥¨åç§°ï¼Œå¦‚'å¹³å®‰é“¶è¡Œ'", default="å¹³å®‰é“¶è¡Œ")
    parser.add_argument("--market", type=str, choices=["Aè‚¡", "æ¸¯è‚¡"], help="å¸‚åœºç±»å‹ï¼š'Aè‚¡'æˆ–'æ¸¯è‚¡'", default="Aè‚¡")
    parser.add_argument("--days", type=int, help="è¦æŸ¥è¯¢çš„å¤©æ•°ï¼Œé»˜è®¤30å¤©", default=30)
    parser.add_argument("--batch", action="store_true", help="æ‰¹é‡å¤„ç†å¤šåªè‚¡ç¥¨ï¼Œå°†å¿½ç•¥--codeå’Œ--nameå‚æ•°")
    
    # æ·»åŠ æŒ‡å®šæ—¥æœŸèŒƒå›´çš„å‚æ•°
    parser.add_argument("--start_date", type=str, help="å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYYMMDDï¼Œå¦‚'20240101'")
    parser.add_argument("--end_date", type=str, help="ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYYMMDDï¼Œå¦‚'20240131'")
    
    # æ·»åŠ å¤šçŸ­æœŸæ€»ç»“åˆå¹¶çš„å‚æ•°
    parser.add_argument("--merge_summaries", action="store_true", help="åˆå¹¶å¤šä¸ªçŸ­æœŸæ€»ç»“ä¸ºé•¿æœŸæ€»ç»“")
    parser.add_argument("--summary_files", nargs="+", help="è¦åˆå¹¶çš„çŸ­æœŸæ€»ç»“æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œç”¨ç©ºæ ¼åˆ†éš”")
    
    # æ·»åŠ å•ç‹¬è¿›è¡Œèåˆçš„å‚æ•°
    parser.add_argument("--fusion_mode", action="store_true", help="å•ç‹¬è°ƒç”¨èåˆåŠŸèƒ½ï¼Œéœ€è¦æŒ‡å®šå½“å‰æ€»ç»“å’Œä¸Šä¸ªæœˆæ€»ç»“æ–‡ä»¶")
    parser.add_argument("--current_summary_file", type=str, help="å½“å‰çŸ­æœŸæ€»ç»“(Ns,Ï„)æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--previous_summary_file", type=str, help="ä¸Šä¸ªæœˆçš„æ¸è¿›å¼æ€»ç»“(PNs,t-1)æ–‡ä»¶è·¯å¾„")
    
    # æ·»åŠ é•¿æ—¶æœŸåˆ†æ®µå¤„ç†æ¨¡å¼çš„å‚æ•°
    parser.add_argument("--long_period_mode", action="store_true", help="é•¿æ—¶æœŸåˆ†æ®µå¤„ç†æ¨¡å¼ï¼Œå°†æŒ‡å®šçš„é•¿æ—¶æœŸåˆ†æˆå¤šä¸ªé—´éš”è¿›è¡Œæ€»ç»“")
    parser.add_argument("--interval_days", type=int, help="é•¿æ—¶æœŸåˆ†æ®µçš„é—´éš”å¤©æ•°ï¼Œé»˜è®¤ä¸º90å¤©", default=90)
    
    # æ·»åŠ å…¬å‘Šä¸“é¡¹æ€»ç»“çš„å‚æ•°
    parser.add_argument("--announcement_only", action="store_true", help="åªç”Ÿæˆå…¬å¸å…¬å‘Šä¸“é¡¹æ€»ç»“ï¼Œä¸å¤„ç†æ–°é—»å†…å®¹")
    
    # æ·»åŠ åªè·å–æ–°é—»çš„æ¨¡å¼
    parser.add_argument("--news_only", action="store_true", help="åªè·å–æŒ‡å®šæ—¶é—´å†…çš„æ–°é—»ï¼Œä¸ç”Ÿæˆæ€»ç»“")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
    logger = setup_logging(args.code, args.name, args.start_date, args.end_date)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not GOOGLE_API_KEY:
        logger.error("è¯·è®¾ç½®GOOGLE_API_KEYç¯å¢ƒå˜é‡")
        return

    # å…¬å‘Šä¸“é¡¹æ€»ç»“æ¨¡å¼
    if args.announcement_only:
        if not args.start_date or not args.end_date:
            logger.error("å…¬å‘Šä¸“é¡¹æ€»ç»“æ¨¡å¼éœ€è¦åŒæ—¶æŒ‡å®š--start_dateå’Œ--end_dateå‚æ•°")
            return
            
        logger.info(f"å¼€å§‹ç”Ÿæˆå…¬å‘Šä¸“é¡¹æ€»ç»“: ä»{args.start_date}åˆ°{args.end_date}")
        
        # åˆ›å»ºProgressiveNewsSummarizerå®ä¾‹
        summarizer = ProgressiveNewsSummarizer(
            args.code, args.name, args.market, days=args.days,
            start_date=args.start_date, end_date=args.end_date
        )
        
        try:
            # æ”¶é›†å…¬å‘Š
            logger.info("æ­£åœ¨æ”¶é›†å…¬å‘Š...")
            announcement_files = summarizer.collect_stock_announcements()
            logger.info(f"å…±æ”¶é›†åˆ°{len(announcement_files)}ä»½å…¬å‘Š")
            
            if not announcement_files:
                logger.warning(f"æœªæ‰¾åˆ°{args.start_date}è‡³{args.end_date}æœŸé—´çš„å…¬å‘Š")
                return
                
            # ç”Ÿæˆå…¬å‘Šä¸“é¡¹æ€»ç»“
            logger.info("å¼€å§‹ç”Ÿæˆå…¬å‘Šä¸“é¡¹æ€»ç»“...")
            announcement_summary = summarizer.generate_announcement_summary(
                announcement_pdfs=announcement_files
            )
            
            # ä¿å­˜å…¬å‘Šä¸“é¡¹æ€»ç»“
            summary_file = summarizer.save_announcement_summary(announcement_summary)
            
            logger.info(f"å…¬å‘Šä¸“é¡¹æ€»ç»“ç”Ÿæˆå®Œæˆï¼Œå·²ä¿å­˜è‡³: {summary_file}")
            return
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå…¬å‘Šä¸“é¡¹æ€»ç»“æ—¶å‡ºé”™: {e}")
            return
    
    # åªè·å–æ–°é—»æ¨¡å¼
    if args.news_only:
        if not args.start_date or not args.end_date:
            logger.error("åªè·å–æ–°é—»æ¨¡å¼éœ€è¦åŒæ—¶æŒ‡å®š--start_dateå’Œ--end_dateå‚æ•°")
            return
            
        logger.info(f"å¼€å§‹è·å–æ–°é—»: ä»{args.start_date}åˆ°{args.end_date}")
        
        # åˆ›å»ºProgressiveNewsSummarizerå®ä¾‹
        summarizer = ProgressiveNewsSummarizer(
            args.code, args.name, args.market, days=args.days,
            start_date=args.start_date, end_date=args.end_date
        )
        
        try:
            # æœç´¢æ–°é—»
            logger.info("æ­£åœ¨æœç´¢æ–°é—»...")
            news_file = summarizer.search_stock_news_with_gemini()
            
            if news_file and os.path.exists(news_file):
                logger.info(f"æ–°é—»è·å–æˆåŠŸï¼Œå·²ä¿å­˜è‡³: {news_file}")
            else:
                logger.error(f"æ–°é—»è·å–å¤±è´¥")
            return
            
        except Exception as e:
            logger.error(f"è·å–æ–°é—»æ—¶å‡ºé”™: {e}")
            return
    
    # å•ç‹¬èåˆæ¨¡å¼
    if args.fusion_mode:
        if not args.current_summary_file or not args.previous_summary_file:
            logger.error("èåˆæ¨¡å¼éœ€è¦åŒæ—¶æŒ‡å®š--current_summary_fileå’Œ--previous_summary_fileå‚æ•°")
            return
            
        logger.info(f"å¼€å§‹èåˆæ€»ç»“...")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        try:
            with open(args.current_summary_file, 'r', encoding='utf-8') as f:
                current_summary = f.read()
                
            with open(args.previous_summary_file, 'r', encoding='utf-8') as f:
                previous_summary = f.read()
                
            # åˆ›å»ºProgressiveNewsSummarizerå®ä¾‹
            summarizer = ProgressiveNewsSummarizer(
                args.code, args.name, args.market, days=args.days,
                start_date=args.start_date, end_date=args.end_date
            )
            
            # è°ƒç”¨èåˆæ–¹æ³•
            fusion_result = summarizer.fusion_progressive_summary(
                current_summary=current_summary,
                previous_monthly_summary=previous_summary
            )
            
            # ä¿å­˜èåˆç»“æœ
            current_date = datetime.now().strftime('%Y%m%d')
            fusion_file = summarizer.long_term_summary_dir / f"fusion_summary_{args.code}_{current_date}.md"
            
            with open(fusion_file, 'w', encoding='utf-8') as f:
                f.write(fusion_result)
                
            logger.info(f"èåˆå®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {fusion_file}")
            return
            
        except Exception as e:
            logger.error(f"èåˆæ€»ç»“æ—¶å‡ºé”™: {e}")
            return
    
    # é•¿æ—¶æœŸåˆ†æ®µå¤„ç†æ¨¡å¼
    if args.long_period_mode:
        if not args.start_date or not args.end_date:
            logger.error("é•¿æ—¶æœŸåˆ†æ®µå¤„ç†æ¨¡å¼éœ€è¦åŒæ—¶æŒ‡å®š--start_dateå’Œ--end_dateå‚æ•°")
            return
        
        logger.info(f"å¼€å§‹é•¿æ—¶æœŸåˆ†æ®µå¤„ç†æ¨¡å¼: ä»{args.start_date}åˆ°{args.end_date}, é—´éš”ä¸º{args.interval_days}å¤©")
        
        # åˆ›å»ºProgressiveNewsSummarizerå®ä¾‹
        summarizer = ProgressiveNewsSummarizer(
            args.code, args.name, args.market, days=args.days,
            start_date=args.start_date, end_date=args.end_date
        )
        
        try:
            # è°ƒç”¨é•¿æ—¶æœŸå¤„ç†æ–¹æ³•
            result = summarizer.process_long_period(interval_days=args.interval_days)
            
            if result:
                logger.info(f"é•¿æ—¶æœŸåˆ†æ®µå¤„ç†å®Œæˆ")
            else:
                logger.error(f"é•¿æ—¶æœŸåˆ†æ®µå¤„ç†å¤±è´¥")
                
        except Exception as e:
            logger.critical(f"é•¿æ—¶æœŸåˆ†æ®µå¤„ç†å‡ºé”™: {e}")
            return
        
        return
    
    # å¤šçŸ­æœŸæ€»ç»“åˆå¹¶æ¨¡å¼
    if args.merge_summaries:
        if not args.summary_files or len(args.summary_files) == 0:
            logger.error("è¯·ä½¿ç”¨--summary_fileså‚æ•°æŒ‡å®šè¦åˆå¹¶çš„çŸ­æœŸæ€»ç»“æ–‡ä»¶")
            return
            
        logger.info(f"å¼€å§‹åˆå¹¶{len(args.summary_files)}ä»½çŸ­æœŸæ€»ç»“...")
        summarizer = ProgressiveNewsSummarizer(
            args.code, args.name, args.market, days=args.days
        )
        merged_summary = summarizer.merge_multiple_summaries(args.summary_files)
        if merged_summary:
            logger.info("çŸ­æœŸæ€»ç»“åˆå¹¶æˆåŠŸ")
        else:
            logger.error("çŸ­æœŸæ€»ç»“åˆå¹¶å¤±è´¥")
        return
    
    # æ‰¹é‡å¤„ç†æ¨¡å¼
    if args.batch:
        stocks = [
            {"code": "002352", "name": "é¡ºä¸°æ§è‚¡", "market": "Aè‚¡"},
            {"code": "002714", "name": "ç‰§åŸè‚¡ä»½", "market": "Aè‚¡"},
            {"code": "603501", "name": "è±ªå¨é›†å›¢", "market": "Aè‚¡"},
            {"code": "002028", "name": "æ€æºç”µæ°”", "market": "Aè‚¡"},
            {"code": "300274", "name": "é˜³å…‰ç”µæº", "market": "Aè‚¡"},
            {"code": "600276", "name": "æ’ç‘åŒ»è¯", "market": "Aè‚¡"},
            {"code": "002371", "name": "åŒ—æ–¹ååˆ›", "market": "Aè‚¡"},
            {"code": "601877", "name": "æ­£æ³°ç”µå™¨", "market": "Aè‚¡"},
            {"code": "688099", "name": "æ™¶æ™¨è‚¡ä»½", "market": "Aè‚¡"},
            {"code": "002027", "name": "åˆ†ä¼—ä¼ åª’", "market": "Aè‚¡"},
        ]
        
        results = {}
        for stock in stocks:
            # ä¸ºæ¯åªè‚¡ç¥¨åˆ›å»ºæ–°çš„æ—¥å¿—è®°å½•å™¨
            stock_logger = setup_logging(stock["code"], stock["name"], args.start_date, args.end_date)
            stock_logger.info(f"å¼€å§‹å¤„ç†: {stock['name']}({stock['code']})...")
            summarizer = ProgressiveNewsSummarizer(
                stock["code"], stock["name"], stock["market"], days=args.days,
                start_date=args.start_date, end_date=args.end_date
            )
            result = summarizer.run()
            if result:
                results[stock["code"]] = result
                stock_logger.info(f"{stock['name']}({stock['code']})å¤„ç†å®Œæˆ")
            else:
                stock_logger.error(f"{stock['name']}({stock['code']})å¤„ç†å¤±è´¥")
            
        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼Œå…±å¤„ç†{len(results)}/{len(stocks)}åªè‚¡ç¥¨")
    else:
        # å•åªè‚¡ç¥¨å¤„ç†æ¨¡å¼
        logger.info(f"å¼€å§‹å¤„ç†: {args.name}({args.code})...")
        summarizer = ProgressiveNewsSummarizer(
            args.code, args.name, args.market, days=args.days,
            start_date=args.start_date, end_date=args.end_date
        )
        result = summarizer.run()
        if result:
            logger.info(f"{args.name}({args.code})å¤„ç†æˆåŠŸ")
        else:
            logger.error(f"{args.name}({args.code})å¤„ç†å¤±è´¥")
        
if __name__ == "__main__":
    main()
