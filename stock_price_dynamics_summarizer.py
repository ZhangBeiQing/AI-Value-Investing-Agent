f"""Stock Price Dynamics Summarizer
===================================

æœ¬æ–‡ä»¶æ˜¯æ•´ä¸ªé¡¹ç›®ç”¨äºâ€œä»·æ ¼åŠ¨æ€åˆ†æâ€çš„ä¸»è„šæœ¬ï¼Œå…¶å®šä½ä¸ MCP å·¥å…·ä¸åŒï¼š

* **æ ¸å¿ƒç”¨é€”**ï¼šç»™ç ”ç©¶å‘˜ / å›æµ‹è„šæœ¬æ‰¹é‡ç”ŸæˆæŒ‡å®šæ—¶é—´æ®µå†…çš„è¡Œæƒ…ã€ç›¸å…³æ€§ã€æŠ€æœ¯æŒ‡æ ‡ã€
  Markdown æŠ¥å‘Šç­‰æœ¬åœ°ç¼“å­˜æ–‡ä»¶ã€‚å®ƒä»¥ `start_date` ä¸ºèµ·ç‚¹ï¼Œä»¥ `end_date` ä¸ºç»ˆç‚¹ï¼Œå½“`end_date`
  ä¸æŒ‡å®šæ—¶ï¼Œè‡ªåŠ¨ä½¿ç”¨å½“å‰æ—¥æœŸdatetime.datetime.now()ä½œä¸ºend_date
  å¹¶ç”Ÿæˆå®Œæ•´çš„ `CSV + Markdown` èµ„æ–™åº“ä¾›äººå·¥/AIè¿›è¡Œè‚¡ç¥¨åˆ†æã€‚
* **æ•°æ®èŒƒå›´**ï¼šçŸ­æœŸè‚¡ç¥¨æ•°æ®ç”± `start_date`/`end_date` æ§åˆ¶ï¼Œé•¿æœŸä»·æ ¼ï¼ˆç”¨äºæœˆåº¦å¹³å‡ã€
  æŠ€æœ¯æŒ‡æ ‡ç­‰ï¼‰ç”± `long_term_start_date` åˆ°â€œend_dateâ€è¦†ç›–ï¼›å½“è°ƒç”¨è€…æœªæŒ‡å®š start_date æ—¶ï¼Œ
  é»˜è®¤å–æœ€è¿‘7ä¸ªäº¤æ˜“æ—¥çš„çŸ­æœŸæ•°æ®ï¼Œå¹¶é»˜è®¤æ‹‰å–æœ€è¿‘3å¹´çš„æ”¶ç›˜ä»·åºåˆ—
* **ç¼“å­˜ç­–ç•¥**ï¼šæ‰€æœ‰æ•°æ®å†™å…¥ `data/stock_name_symbol/analysis/` ç­‰ç›®å½•ï¼Œå¹¶ä¾ç…§end_date
  å‘½åæ–‡ä»¶ï¼ˆä¾‹å¦‚ `.../price_dynamics_summary_20251031.csv`ï¼‰ã€‚ç¼“å­˜ç³»ç»Ÿé‡‡ç”¨æ™ºèƒ½æ—¥æœŸå¤„ç†æœºåˆ¶ï¼š
  - **äº¤æ˜“æ—¥è¯†åˆ«**ï¼šä½¿ç”¨ `get_latest_trading_day()` å‡½æ•°è‡ªåŠ¨è¯†åˆ«æœ€è¿‘çš„äº¤æ˜“æ—¥ï¼Œç¡®ä¿ç¼“å­˜æ–‡ä»¶
    å‘½ååŸºäºå®é™…äº¤æ˜“æ—¥è€Œéæ—¥å†æ—¥æœŸ
  - **èŠ‚å‡æ—¥å¤„ç†**ï¼šå½“ `end_date` è½åœ¨å‘¨æœ«æˆ–èŠ‚å‡æ—¥æ—¶ï¼Œç³»ç»Ÿè‡ªåŠ¨ä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥çš„ç¼“å­˜æ•°æ®
  - **ç¼“å­˜éªŒè¯**ï¼šé€šè¿‡ `manage_cache_with_cleanup()` å‡½æ•°éªŒè¯ç¼“å­˜æ•°æ®çš„æ—¶é—´èŒƒå›´ï¼Œæ”¯æŒå®¹é”™å¤©æ•°
    ï¼ˆé»˜è®¤3å¤©ï¼‰æ¥å¤„ç†æ—¥æœŸè¾¹ç•Œæƒ…å†µ
  - **è¿‡æœŸæ¸…ç†**ï¼šè‡ªåŠ¨æ¸…ç†éå½“æ—¥çš„å†å²ç¼“å­˜æ–‡ä»¶ï¼Œä¿æŒå­˜å‚¨ç©ºé—´é«˜æ•ˆ
  - **æ™ºèƒ½åˆ·æ–°**ï¼šå½“è¯·æ±‚æ—¥æœŸä¸ç¼“å­˜æ—¥æœŸä¸åŒ¹é…æ—¶ï¼Œç³»ç»Ÿä¼šæ£€æŸ¥æ—¥æœŸå·®å¼‚ï¼šå¦‚æœç¼“å­˜æ—¥æœŸæ˜¯å‘¨äº”æˆ–èŠ‚å‡æ—¥å‰
    ä¸€å¤©ï¼Œè€Œå½“å‰ `end_date` æ˜¯å‘¨æœ«æˆ–èŠ‚å‡æ—¥ï¼Œåˆ™æ²¿ç”¨ç°æœ‰ç¼“å­˜ï¼›å¦åˆ™é‡æ–°è·å–æ•°æ®
* **ä¸»è¦å…¥å£**ï¼š`stock_price_dynamics_summarizer(...)`ã€‚è¯¥å‡½æ•°æ¥å—ç›®æ ‡è‚¡ç¥¨ã€æŒ‡æ•°é…ç½®ã€
  æ—¥æœŸèŒƒå›´ã€ç›¸ä¼¼è‚¡ç¥¨æ•°é‡ç­‰å‚æ•°ï¼Œè¿”å›ä¸€ä¸ªä»¥ç›®æ ‡è‚¡ç¥¨ä¸º key çš„ç»“æœå­—å…¸ï¼Œå…¶ä¸­åŒ…å«ï¼š
  - `summary`ï¼šç´¯è®¡æ”¶ç›Š/å¤æ™®/æ³¢åŠ¨ç‡/æœ€å¤§å›æ’¤ç­‰æ±‡æ€»è¡¨
  - `correlation`ï¼šä¸æŒ‡æ•°ã€ç›¸ä¼¼è‚¡ç¥¨çš„ç›¸å…³ç³»æ•°çŸ©é˜µ
  - `price_data`ï¼šåˆå¹¶åçš„æ—¶åºæ•°æ®ï¼ˆæ”¶ç›˜ä»·ã€æˆäº¤é‡ç­‰ï¼‰
  - `technical_indicators`ï¼šMACDã€RSIã€æ³¢åŠ¨ç‡ç­‰æŒ‡æ ‡
  - `markdown_path`ï¼šè‡ªåŠ¨åˆæˆçš„ Markdown æŠ¥å‘Šè·¯å¾„ï¼ˆå­˜æ¡£äº `analysis/` ä¸
    `data/0_transaction_package/`ï¼‰
  - `analysis_date`ï¼šç”ŸæˆæŠ¥å‘Šæ—¶çš„çœŸå®æ—¥æœŸï¼ˆYYYYMMDDï¼‰
* **è®¾è®¡è§„åˆ™ / é™åˆ¶**ï¼š
  1. **æ•°æ®å®Œæ•´æ€§ä¼˜å…ˆ**ï¼šè‹¥ç¼“å­˜ä¸è¶³ï¼Œä¼šè‡ªåŠ¨ä¸‹å‘ API è¯·æ±‚ï¼Œå¹¶ä¸ºä¸åŒå¸‚åœºè‡ªåŠ¨é€‰å–æ­£ç¡®çš„
     Akshare æ¥å£ã€‚å¤±è´¥ä¼šè®°å½•æ—¥å¿—ï¼Œä½†ä»è¿”å›å·²æœ‰å†…å®¹ã€‚
  2. **æ— â€œå†å² todayâ€ æ¦‚å¿µ**ï¼šstart/end/long_term å‚æ•°å®Œå…¨ç”±è°ƒç”¨è€…æ§åˆ¶ã€‚è‹¥ end_date è¶…å‡º
     å½“å‰atetime.datetime.now()çœŸå®æ—¥æœŸï¼Œåˆ™è‡ªåŠ¨æˆªæ–­ä¸ºä»Šå¤©å¹¶è®°å½•è­¦å‘Šã€‚
  3. **Markdown åˆå¹¶**ï¼š`merge_csv_to_markdown` ä¼šèšåˆ CSV æˆå•ä»½æŠ¥å‘Šï¼Œå†…å®¹åŒ…æ‹¬ä»·æ ¼åŠ¨æ€
     æ€»ç»“ã€ç›¸å…³æ€§çŸ©é˜µã€æŠ€æœ¯æŒ‡æ ‡è¡¨ç­‰ï¼›å‘½åè§„åˆ™ä¸º
     `stock_name_symbol_è‚¡ç¥¨åˆ†ææŠ¥å‘Š_end_date.md`ã€‚
  4. **ç›¸ä¼¼è‚¡ç¥¨**ï¼šåŸºäº `get_similar_stocks` çš„é…ç½®/è§„åˆ™ï¼Œè‹¥æ— å¯ç”¨è®°å½•åˆ™è¿”å›ç©ºåˆ—è¡¨ã€‚
* **æ‰©å±•æŒ‡å¼•**ï¼š
  - å¦‚éœ€æ–°å¢æŒ‡æ ‡ï¼Œåªéœ€åœ¨ `_PRICE_MODULE.calculate_metrics` æˆ–
    `calculate_technical_indicators` ä¸­æ‰©å±•åˆ—ï¼Œå†åœ¨ Markdown æ±‡æ€»é€»è¾‘é‡Œè¿½åŠ ã€‚
  - è‹¥æƒ³åœ¨ MCP å·¥å…·ä¸­æ–½åŠ â€œtoday_timeâ€æˆ–â€œstart_date é™åˆ¶â€ç­‰å›æµ‹è§„åˆ™ï¼Œåº”åœ¨å·¥å…·å±‚å®ç°ï¼Œ
    ä¸åœ¨æœ¬è„šæœ¬ä¸­å¼•å…¥ï¼Œä»¥ä¾¿è¯¥è„šæœ¬ç»§ç»­æœåŠ¡äºçœŸå®æ—¶é—´çš„äººå·¥åˆ†æã€‚

ä»¥ä¸‹ä»£ç éƒ¨åˆ†ä¿æŒåŸæœ‰ç»“æ„ï¼šå…ˆå°è£… API æŠ“å–/ç¼“å­˜å·¥å…·ï¼Œå†å®ç°æ‰¹é‡åˆ†æã€Markdown ç”Ÿæˆã€
ä»¥åŠå‘½ä»¤è¡Œå…¥å£ã€‚"""

import json
import pandas as pd
from pandas.tseries.offsets import BDay
import numpy as np
from datetime import datetime
from pathlib import Path
import os
import argparse
from typing import List, Dict, Any, Optional, Tuple, Iterable
import shutil  # å¯¼å…¥shutilæ¨¡å—ç”¨äºæ–‡ä»¶å¤åˆ¶
import logging
import sys

from indicator_library import IndicatorBatchRequest, IndicatorLibrary, IndicatorSpec
from indicator_library.calculators.risk import return_metrics_indicator
from indicator_library.gateways import DataFrameGateway

# ä»utlityåŒ…å¯¼å…¥ç¼“å­˜ä¸è‚¡ç¥¨å·¥å…·
from utlity import (
    ensure_stock_subdir,
    get_latest_trading_day,
    get_next_trading_day,
    is_trading_day,
    parse_symbol,
    resolve_base_dir,
    SymbolInfo,
    is_cn_etf,
)
from utlity.get_similar_stocks import get_similar_stocks

# è®¾ç½®pandasé€‰é¡¹ä»¥é¿å…FutureWarning
pd.set_option('future.no_silent_downcasting', True)

# APIè°ƒç”¨å»¶è¿Ÿé…ç½®ï¼ˆç§’ï¼‰
API_DELAY = 0.5  # å»ºè®®1ç§’å»¶è¿Ÿï¼Œæ—¢èƒ½é¿å…é¢‘ç‡é™åˆ¶åˆä¸ä¼šè¿‡åº¦å½±å“æ€§èƒ½
# é…ç½®ç‹¬ç«‹çš„æ—¥å¿—ç³»ç»Ÿï¼ˆéMCPå·¥å…·ï¼‰
LOG_ENV_KEY = "STOCK_ANALYZER_LOG_FILE"


def setup_main_logger(log_level=logging.INFO):
    """é…ç½®ä¸»è„šæœ¬æ—¥å¿—ç³»ç»Ÿï¼Œä¿è¯æ•´ä¸ªè¿›ç¨‹å¤ç”¨åŒä¸€ä¸ªæ—¥å¿—æ–‡ä»¶ã€‚"""

    logger = logging.getLogger('StockAnalyzer')
    if logger.handlers:
        logger.setLevel(log_level)
        return logger

    log_dir = Path('logs') / 'main_scripts' / 'StockAnalyzer'
    log_dir.mkdir(parents=True, exist_ok=True)

    existing_path = os.environ.get(LOG_ENV_KEY)
    if existing_path:
        log_filename = Path(existing_path)
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_filename = (log_dir / f'stock_analyzer_{timestamp}.log').resolve()
        os.environ[LOG_ENV_KEY] = str(log_filename)

    logger.setLevel(log_level)
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    file_handler = logging.FileHandler(log_filename, encoding='utf-8')
    file_handler.setLevel(log_level)
    file_handler.setFormatter(formatter)

    console_handler = logging.StreamHandler()
    console_handler.setLevel(log_level)
    console_handler.setFormatter(formatter)

    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    logger.info(f"ä¸»è„šæœ¬æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œæ—¥å¿—æ–‡ä»¶: {log_filename}")
    return logger

# åˆå§‹åŒ–å…¨å±€logger
logger = setup_main_logger()

class DataValidationError(RuntimeError):
    """å…³é”®è¡Œæƒ…æ•°æ®ç¼ºå¤±æˆ–æ— æ•ˆæ—¶æŠ›å‡ºï¼Œé¿å…äº§ç”Ÿé”™è¯¯ç»“è®ºã€‚"""
    pass


RESERVED_CACHE_FILES = {".cache_registry_meta.json"}


def cleanup_output_directory(directory: Path, keep: Optional[Iterable[str]] = None) -> None:
    """æ¸…ç†è¾“å‡ºç›®å½•ï¼Œåªä¿ç•™ç¼“å­˜å…ƒæ•°æ®æ–‡ä»¶ã€‚"""
    target = Path(directory)
    target.mkdir(parents=True, exist_ok=True)
    keep_names = set(RESERVED_CACHE_FILES)
    if keep:
        keep_names.update(keep)
    for entry in target.iterdir():
        if entry.name in keep_names:
            continue
        try:
            if entry.is_dir():
                shutil.rmtree(entry, ignore_errors=True)
            else:
                entry.unlink(missing_ok=True)
        except Exception as exc:  # pragma: no cover - å®¹é”™æ—¥å¿—
            logger.warning("æ¸…ç†ç›®å½• %s æ—¶è·³è¿‡ %s: %s", target, entry, exc)




def calculate_technical_indicators(
    data: pd.DataFrame,
    symbolInfo: SymbolInfo,
    similar_names: List[str] | None = None,
    etf_name: str | None = None,
    period: int | None = None,
    long_term_period: int | None = None,
    short_term_start_date: str | None = None,
) -> pd.DataFrame:
    """ç”Ÿæˆç›®æ ‡è‚¡ç¥¨çš„æ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡å’Œç»Ÿè®¡æ•°æ®ï¼ˆç”±å…±äº«æŒ‡æ ‡åº“é©±åŠ¨ï¼‰ã€‚"""
    _ = similar_names, period, long_term_period
    if data.empty:
        return pd.DataFrame()
    working = data.copy()
    if not isinstance(working.index, pd.DatetimeIndex):
        working.index = pd.to_datetime(working.index)
    working = working.sort_index()
    target_prefix = (symbolInfo.stock_name or symbolInfo.symbol).strip() or symbolInfo.symbol
    target_close_col = f"{target_prefix}_æ”¶ç›˜"
    if target_close_col not in working.columns:
        logger.error("ç›®æ ‡åˆ— %s ä¸å­˜åœ¨ï¼Œæ— æ³•è®¡ç®—æŠ€æœ¯æŒ‡æ ‡", target_close_col)
        return pd.DataFrame()

    gateway = DataFrameGateway(working)
    indicator_lib = IndicatorLibrary(gateway=gateway, logger=logger)
    start_date = working.index.min().date()
    end_date = working.index.max().date()

    try:
        batch = indicator_lib.calculate(
            IndicatorBatchRequest(
                symbolInfo=symbolInfo,
                start_date=start_date,
                end_date=end_date,
                specs=[
                    IndicatorSpec(
                        name="price_snapshot",
                        params={"symbolInfo": symbolInfo, "include_turnover": True},
                        alias="price_snapshot",
                    ),
                    IndicatorSpec(
                        name="pct_change",
                        params={"column_name": "æ¶¨è·Œå¹…(%)"},
                        alias="pct_change",
                    ),
                    IndicatorSpec(
                        name="macd",
                        params={"column_name": "MACD"},
                        alias="macd",
                    ),
                    IndicatorSpec(
                        name="rsi",
                        params={"period": 14, "column_name": "RSI(14)"},
                        alias="rsi",
                    ),
                ],
                price_fields=["æ”¶ç›˜", "æˆäº¤é‡", "æˆäº¤é¢", "æ¢æ‰‹ç‡"],
            )
        )
    except Exception as exc:
        logger.error("è°ƒç”¨ IndicatorLibrary è®¡ç®—æŠ€æœ¯æŒ‡æ ‡å¤±è´¥: %s", exc)
        return pd.DataFrame()

    # åˆå¹¶æŠ€æœ¯æŒ‡æ ‡è¡¨æ ¼
    tables: List[pd.DataFrame] = []
    for key in ("price_snapshot", "pct_change", "macd", "rsi"):
        frame = batch.tabular.get(key)
        if isinstance(frame, pd.DataFrame) and not frame.empty:
            tables.append(frame)
    technical_indicators = pd.concat(tables, axis=1) if tables else pd.DataFrame(index=working.index)

    # æ·»åŠ ETFæ”¶ç›˜ä»·ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if etf_name:
        etf_col = f"{etf_name}_æ”¶ç›˜"
        if etf_col in working.columns:
            technical_indicators[f"ETF({etf_name})æ”¶ç›˜ä»·"] = working[etf_col]
        else:
            matches = [col for col in working.columns if col.endswith('_æ”¶ç›˜') and etf_name in col]
            if matches:
                technical_indicators[f"ETF({etf_name})æ”¶ç›˜ä»·"] = working[matches[0]]

    # å¤„ç†æŠ€æœ¯æŒ‡æ ‡æ•°æ®
    for col in technical_indicators.columns:
        if col != "æ”¶ç›˜ä»·" and (not etf_name or col != f"ETF({etf_name})æ”¶ç›˜ä»·"):
            technical_indicators[col] = technical_indicators[col].ffill().infer_objects(copy=False)

    # æ ¹æ®çŸ­æœŸå¼€å§‹æ—¥æœŸè¿‡æ»¤æ•°æ®
    if short_term_start_date:
        mask = technical_indicators.index >= pd.to_datetime(short_term_start_date)
        technical_indicators = technical_indicators.loc[mask]

    return technical_indicators

# ç›¸ä¼¼è‚¡ç¥¨ç®¡ç†è¾…åŠ©å‡½æ•°
LEGACY_SIMILAR_STOCKS_PATH = Path(__file__).resolve().with_name("similar_stocks.csv")


# ä¸»å‡½æ•°ï¼šè‚¡ç¥¨ä»·æ ¼åŠ¨æ€æ€»ç»“
def stock_price_dynamics_summarizer(
                                   symbolsInfo: List[SymbolInfo],
                                   index_symbolInfo: SymbolInfo,
                                   start_date: str = None, end_date: str = None,
                                   long_term_start_date: str = None,
                                   top_n_similar: int = 2,
                                   base_dir: str = 'data',
                                   force_refresh: bool = False,
                                   only_find_similar: bool = False,
                                   force_refresh_financials: bool = False) -> Dict[str, Any]:
    """ç”Ÿæˆç›®æ ‡è‚¡ç¥¨çš„ä»·æ ¼åŠ¨æ€æ€»ç»“
    
    å‚æ•°:
        symbolsInfo: è‚¡ç¥¨ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º SymbolInfo ç±»å‹
        index_info: æŒ‡æ•°ä¿¡æ¯å­—å…¸ï¼Œé”®ä¸ºæŒ‡æ•°ä»£ç ï¼Œå€¼ä¸ºæŒ‡æ•°åç§°
        start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼Œé»˜è®¤å– end_date å‘å‰7ä¸ªäº¤æ˜“æ—¥
        end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼Œé»˜è®¤ä¸ºä»Šå¤©ï¼ˆè‡ªåŠ¨æˆªæ–­åˆ°æœ€è¿‘äº¤æ˜“æ—¥ï¼‰
        long_term_start_date: é•¿æœŸæ”¶ç›˜ä»·çš„å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼Œé»˜è®¤ä¸ºNoneï¼ˆä½¿ç”¨ end_date å‘å‰ä¸‰å¹´ï¼‰
        index_code: æŒ‡æ•°ä»£ç ï¼Œé»˜è®¤ä¸ºä¸Šè¯æŒ‡æ•°
        top_n_similar: æ¯åªè‚¡ç¥¨è·å–çš„ç›¸ä¼¼è‚¡ç¥¨æ•°é‡
        base_dir: æ•°æ®å­˜å‚¨çš„åŸºç¡€ç›®å½•
        force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°æ•°æ®ï¼Œä¸ä½¿ç”¨ç¼“å­˜
        only_find_similar: å¦‚æœä¸ºTrueï¼Œåˆ™åªæ‰§è¡Œç›¸ä¼¼è‚¡ç¥¨æŸ¥æ‰¾ï¼Œä¸è·å–æ•°æ®å’Œè®¡ç®—æŒ‡æ ‡
        
    è¿”å›:
        åŒ…å«å„ç§åˆ†æç»“æœçš„å­—å…¸ï¼Œé”®ä¸ºè‚¡ç¥¨ä»£ç 
    """
    # å¤„ç†æ—¥æœŸå‚æ•°
    today = pd.Timestamp(datetime.now().date().strftime("%Y-%m-%d"))
    if end_date is None:
        end_dt = today
    else:
        end_dt = pd.to_datetime(end_date, format="%Y-%m-%d", errors="coerce")
    
    if start_date is None:
        start_dt = end_dt - BDay(14)
    else:
        start_dt = pd.to_datetime(start_date, format="%Y-%m-%d", errors="coerce")

    if start_dt > end_dt:
        raise ValueError("start_date éœ€è¦æ—©äºæˆ–ç­‰äº end_date")
    
    # å¤„ç†é•¿æœŸæ—¥æœŸå‚æ•°
    if long_term_start_date is None:
        long_term_start_dt = end_dt - pd.DateOffset(years=3)
    else:
        long_term_start_dt = pd.to_datetime(
            long_term_start_date, format="%Y-%m-%d", errors="coerce"
        )
    if pd.isna(long_term_start_dt):
        raise ValueError("long_term_start_date æ— æ³•è§£æä¸ºæœ‰æ•ˆæ—¥æœŸ")
    if getattr(long_term_start_dt, "tzinfo", None) is not None:
        long_term_start_dt = long_term_start_dt.tz_convert(None)
    long_term_start_dt = pd.Timestamp(long_term_start_dt)

    end_date = end_dt.strftime("%Y-%m-%d")
    start_date = start_dt.strftime("%Y-%m-%d")
    long_term_start_date = long_term_start_dt.strftime("%Y-%m-%d")
    analysis_date_str = datetime.now().strftime("%Y-%m-%d")
    
    # è®¡ç®—é•¿æœŸå’ŒçŸ­æœŸçš„æ—¶é—´è·¨åº¦ï¼ˆä»¥å¤©ä¸ºå•ä½ï¼‰
    short_term_days = (pd.to_datetime(end_date) - pd.to_datetime(start_date)).days
    long_term_days = (pd.to_datetime(end_date) - pd.to_datetime(long_term_start_date)).days
    
    # è½¬æ¢ä¸ºäº¤æ˜“æ—¥æ•°é‡ï¼ˆç²—ç•¥ä¼°è®¡ï¼šä¸€å¹´çº¦250ä¸ªäº¤æ˜“æ—¥ï¼‰
    short_term_period = int(short_term_days * 250 / 365)
    long_term_period = int(long_term_days * 250 / 365)
    
    
    # å­˜å‚¨æ¯åªè‚¡ç¥¨çš„åˆ†æç»“æœ
    results = {}
    
    # ä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨æ‰€æœ‰è‚¡ç¥¨çš„åç§°ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç›®æ ‡è‚¡ç¥¨ã€æŒ‡æ•°å’Œç›¸ä¼¼è‚¡ç¥¨
    all_stock_names = {}
    all_stock_names[index_symbolInfo.symbol] = index_symbolInfo.stock_name

    # å¯¹æ¯åªç›®æ ‡è‚¡ç¥¨è¿›è¡Œåˆ†æ
    for symbolInfo in symbolsInfo:
        logger.info(f"\nåˆ†æè‚¡ç¥¨: {symbolInfo.stock_name} ({symbolInfo.symbol})")
        etf_mode = is_cn_etf(symbolInfo) or symbolInfo.market == "CN_INDEX"
        if etf_mode:
            logger.info("æ£€æµ‹åˆ° ETF/æŒ‡æ•°æ ‡çš„ï¼Œè·³è¿‡ç›¸ä¼¼è‚¡ç¥¨ä¸ç›¸å…³æ€§åˆ†æï¼Œä»…è¾“å‡ºè‡ªèº«æŒ‡æ ‡ã€‚")
        
        all_stock_names[symbolInfo.symbol] = symbolInfo.stock_name
        effective_trading_day = get_latest_trading_day(
            end_dt.date(), symbolInfo.calendar, logger=logger
        )
        report_date = effective_trading_day.strftime("%Y%m%d")

        similar_stocks_symbolInfo: List[SymbolInfo] = []
        similar_names: List[str] = []
        similar_symbols: List[str] = []
        if not etf_mode:
            similar_stocks_info = get_similar_stocks(symbolInfo, base_dir)[:top_n_similar]
            structured_similars: List[Tuple[SymbolInfo, str]] = []
            for similar_stock_info in similar_stocks_info:
                code = similar_stock_info.get("code")
                name = (similar_stock_info.get("name") or "").strip()
                if not code:
                    logger.warning("ç›¸ä¼¼è‚¡ç¥¨[%s]ç¼ºå°‘ä»£ç ï¼Œè·³è¿‡: %s", name or "Unknown", similar_stock_info)
                    continue
                try:
                    similar_info = parse_symbol(code)
                except Exception as exc:
                    logger.warning("è§£æç›¸ä¼¼è‚¡ç¥¨ä»£ç  %s å¤±è´¥: %s", code, exc)
                    continue
                structured_similars.append((similar_info, name or similar_info.symbol))
            similar_stocks_symbolInfo = [item[0] for item in structured_similars]
            similar_names = [item[1] for item in structured_similars]
            similar_symbols = [item[0].symbol for item in structured_similars]

            # æ›´æ–°è‚¡ç¥¨åç§°å­—å…¸
            for info, name in zip(similar_stocks_symbolInfo, similar_names):
                all_stock_names[info.symbol] = name

            logger.info(f"ç›¸ä¼¼è‚¡ç¥¨: {[f'{name}({info.symbol})' for name, info in zip(similar_names, similar_stocks_symbolInfo)]}")
        
        # å¦‚æœåªéœ€è¦æŸ¥æ‰¾ç›¸ä¼¼è‚¡ç¥¨ï¼Œåˆ™è·³è¿‡åç»­æ­¥éª¤
        if only_find_similar:
            results[symbolInfo.symbol] = {
                'similar_stocks': similar_symbols,
                'similar_names': similar_names,
            }
            continue

        # ä½¿ç”¨SharedDataAccessè·å–æ•°æ®
        from shared_data_access.data_access import SharedDataAccess
        
        data_access = SharedDataAccess(
            base_dir=base_dir,
            logger=logger
        )
        
        # è·å–ç›®æ ‡è‚¡ç¥¨/æŒ‡æ•°æ•°æ®
        logger.info(f"æ­£åœ¨è·å–{symbolInfo.stock_name} {symbolInfo.symbol}æ•°æ®...")
        target_dataset = data_access.prepare_dataset(
            symbolInfo=symbolInfo,
            as_of_date=end_date,
            force_refresh=force_refresh,
            force_refresh_financials=force_refresh_financials,
        )
        target_df = target_dataset.prices.frame.copy()
        target_df = target_df.sort_index()
        
        # è·å–æŒ‡æ•°æ•°æ®ï¼ˆå¦‚æœç›®æ ‡ä¸æ˜¯æŒ‡æ•°ï¼‰
        index_df = pd.DataFrame()
        if symbolInfo.market != "CN_INDEX":
            logger.info(f"æ­£åœ¨è·å–æŒ‡æ•°{index_symbolInfo.stock_name} {index_symbolInfo.symbol}æ•°æ®...")
            index_dataset = data_access.prepare_dataset(
                symbolInfo=index_symbolInfo,
                as_of_date=end_date,
                force_refresh=force_refresh,
                force_refresh_financials=force_refresh_financials,
            )
            index_df = index_dataset.prices.frame.copy()
            index_df = index_df.sort_index()
        
        # è·å–ç›¸ä¼¼è‚¡ç¥¨æ•°æ®
        prefixed_similar_dfs: List[pd.DataFrame] = []
        column_symbol_map: Dict[str, str] = {}
        etf_display_name: Optional[str] = None
        if similar_stocks_symbolInfo:
            logger.info("æ­£åœ¨è·å–ç›¸ä¼¼è‚¡ç¥¨æ•°æ®...")
            for similar_info, similar_name in zip(similar_stocks_symbolInfo, similar_names):
                try:
                    similar_dataset = data_access.prepare_dataset(
                        symbolInfo=similar_info,
                        as_of_date=end_date,
                        force_refresh=force_refresh,
                        force_refresh_financials=force_refresh_financials,
                    )
                    similar_df = similar_dataset.prices.frame.copy().sort_index()
                except Exception as e:
                    logger.warning(
                        "è·å–ç›¸ä¼¼è‚¡ç¥¨%sçš„å®Œæ•´æ•°æ®å¤±è´¥ï¼Œå°è¯•ä»…åŠ è½½ä»·æ ¼: %s",
                        similar_info.symbol,
                        e,
                    )
                    try:
                        price_bundle = data_access._load_price_bundle(  # type: ignore[attr-defined]
                            similar_info,
                            pd.to_datetime(end_date),
                        )
                        similar_df = price_bundle.frame.copy().sort_index()
                    except Exception as price_exc:
                        logger.warning(
                            "ç›¸ä¼¼è‚¡ç¥¨%sä»·æ ¼æ•°æ®åŠ è½½å¤±è´¥ï¼Œè·³è¿‡: %s",
                            similar_info.symbol,
                            price_exc,
                        )
                        continue
                if similar_df.empty:
                    continue
                similar_df.index = pd.to_datetime(similar_df.index)
                similar_df = similar_df[similar_df.index >= long_term_start_dt]
                safe_name = similar_name.strip()
                prefixed_df = similar_df.add_prefix(f"{safe_name}_")
                prefixed_similar_dfs.append(prefixed_df)
                for col in prefixed_df.columns:
                    column_symbol_map[col] = similar_info.symbol
                if etf_display_name is None and "ETF" in safe_name.upper():
                    etf_display_name = safe_name
        
        # ä¸ºæ¯ä¸ªæ•°æ®æºçš„åˆ—æ·»åŠ å”¯ä¸€å‰ç¼€ï¼Œé¿å…åˆ—åå†²çª
        # ç›®æ ‡è‚¡ç¥¨æ•°æ®
        if not target_df.empty:
            target_df.index = pd.to_datetime(target_df.index)
            target_df = target_df[target_df.index >= long_term_start_dt]
            target_prefix = symbolInfo.stock_name.strip()
            target_df = target_df.add_prefix(f"{target_prefix}_")
            for col in target_df.columns:
                column_symbol_map[col] = symbolInfo.symbol
        
        # æŒ‡æ•°æ•°æ®
        if not index_df.empty:
            index_df.index = pd.to_datetime(index_df.index)
            index_df = index_df[index_df.index >= long_term_start_dt]
            index_prefix = index_symbolInfo.stock_name.strip()
            index_df = index_df.add_prefix(f"{index_prefix}_")
            for col in index_df.columns:
                column_symbol_map[col] = index_symbolInfo.symbol
        
        # ç›¸ä¼¼è‚¡ç¥¨æ•°æ®
        similar_dfs = pd.concat(prefixed_similar_dfs, axis=1) if prefixed_similar_dfs else pd.DataFrame()
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        all_data = pd.concat([target_df, index_df, similar_dfs], axis=1)
        
        # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ - ä»…ä½¿ç”¨æ”¶ç›˜ä»·åˆ—
        close_cols = [
            col for col in all_data.columns
            if col.endswith('_æ”¶ç›˜')
        ]
        close_df = all_data[close_cols].copy() if close_cols else pd.DataFrame()
        
        daily_returns = close_df.pct_change(fill_method=None) if not close_df.empty else pd.DataFrame()
        correlation_matrix = (
            daily_returns.corr(method='pearson', min_periods=30)
            if not daily_returns.empty else pd.DataFrame()
        )
        if etf_mode:
            correlation_matrix = pd.DataFrame()
        
        # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
        technical_indicators = calculate_technical_indicators(
            all_data,
            symbolInfo=symbolInfo,
            similar_names=similar_names,
            etf_name=etf_display_name,
            period=short_term_period,
            long_term_period=long_term_period,
            short_term_start_date=start_date
        )
        
        # è®¡ç®—æ¯ä¸ªè‚¡ç¥¨å’ŒæŒ‡æ•°çš„æŒ‡æ ‡ - ä»…ä½¿ç”¨æ”¶ç›˜ä»·åˆ—
        metrics: Dict[str, Dict[int, Dict[str, float]]] = {}
        symbol_close_col: Dict[str, str] = {}
        for column in close_cols:
            symbol_key = column_symbol_map.get(column)
            if not symbol_key:
                continue
            symbol_close_col.setdefault(symbol_key, column)
            series = close_df[column].dropna()
            if series.empty:
                continue
            metric_result = return_metrics_indicator(
                pd.DataFrame({"æ”¶ç›˜": series}),
                windows=(63, 126, 252),
            )
            if metric_result:
                metrics[symbol_key] = metric_result
        
        # æ‰€æœ‰è‚¡ç¥¨ä»£ç ï¼ˆåŒ…æ‹¬ç›®æ ‡è‚¡ç¥¨å’ŒæŒ‡æ•°ï¼‰
        index_symbol_key = f'{index_symbolInfo.symbol}'
        if etf_mode:
            all_symbols = [symbolInfo.symbol]
        elif symbolInfo.market == "CN_INDEX" and symbolInfo.symbol == index_symbolInfo.symbol:
            all_symbols = [symbolInfo.symbol] + similar_symbols
        else:
            all_symbols = [symbolInfo.symbol, index_symbol_key] + similar_symbols
        
        # ä¸ºä¸åŒæ—¶é—´æ®µåˆ›å»ºç»“æœDataFrame
        period_results = {}
        
        for period in [63, 126, 252]:
            period_months = period // 21
            
            # åˆ›å»ºå¸¦æœ‰è‚¡ç¥¨åç§°å’Œä»£ç çš„ç´¢å¼•
            index_with_names = [
                f"{all_stock_names.get(s, s.split('.')[0])}({s})" 
                for s in all_symbols
            ]
            
            # åˆå§‹åŒ–è¯¥æ—¶é—´æ®µçš„DataFrame
            period_df = pd.DataFrame(index=index_with_names)
            
            metric_column_map = {
                "ç´¯è®¡æ”¶ç›Šç‡(%)": f"{period_months}ä¸ªæœˆç´¯è®¡æ”¶ç›Šç‡(%)",
                "å¤æ™®æ¯”ç‡": f"{period_months}ä¸ªæœˆå¤æ™®æ¯”ç‡",
                "å¹´åŒ–æ³¢åŠ¨ç‡(%)": f"{period_months}ä¸ªæœˆå¹´åŒ–æ³¢åŠ¨ç‡(%)",
                "æœ€å¤§å›æ’¤(%)": f"{period_months}ä¸ªæœˆæœ€å¤§å›æ’¤(%)",
            }
            for metric_key, column_name in metric_column_map.items():
                period_df[column_name] = [
                    metrics.get(s, {}).get(period, {}).get(metric_key, 0.0)
                    for s in all_symbols
                ]
            
            period_results[f"{period_months}m"] = period_df
        
        # åˆå¹¶æ‰€æœ‰æ—¶é—´æ®µçš„ç»“æœ
        summary_df = pd.concat(period_results.values(), axis=1)
        
        # æ·»åŠ ç›¸å…³æ€§æŒ‡æ ‡
        correlations = []
        target_close_col = symbol_close_col.get(symbolInfo.symbol)
        for s in all_symbols:
            compare_col = symbol_close_col.get(s)
            if s == symbolInfo.symbol:
                correlations.append(1.0)
            elif (
                target_close_col
                and compare_col
                and target_close_col in correlation_matrix.columns
                and compare_col in correlation_matrix.columns
            ):
                correlations.append(correlation_matrix.loc[target_close_col, compare_col])
            else:
                correlations.append(np.nan)
        if not etf_mode:
            summary_df['ä¸ç›®æ ‡è‚¡ç¥¨ç›¸å…³ç³»æ•°'] = correlations
        
        # æ ¼å¼åŒ–
        for col in summary_df.columns:
            if any(keyword in col for keyword in ('æ”¶ç›Šç‡', 'æ³¢åŠ¨ç‡', 'å›æ’¤')):
                summary_df[col] = summary_df[col].round(1)
            elif 'å¤æ™®æ¯”ç‡' in col or 'ç›¸å…³ç³»æ•°' in col:
                # ä¿æŒåŸæœ‰çš„æ•°å€¼åˆ—å¤„ç†é€»è¾‘
                summary_df[col] = summary_df[col].round(3)
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_dir = ensure_stock_subdir(symbolInfo, "analysis", base_dir)
        cleanup_output_directory(analysis_dir)

        summary_file = analysis_dir / f"price_dynamics_summary_{report_date}.csv"
        summary_df.to_csv(summary_file)

        corr_file = analysis_dir / f"correlation_matrix_{report_date}.csv"
        if not correlation_matrix.empty:
            correlation_matrix.to_csv(corr_file)
        elif corr_file.exists():
            corr_file.unlink(missing_ok=True)

        target_close_col = symbol_close_col.get(symbolInfo.symbol)
        long_term_price_data = pd.DataFrame()
        price_file = None
        if target_close_col and target_close_col in target_df.columns:
            long_term_price_data = target_df[[target_close_col]].copy()
            long_term_price_data.rename(columns={target_close_col: 'æ”¶ç›˜ä»·'}, inplace=True)
            price_file = analysis_dir / f"close_price_{report_date}.csv"
            if not long_term_price_data.empty:
                long_term_price_data['æ”¶ç›˜ä»·'] = long_term_price_data['æ”¶ç›˜ä»·'].apply(
                    lambda x: round(x, 3) if pd.notnull(x) else x
                )
                long_term_price_data.to_csv(price_file, float_format='%.2f')
                logger.info(f"é•¿æœŸæ”¶ç›˜ä»·æ•°æ®å·²ä¿å­˜åˆ°: {price_file}")
        else:
            logger.warning("æœªæ‰¾åˆ° %s çš„æ”¶ç›˜ä»·åˆ—ï¼Œæ— æ³•å¯¼å‡ºé•¿æœŸæ”¶ç›˜ä»·æ•°æ®", symbolInfo.symbol)

        tech_file = None
        if not technical_indicators.empty:
            tech_indicators = technical_indicators.copy()
            if 'æ”¶ç›˜ä»·' in tech_indicators.columns and len(tech_indicators.columns) > 1:
                tech_indicators = tech_indicators.drop(columns=['æ”¶ç›˜ä»·'])

            if not tech_indicators.empty:
                tech_file = analysis_dir / f"technical_indicators_{report_date}.csv"
                tech_indicators.index = pd.to_datetime(tech_indicators.index).strftime('%Y-%m-%d')
                valid_indicators = tech_indicators.dropna(axis=1, how='all')

                if not valid_indicators.empty:
                    for col in valid_indicators.columns:
                        valid_indicators.loc[:, col] = valid_indicators[col].apply(
                            lambda x: round(x, 3) if pd.notnull(x) else x
                        )
                    valid_indicators.to_csv(tech_file, na_rep='', float_format='%.2f')
                    logger.info(f"çŸ­æœŸæŠ€æœ¯æŒ‡æ ‡å·²ä¿å­˜åˆ°: {tech_file} (ä»{start_date}å¼€å§‹)")
                else:
                    logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„æŠ€æœ¯æŒ‡æ ‡å¯ä»¥ä¿å­˜")
                    
        logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {analysis_dir}")

        markdown_path = merge_csv_to_markdown(
            symbolInfo.symbol,
            symbolInfo.stock_name,
            analysis_dir,
            report_date,
            index_symbolInfo.symbol,
            index_symbolInfo.stock_name,
            start_date,
            long_term_start_date,
            is_etf=etf_mode,
        )

        # å°†ç»“æœæ·»åŠ åˆ°å­—å…¸
        results[symbolInfo.symbol] = {
            'summary': summary_df,
            'correlation': correlation_matrix,
            'price_data': all_data,
            'similar_stocks': similar_symbols,
            'similar_names': similar_names,
            'technical_indicators': technical_indicators,
            'markdown_path': markdown_path,
            'json_path': analysis_dir / f"{symbolInfo.stock_name}_{symbolInfo.symbol}_è‚¡ç¥¨åˆ†ææŠ¥å‘Š_{report_date}.json",
            'analysis_date': analysis_date_str,
            'report_date': report_date,
        }
    
    return results


# å°†å¤šä¸ªCSVæ–‡ä»¶åˆå¹¶ä¸ºä¸€ä¸ªMarkdownæ–‡ä»¶
def merge_csv_to_markdown(
    target_symbol: str,
    stock_name: str,
    analysis_dir: str,
    report_date: str,
    index_code: str = '000300',
    index_name: str = 'æ²ªæ·±300',
    start_date: str = None,
    long_term_start_date: str | None = None,
    is_etf: bool = False,
) -> str:
    """å°†åˆ†æç»“æœçš„æ‰€æœ‰CSVæ–‡ä»¶åˆå¹¶åˆ°ä¸€ä¸ªMarkdownæ–‡ä»¶ä¸­"""
    analysis_dir_path = Path(analysis_dir)
    base_data_dir = analysis_dir_path.parent.parent

    # æ£€æŸ¥ç›¸å…³æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    summary_file = analysis_dir_path / f"price_dynamics_summary_{report_date}.csv"
    corr_file = analysis_dir_path / f"correlation_matrix_{report_date}.csv"
    price_file = analysis_dir_path / f"close_price_{report_date}.csv"
    tech_file = analysis_dir_path / f"technical_indicators_{report_date}.csv"

    iso_report_date = f"{report_date[:4]}-{report_date[4:6]}-{report_date[6:]}"
    json_payload: Dict[str, Any] = {
        "meta": {
            "stock_name": stock_name,
            "symbol": target_symbol,
            "analysis_date": iso_report_date,
            "report_date": report_date,
            "short_term_start_date": start_date,
            "long_term_start_date": long_term_start_date,
            "index": {"code": index_code, "name": index_name},
        },
        "summary_table": [],
        "correlation_matrix": [],
        "technical_section": {
            "table": [],
            "latest_focus": None,
            "ma_indicators": {},
            "start_date": start_date,
        },
        "monthly_section": {},
    }
    extreme_price_section: Dict[str, Any] = {}
    
    # åˆ›å»ºMarkdownæ–‡ä»¶å†…å®¹
    md_content = f"# {stock_name}({target_symbol}) è‚¡ç¥¨åˆ†ææŠ¥å‘Š\n\n"
    md_content += f"åˆ†ææ—¥æœŸ: {iso_report_date}\n\n"
    
    # æ·»åŠ ä»·æ ¼åŠ¨æ€æ€»ç»“
    if summary_file.exists():
        try:
            summary_df = pd.read_csv(summary_file, index_col=0)
            summary_df.index.name = "è‚¡ç¥¨"
            if is_etf:
                md_content += "## ç›®æ ‡è‚¡ç¥¨ä»·æ ¼åŠ¨æ€æ€»ç»“(3ã€6ã€12ä¸ªæœˆçš„ç´¯è®¡å›æŠ¥ç‡ã€å¤æ™®æ¯”ç‡ã€æ³¢åŠ¨ç‡ã€æœ€å¤§å›æ’¤)\n\n"
            else:
                md_content += "## ç›®æ ‡è‚¡ç¥¨å’Œç›¸ä¼¼è‚¡ç¥¨ä»·æ ¼åŠ¨æ€æ€»ç»“(3ã€6ã€12ä¸ªæœˆçš„ç´¯è®¡å›æŠ¥ç‡ã€å¤æ™®æ¯”ç‡ã€æ³¢åŠ¨ç‡ã€æœ€å¤§å›æ’¤)\n\n"
            md_content += summary_df.to_markdown() + "\n\n"
            json_payload["summary_table"] = dataframe_to_table(
                summary_df, include_index=True, index_name="è‚¡ç¥¨"
            )
        except Exception as e:
            md_content += f"è¯»å–ä»·æ ¼åŠ¨æ€æ€»ç»“æ–‡ä»¶æ—¶å‡ºé”™: {e}\n\n"
    
    # æ·»åŠ ç›¸å…³æ€§çŸ©é˜µ
    if (not is_etf) and corr_file.exists():
        try:
            corr_df = pd.read_csv(corr_file, index_col=0)
            md_content += "## ç›®æ ‡è‚¡ç¥¨å’Œç›¸ä¼¼è‚¡ç¥¨ç›¸å…³æ€§çŸ©é˜µ\n\n"
            md_content += corr_df.round(3).to_markdown() + "\n\n"
            json_payload["correlation_matrix"] = dataframe_to_table(
                corr_df, include_index=True, index_name="symbol"
            )
        except Exception as e:
            md_content += f"è¯»å–ç›¸å…³æ€§çŸ©é˜µæ–‡ä»¶æ—¶å‡ºé”™: {e}\n\n"
    
    tech_section = json_payload["technical_section"]
    # æ·»åŠ æŠ€æœ¯æŒ‡æ ‡ - æ˜¾ç¤ºå…¨éƒ¨æ•°æ®ï¼Œå¹¶æ·»åŠ æ”¶ç›˜ä»·ä½œä¸ºç¬¬ä¸€åˆ—
    if tech_file.exists() and price_file.exists():
        try:
            # è¯»å–æ”¶ç›˜ä»·æ•°æ®
            price_df = pd.read_csv(price_file, index_col=0)
            price_df.index = pd.to_datetime(price_df.index)
            min_price_date = price_df.index.min() if not price_df.empty else None
            
            # è¯»å–æŠ€æœ¯æŒ‡æ ‡æ•°æ®
            tech_df = pd.read_csv(tech_file, index_col=0)
            tech_df.index = pd.to_datetime(tech_df.index)
            
            # è½¬æ¢ä¸ºæ ‡å‡†æ—¥æœŸæ ¼å¼
            price_df.index = price_df.index.strftime('%Y-%m-%d')
            tech_df.index = tech_df.index.strftime('%Y-%m-%d')
            
            # åªä¿ç•™æœ‰æ”¶ç›˜ä»·çš„æ—¥æœŸï¼ˆå³å®é™…äº¤æ˜“æ—¥ï¼‰
            valid_dates = [idx for idx in price_df.index if not pd.isna(price_df.loc[idx, 'æ”¶ç›˜ä»·'])]
            
            # åˆå¹¶æ•°æ® - ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼ï¼Œåªä½¿ç”¨æœ‰æ•ˆäº¤æ˜“æ—¥
            combined_df = pd.DataFrame(index=valid_dates) if valid_dates else pd.DataFrame()
            display_df = pd.DataFrame()
            if valid_dates:
                
                # æ·»åŠ æ”¶ç›˜ä»·
                if 'æ”¶ç›˜ä»·' in price_df.columns:
                    combined_df['æ”¶ç›˜ä»·'] = None
                    for idx in valid_dates:
                        combined_df.loc[idx, 'æ”¶ç›˜ä»·'] = price_df.loc[idx, 'æ”¶ç›˜ä»·']
                
                # æ·»åŠ æ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡
                for col in tech_df.columns:
                    if col != 'æ”¶ç›˜ä»·':  # é¿å…é‡å¤æ·»åŠ æ”¶ç›˜ä»·
                        combined_df[col] = None
                        for idx in tech_df.index:
                            if idx in combined_df.index:
                                combined_df.loc[idx, col] = tech_df.loc[idx, col]
                
                # è¿‡æ»¤åªæ˜¾ç¤ºstart_dateä¹‹åçš„æ•°æ®
                if start_date:
                    start_date_formatted = pd.to_datetime(start_date).strftime('%Y-%m-%d')
                    # ç¡®ä¿ç´¢å¼•æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œä¸start_date_formattedå¯æ¯”è¾ƒ
                    if combined_df.index.dtype != 'object':
                        combined_df.index = combined_df.index.astype(str)
                    combined_df = combined_df[combined_df.index >= start_date_formatted]
                
                # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½å­˜åœ¨
                expected_columns = ['æ”¶ç›˜ä»·', 'MACD', 'RSI(14)', 'æ¶¨è·Œå¹…(%)', 'æˆäº¤é¢(äº¿å…ƒ)', 'æˆäº¤é‡(ä¸‡æ‰‹)', 'æ¢æ‰‹ç‡(%)']
                
                for col in expected_columns:
                    if col not in combined_df.columns:
                        combined_df[col] = None
                
                # æ·»åŠ ETFæ”¶ç›˜ä»·åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                etf_cols = [col for col in tech_df.columns if 'ETF' in col and 'æ”¶ç›˜ä»·' in col]
                for col in etf_cols:
                    if col not in combined_df.columns:
                        combined_df[col] = None
                        for idx in tech_df.index:
                            if idx in combined_df.index:
                                combined_df.loc[idx, col] = tech_df.loc[idx, col]
                
                # æ•´ç†åˆ—çš„é¡ºåº
                ordered_columns = []
                if 'æ”¶ç›˜ä»·' in combined_df.columns:
                    ordered_columns.append('æ”¶ç›˜ä»·')
                
                # æ·»åŠ MACDä¸»æŒ‡æ ‡ï¼ˆä¸åŒ…æ‹¬Signalå’ŒHistï¼‰
                if 'MACD' in combined_df.columns:
                    ordered_columns.append('MACD')
                
                # æ·»åŠ RSI
                if 'RSI(14)' in combined_df.columns:
                    ordered_columns.append('RSI(14)')
                
                # æ·»åŠ æ¶¨è·Œå¹…ã€æˆäº¤é¢/æˆäº¤é‡å’Œæ¢æ‰‹ç‡
                if 'æ¶¨è·Œå¹…(%)' in combined_df.columns:
                    ordered_columns.append('æ¶¨è·Œå¹…(%)')
                if 'æˆäº¤é¢(äº¿å…ƒ)' in combined_df.columns:
                    ordered_columns.append('æˆäº¤é¢(äº¿å…ƒ)')
                if 'æ¢æ‰‹ç‡(%)' in combined_df.columns:
                    ordered_columns.append('æ¢æ‰‹ç‡(%)')
                if 'æˆäº¤é‡(ä¸‡æ‰‹)' in combined_df.columns:
                    ordered_columns.append('æˆäº¤é‡(ä¸‡æ‰‹)')
                
                # æ·»åŠ ETFæ”¶ç›˜ä»·
                for col in etf_cols:
                    if col in combined_df.columns:
                        ordered_columns.append(col)
                
                # æ·»åŠ ä»»ä½•å¯èƒ½é—æ¼çš„åˆ—
                for col in combined_df.columns:
                    if col not in ordered_columns:
                        ordered_columns.append(col)
                
                # æŒ‰æ–°çš„åˆ—é¡ºåºé‡æ’
                if ordered_columns:
                    combined_df = combined_df[ordered_columns]
                
                # æŒ‰æ—¥æœŸæ’åº
                combined_df = combined_df.sort_index()
                
                # è¿‡æ»¤åªæ˜¾ç¤ºstart_dateä¹‹åçš„æ•°æ®
                if start_date:
                    start_date_formatted = pd.to_datetime(start_date).strftime('%Y-%m-%d')
                    # ç¡®ä¿ç´¢å¼•æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œä¸start_date_formattedå¯æ¯”è¾ƒ
                    if combined_df.index.dtype != 'object':
                        combined_df.index = combined_df.index.astype(str)
                    combined_df = combined_df[combined_df.index >= start_date_formatted]
                else:
                    start_date_formatted = None
                
                display_df = combined_df.drop(columns=['æˆäº¤é‡(ä¸‡æ‰‹)'], errors='ignore')
                
                # æ˜¾ç¤ºç»“æœ
                md_content += "## æŠ€æœ¯æŒ‡æ ‡å’Œè¡Œä¸šETFæ”¶ç›˜ä»·\n\n"
                if combined_df.empty or display_df.empty:
                    if start_date:
                        md_content += f"*æ²¡æœ‰ä» {start_date_formatted} å¼€å§‹çš„æŠ€æœ¯æŒ‡æ ‡æ•°æ®*\n\n"
                        tech_section["note"] = f"æ²¡æœ‰ä» {start_date_formatted} å¼€å§‹çš„æŠ€æœ¯æŒ‡æ ‡æ•°æ®"
                    else:
                        md_content += "*æ²¡æœ‰æŠ€æœ¯æŒ‡æ ‡æ•°æ®*\n\n"
                        tech_section["note"] = "æ²¡æœ‰æŠ€æœ¯æŒ‡æ ‡æ•°æ®"
                else:
                    tech_section.pop("note", None)
                    # è·å–æœ€æ–°æ—¥æœŸï¼ˆä»Šå¤©çš„æ•°æ®ï¼‰
                    latest_date = combined_df.index[-1] if not combined_df.empty else None
                    
                    # åˆ›å»ºè‡ªå®šä¹‰çš„Markdownè¡¨æ ¼ï¼Œçªå‡ºæ˜¾ç¤ºä»Šå¤©çš„æ•°æ®
                    if latest_date and not display_df.empty:
                        # æ„å»ºè¡¨æ ¼å¤´
                        headers = ['æ—¥æœŸ'] + list(display_df.columns)
                        md_content += '| ' + ' | '.join(headers) + ' |\n'
                        md_content += '|' + '|'.join(['---' for _ in headers]) + '|\n'
                        
                        table_df = display_df.reset_index().rename(columns={'index': 'æ—¥æœŸ'})
                        tech_section['table'] = dataframe_to_table(table_df)
                        
                        # æ„å»ºè¡¨æ ¼è¡Œ
                        for idx in display_df.index:
                            row_data = [str(idx)]
                            for col in display_df.columns:
                                value = display_df.loc[idx, col]
                                if pd.isna(value):
                                    row_data.append('')
                                elif isinstance(value, (int, float)):
                                    row_data.append(f'{value:.2f}')
                                else:
                                    row_data.append(str(value))
                            
                            # å¦‚æœæ˜¯æœ€æ–°æ—¥æœŸï¼Œæ·»åŠ ç²—ä½“æ ‡è®°çªå‡ºæ˜¾ç¤º
                            if idx == latest_date:
                                row_data = [f'**{data}**' for data in row_data]
                                md_content += '| ' + ' | '.join(row_data) + ' | â† **ä»Šæ—¥æ•°æ®** |\n'
                            else:
                                md_content += '| ' + ' | '.join(row_data) + ' |\n'
                        
                        md_content += '\n'
                        
                        # æ·»åŠ ä»Šæ—¥æ•°æ®è¯´æ˜
                        md_content += f"ğŸ“ˆ **ä»Šæ—¥é‡ç‚¹å…³æ³¨** ({latest_date}):\n\n"
                        latest_row = combined_df.loc[latest_date]
                        latest_focus: Dict[str, Any] = {}
                        for col in combined_df.columns:
                            value = latest_row[col]
                            if pd.notna(value):
                                if isinstance(value, (int, float)):
                                    md_content += f"- **{col}**: {value:.2f}\n"
                                else:
                                    md_content += f"- **{col}**: {value}\n"
                                latest_focus[col] = _normalize_cell_value(value)
                        tech_section["latest_focus"] = latest_focus
                        md_content += '\n'
                    else:
                        # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤è¡¨æ ¼
                        md_content += display_df.to_markdown() + "\n\n"
                        tech_section['table'] = dataframe_to_table(
                            display_df.reset_index().rename(columns={'index': 'æ—¥æœŸ'})
                        )
                    
                    # æ·»åŠ æœ€æ–°MAæŒ‡æ ‡å±•ç¤º - ä½¿ç”¨é•¿å‘¨æœŸæ”¶ç›˜ä»·æ•°æ®è®¡ç®—
                    ma_source = price_df['æ”¶ç›˜ä»·'].dropna() if 'æ”¶ç›˜ä»·' in price_df.columns else pd.Series(dtype=float)
                    if ma_source.empty and not combined_df.empty and 'æ”¶ç›˜ä»·' in combined_df.columns:
                        ma_source = combined_df['æ”¶ç›˜ä»·'].dropna()

                    if not ma_source.empty:
                        latest_date_ts = ma_source.index[-1]
                        latest_date = latest_date_ts.strftime('%Y-%m-%d') if isinstance(latest_date_ts, pd.Timestamp) else latest_date_ts
                        ma_indicators = {}

                        for period in [5, 10, 20, 60]:
                            window_series = ma_source.tail(period)
                            if len(window_series) < 2:
                                continue
                            ma_value = window_series.mean()
                            if pd.notna(ma_value):
                                ma_indicators[f'MA({period})'] = ma_value

                        if ma_indicators:
                            md_content += "### æœ€æ–°ç§»åŠ¨å¹³å‡çº¿æŒ‡æ ‡\n\n"
                            md_content += f"**æ—¥æœŸ**: {latest_date}\n\n"
                            for ma_name, ma_value in ma_indicators.items():
                                md_content += f"- **{ma_name}**: {ma_value:.2f}å…ƒ\n"
                            md_content += "\n"
                            tech_section["ma_indicators"] = {
                                ma_name: round(float(ma_value), 4)
                                for ma_name, ma_value in ma_indicators.items()
                            }
            else:
                md_content += "## æŠ€æœ¯æŒ‡æ ‡å’Œè¡Œä¸šETFæ”¶ç›˜ä»·\n\n"
                md_content += "*æ²¡æœ‰æœ‰æ•ˆçš„äº¤æ˜“æ—¥æ•°æ®*\n\n"
                tech_section["note"] = "æ²¡æœ‰æœ‰æ•ˆçš„äº¤æ˜“æ—¥æ•°æ®"
                
        except Exception as e:
            md_content += f"è¯»å–æŠ€æœ¯æŒ‡æ ‡æ–‡ä»¶æ—¶å‡ºé”™: {e}\n\n"
            tech_section["note"] = f"è¯»å–æŠ€æœ¯æŒ‡æ ‡æ–‡ä»¶æ—¶å‡ºé”™: {e}"
            
            # å¦‚æœåˆå¹¶å¤±è´¥ï¼Œå°è¯•åªæ˜¾ç¤ºæŠ€æœ¯æŒ‡æ ‡
            try:
                tech_df = pd.read_csv(tech_file, index_col=0)
                tech_df.index = pd.to_datetime(tech_df.index).strftime('%Y-%m-%d')
                
                # è¿‡æ»¤åªæ˜¾ç¤ºstart_dateä¹‹åçš„æ•°æ®
                if start_date:
                    start_date_formatted = pd.to_datetime(start_date).strftime('%Y-%m-%d')
                    tech_df = tech_df[tech_df.index >= start_date_formatted]
                
                md_content += "## æŠ€æœ¯æŒ‡æ ‡å’Œè¡Œä¸šETFæ”¶ç›˜ä»·\n\n"
                if tech_df.empty:
                    md_content += f"*æ²¡æœ‰ä» {start_date_formatted} å¼€å§‹çš„æŠ€æœ¯æŒ‡æ ‡æ•°æ®*\n\n"
                    tech_section["note"] = f"æ²¡æœ‰ä» {start_date_formatted} å¼€å§‹çš„æŠ€æœ¯æŒ‡æ ‡æ•°æ®"
                else:
                    md_content += tech_df.to_markdown() + "\n\n"
                    tech_section["table"] = dataframe_to_table(
                        tech_df.reset_index().rename(columns={'index': 'æ—¥æœŸ'})
                    )
            except Exception as e2:
                md_content += f"å†æ¬¡å°è¯•è¯»å–æŠ€æœ¯æŒ‡æ ‡æ–‡ä»¶æ—¶å‡ºé”™: {e2}\n\n"
                tech_section["note"] = f"å†æ¬¡å°è¯•è¯»å–æŠ€æœ¯æŒ‡æ ‡æ–‡ä»¶æ—¶å‡ºé”™: {e2}"
    
    monthly_section: Dict[str, Any] = {}
    same_as_index = (
        (target_symbol or "").strip().upper()
        == (index_code or "").strip().upper()
    )
    # æ·»åŠ æ”¶ç›˜ä»·æ•°æ® - å°†æ¯æ—¥æ”¶ç›˜ä»·è½¬æ¢ä¸ºæ¯æœˆå‡å€¼ï¼Œå¹¶åŠ å…¥æŒ‡æ•°æ•°æ®
    if price_file.exists():
        try:
            # è¯»å–æ”¶ç›˜ä»·æ•°æ®
            price_df = pd.read_csv(price_file, index_col=0)
            price_df.index = pd.to_datetime(price_df.index)

            # è·å–æœ€æ–°çš„æ”¶ç›˜ä»·ï¼ˆç”¨äºæ”¯æ’‘çº¿/é˜»åŠ›çº¿åˆ†æï¼‰
            latest_price = price_df['æ”¶ç›˜ä»·'].iloc[-1] if not price_df.empty else None
            monthly_section["latest_price"] = float(latest_price) if latest_price is not None else None

            # è®¡ç®—è‚¡ç¥¨æ¯æœˆå‡å€¼
            # è®¡ç®—æœˆåº¦å‡å€¼ (resampleæŒ‰æœˆç»“æŸæ—¥ï¼Œè¿™é‡Œç®€å•å–mean)
            monthly_price_df = price_df.resample('ME').mean().round(3)
            monthly_price_df.index = monthly_price_df.index.strftime('%Y-%m')
            monthly_price_display = monthly_price_df.tail(12)
            monthly_section["stock_monthly"] = dataframe_to_table(
                monthly_price_display.reset_index().rename(columns={'index': 'æ—¥æœŸ'})
            )
            min_price_date = price_df.index.min() if not price_df.empty else None
            max_price_date = price_df.index.max() if not price_df.empty else None
            
            if same_as_index:
                md_content += "## æ”¶ç›˜ä»·æ•°æ®ï¼ˆæœˆåº¦å‡å€¼ï¼‰\n\n"
                md_content += monthly_price_display.to_markdown() + "\n\n"
            else:
                # è·å–æŒ‡æ•°æ•°æ®çš„æ–‡ä»¶è·¯å¾„
                clean_index_name = (index_name or "").strip() or index_code
                index_dir_name = f"{clean_index_name}_{index_code}"
                if not index_dir_name.endswith(".IDX"):
                    index_dir_name = f"{index_dir_name}.IDX"
                index_prices_dir = base_data_dir / index_dir_name / "prices"
                index_file = index_prices_dir / f"{report_date}.csv"
                if not index_file.exists():
                    csv_candidates = sorted(index_prices_dir.glob("*.csv"))
                    index_file = csv_candidates[-1] if csv_candidates else None

                rendered_monthly_table = False

                # å¦‚æœæ‰¾åˆ°äº†æŒ‡æ•°æ•°æ®æ–‡ä»¶
                if index_file and index_file.exists():
                    try:
                        # è¯»å–æŒ‡æ•°æ”¶ç›˜ä»·æ•°æ®
                        index_df = pd.read_csv(index_file, index_col=0)
                        index_df.index = pd.to_datetime(index_df.index)
                        if min_price_date is not None:
                            index_df = index_df[index_df.index >= min_price_date]
                        if max_price_date is not None:
                            index_df = index_df[index_df.index <= max_price_date]

                        index_close_col = _detect_close_column(index_df.columns)

                        if index_close_col:
                            index_df[index_close_col] = pd.to_numeric(
                                index_df[index_close_col], errors='coerce'
                            )
                            monthly_index_df = (
                                index_df[[index_close_col]].resample('ME').mean().round(3)
                            )
                            monthly_index_df.index = monthly_index_df.index.strftime('%Y-%m')
                            display_name = (index_name or "").strip() or index_code
                            monthly_index_df.rename(
                                columns={index_close_col: f'{display_name}æŒ‡æ•°'}, inplace=True
                            )

                            monthly_index_display = monthly_index_df.reindex(monthly_price_display.index)
                            merged_monthly_df = pd.concat([monthly_price_display, monthly_index_display], axis=1)
                            monthly_section["stock_monthly"] = dataframe_to_table(
                                monthly_price_display.reset_index().rename(columns={'index': 'æ—¥æœŸ'})
                            )
                            monthly_section["index_monthly"] = dataframe_to_table(
                                monthly_index_display.reset_index().rename(columns={'index': 'æ—¥æœŸ'})
                            )
                            monthly_section["index_name"] = index_name
                            monthly_section["index_code"] = index_code

                            md_content += "## æ”¶ç›˜ä»·æ•°æ®ï¼ˆæœˆåº¦å‡å€¼ï¼‰\n\n"
                            md_content += merged_monthly_df.to_markdown() + "\n\n"
                            rendered_monthly_table = True
                        else:
                            logger.warning("æŒ‡æ•°æ•°æ®ç¼ºå°‘å¯è¯†åˆ«çš„æ”¶ç›˜ä»·åˆ—: %s", index_file)
                            monthly_section.setdefault("notes", []).append("æœªæ‰¾åˆ°æŒ‡æ•°æ”¶ç›˜ä»·åˆ—")
                    except Exception as exc:
                        logger.warning("æŒ‡æ•°æœˆåº¦æ•°æ®å¤„ç†å¤±è´¥: %s", exc)
                        monthly_section.setdefault("notes", []).append(str(exc))

                if not rendered_monthly_table:
                    # å¦‚æœæ²¡æœ‰å¯ç”¨çš„æŒ‡æ•°æ•°æ®æˆ–æ¸²æŸ“å¤±è´¥ï¼Œåªæ˜¾ç¤ºè‚¡ç¥¨æ•°æ®
                    md_content += "## æ”¶ç›˜ä»·æ•°æ®ï¼ˆæœˆåº¦å‡å€¼ï¼‰\n\n"
                    md_content += monthly_price_display.to_markdown() + "\n\n"
                    if not index_file or not index_file.exists():
                        monthly_section["note"] = f"æœªæ‰¾åˆ°{index_name}({index_code})æŒ‡æ•°æ•°æ®"
                        md_content += f"*æ³¨: æœªæ‰¾åˆ°{index_name}({index_code})æŒ‡æ•°æ•°æ®*\n\n"
                    elif "notes" in monthly_section:
                        md_content += "*æ³¨: æŒ‡æ•°æœˆåº¦æ•°æ®å¤„ç†å¤±è´¥ï¼Œå·²ä»…å±•ç¤ºç›®æ ‡æ ‡çš„æ•°æ®*\n\n"
                        monthly_section["note"] = "; ".join(monthly_section.pop("notes"))

            if (same_as_index or is_etf) and 'æ”¶ç›˜ä»·' in price_df.columns:
                simple_extremes = _build_simple_price_extremes(price_df['æ”¶ç›˜ä»·'].dropna(), reference_date=price_df.index.max())
                if simple_extremes:
                    md_content += "## æœ€è¿‘ä¸‰å¹´ä»·æ ¼æå€¼\n\n"
                    extremes_df = pd.DataFrame(simple_extremes)
                    md_content += extremes_df.to_markdown(index=False) + "\n\n"
                    extreme_price_section["table"] = dataframe_to_table(extremes_df)
                    extreme_price_section["note"] = "åŸºäºæœ€è¿‘ä¸‰å¹´æ”¶ç›˜ä»·è®¡ç®—çš„æœ€é«˜/æœ€ä½è®°å½•"
        except Exception as e:
            md_content += f"è¯»å–å¹¶å¤„ç†æ”¶ç›˜ä»·æ–‡ä»¶æ—¶å‡ºé”™: {e}\n\n"
            monthly_section["note"] = f"è¯»å–å¹¶å¤„ç†æ”¶ç›˜ä»·æ–‡ä»¶æ—¶å‡ºé”™: {e}"

    json_payload["monthly_section"] = monthly_section
    if extreme_price_section:
        json_payload["extreme_price_section"] = extreme_price_section
    
    # ä¿å­˜Markdownæ–‡ä»¶
    md_file = analysis_dir_path / f"{stock_name}_{target_symbol}_è‚¡ç¥¨åˆ†ææŠ¥å‘Š_{report_date}.md"
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    json_file = analysis_dir_path / f"{stock_name}_{target_symbol}_è‚¡ç¥¨åˆ†ææŠ¥å‘Š_{report_date}.json"
    try:
        with open(json_file, 'w', encoding='utf-8') as jf:
            json.dump(json_payload, jf, ensure_ascii=False, separators=(',', ':'))
        logger.info(f"ç»“æ„åŒ–JSONå·²ä¿å­˜åˆ°: {json_file}")
    except Exception as exc:
        logger.error(f"å†™å…¥JSONæŠ¥å‘Šå¤±è´¥: {exc}")
    
    logger.info(f"åˆå¹¶æŠ¥å‘Šå·²ä¿å­˜åˆ°: {md_file}")
    
    # åˆ›å»ºé›†ä¸­å­˜æ”¾åˆ†ææŠ¥å‘Šçš„ç›®å½•
    transaction_package_dir = base_data_dir / "0_transaction_package"
    transaction_package_dir.mkdir(parents=True, exist_ok=True)
    
    # å¤åˆ¶æŠ¥å‘Šåˆ°é›†ä¸­ç›®å½•
    transaction_md_file = transaction_package_dir / f"{stock_name}_{target_symbol}_è‚¡ç¥¨åˆ†ææŠ¥å‘Š_{report_date}.md"
    try:
        shutil.copy2(md_file, transaction_md_file)
        logger.info(f"æŠ¥å‘Šå·²å¤åˆ¶åˆ°: {transaction_md_file}")
    except Exception as e:
        logger.error(f"å¤åˆ¶æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
    
    transaction_json_file = transaction_package_dir / f"{stock_name}_{target_symbol}_è‚¡ç¥¨åˆ†ææŠ¥å‘Š_{report_date}.json"
    try:
        shutil.copy2(json_file, transaction_json_file)
        logger.info(f"JSONæŠ¥å‘Šå·²å¤åˆ¶åˆ°: {transaction_json_file}")
    except Exception as e:
        logger.error(f"å¤åˆ¶JSONæŠ¥å‘Šæ—¶å‡ºé”™: {e}")
    
    return str(md_file)


def _detect_close_column(columns: Iterable[Any]) -> Optional[str]:
    """åœ¨å¤šç§å‘½åè§„åˆ™ä¸­å¯»æ‰¾æ”¶ç›˜ä»·åˆ—ï¼Œå…¼å®¹ä¸­æ–‡/è‹±æ–‡/å¸¦å‰ç¼€çš„åˆ—åã€‚"""
    prioritized: List[str] = []
    for col in columns:
        if not isinstance(col, str):
            continue
        cleaned = col.strip()
        lowered = cleaned.lower()
        if lowered.endswith('_close') or lowered == 'close':
            return col
        if 'close' in lowered:
            prioritized.append(col)
        if 'æ”¶ç›˜' in cleaned:
            return col
    return prioritized[0] if prioritized else None


def _normalize_cell_value(value: Any) -> Any:
    """å°†DataFrameå•å…ƒæ ¼è½¬æ¢ä¸ºå¯åºåˆ—åŒ–ä¸”ç´§å‡‘çš„ç±»å‹ã€‚"""
    if value is None or (isinstance(value, float) and np.isnan(value)):
        return None
    if isinstance(value, (np.integer, int)):
        return int(value)
    if isinstance(value, (np.floating, float)):
        return round(float(value), 6)
    if isinstance(value, (pd.Timestamp, datetime)):
        return value.strftime("%Y-%m-%d")
    if isinstance(value, (np.bool_, bool)):
        return bool(value)
    return value


def dataframe_to_table(df: pd.DataFrame, include_index: bool = False, index_name: str | None = None) -> Dict[str, Any]:
    """å°†DataFrameè½¬æ¢ä¸ºåˆ—+äºŒç»´æ•°ç»„æ ¼å¼ï¼Œä¾¿äºèŠ‚çœtokenã€‚"""
    if df is None or df.empty:
        return {"columns": [], "data": []}
    working = df.copy()
    if include_index:
        working = working.reset_index()
        if index_name:
            working = working.rename(columns={"index": index_name})
    columns = list(working.columns)
    data: List[List[Any]] = []
    for _, row in working.iterrows():
        data.append([_normalize_cell_value(row[col]) for col in columns])
    return {"columns": columns, "data": data}


def _build_simple_price_extremes(
    price_series: pd.Series,
    *,
    reference_date: Optional[pd.Timestamp] = None,
    years: int = 3,
    high_count: int = 3,
) -> List[Dict[str, Any]]:
    """è®¡ç®—æœ€è¿‘ä¸‰å¹´å†…æœ€é«˜çš„ä¸‰ä¸ªä»·æ ¼å’Œæœ€ä½ä»·ã€‚"""
    if price_series is None or price_series.empty:
        return []
    if reference_date is None:
        reference_date = price_series.index.max()
    if reference_date is None:
        return []
    window_start = pd.Timestamp(reference_date) - pd.Timedelta(days=365 * years)
    window = price_series[price_series.index >= window_start]
    if window.empty:
        return []

    def _single_point(series: pd.Series, *, largest: bool) -> Optional[Tuple[pd.Timestamp, float]]:
        if series is None or series.empty:
            return None
        ordered = series.sort_values(ascending=not largest)
        for ts, value in ordered.items():
            if pd.isna(value):
                continue
            return pd.Timestamp(ts), float(value)
        return None

    windows = [
        ("ä¸‰å¹´æœ€é«˜ä»·", 365 * 3),
        ("ä¸€å¹´æœ€é«˜ä»·", 365),
        ("ä¸‰ä¸ªæœˆæœ€é«˜ä»·", 90),
    ]

    highs: List[Tuple[str, pd.Timestamp, float]] = []
    for label, days in windows:
        subset = price_series[price_series.index >= (reference_date - pd.Timedelta(days=days))]
        point = _single_point(subset, largest=True)
        if point:
            highs.append((label, point[0], point[1]))

    lows_point = _single_point(window, largest=False)
    records: List[Dict[str, Any]] = []
    for label, ts, price in highs:
        records.append(
            {
                "ç±»åˆ«": label,
                "æ—¥æœŸ": ts.strftime("%Y-%m-%d"),
                "ä»·æ ¼(å…ƒ)": price,
            }
        )
    if lows_point:
        ts, price = lows_point
        records.append(
            {
                "ç±»åˆ«": "ä¸‰å¹´æœ€ä½ä»·",
                "æ—¥æœŸ": ts.strftime("%Y-%m-%d"),
                "ä»·æ ¼(å…ƒ)": price,
            }
        )
    return records


# å‘½ä»¤è¡Œæ¥å£
def main():
    parser = argparse.ArgumentParser(description='è‚¡ç¥¨ä»·æ ¼åŠ¨æ€åˆ†æå·¥å…·')
    parser.add_argument('--symbols', nargs='+', help='è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œå¸¦å¸‚åœºåç¼€ï¼Œå¦‚ 002415.SZ')
    parser.add_argument('--start-date', type=str, help='çŸ­æœŸåˆ†æå¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼ˆå¯çœç•¥å‰å¯¼0ï¼‰ï¼Œé»˜è®¤å–ç»“æŸæ—¥æœŸå‰æœ€è¿‘7ä¸ªäº¤æ˜“æ—¥')
    parser.add_argument('--end-date', type=str, help='ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼ˆå¯çœç•¥å‰å¯¼0ï¼‰ï¼Œé»˜è®¤ä¸ºä»Šå¤©ï¼ˆè‡ªåŠ¨æˆªæ–­åˆ°æœ€è¿‘äº¤æ˜“æ—¥ï¼‰')
    parser.add_argument('--long-term-start-date', type=str, help='æ”¶ç›˜ä»·æ•°æ®çš„é•¿æœŸå¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼ˆå¯çœç•¥å‰å¯¼0ï¼‰ï¼Œé»˜è®¤å–ç»“æŸæ—¥æœŸå‰ä¸€å¹´')
    parser.add_argument('--index', type=str, default='000001.IDX', help='æŒ‡æ•°ä»£ç ï¼Œé»˜è®¤ä¸ºä¸Šè¯æŒ‡æ•°')
    parser.add_argument('--similar', type=int, default=5, help='ç›¸ä¼¼è‚¡ç¥¨æ•°é‡ï¼Œé»˜è®¤ä¸º5')
    parser.add_argument('--data-dir', type=str, default='data', help='æ•°æ®å­˜å‚¨ç›®å½•ï¼Œé»˜è®¤ä¸ºdata')
    parser.add_argument('--force-refresh', action='store_true', help='å¼ºåˆ¶åˆ·æ–°æ•°æ®ï¼Œä¸ä½¿ç”¨ç¼“å­˜')
    parser.add_argument('--only-find-similar', action='store_true', help='åªæ‰§è¡Œç›¸ä¼¼è‚¡ç¥¨æŸ¥æ‰¾ï¼Œä¸è·å–æ•°æ®å’Œè®¡ç®—æŒ‡æ ‡')
    parser.add_argument('--force_refresh_financials', action='store_true', help='åªæ‰§è¡Œç›¸ä¼¼è‚¡ç¥¨æŸ¥æ‰¾ï¼Œä¸è·å–æ•°æ®å’Œè®¡ç®—æŒ‡æ ‡')
    
    args = parser.parse_args()
    
    if not isinstance(args.symbols, list):
        args.symbols = [args.symbols]

    if not args.symbols:
        # é»˜è®¤ä½¿ç”¨æµ·åº·å¨è§†ä½œä¸ºç¤ºä¾‹
        args.symbols = ["300274.SZ"]
        args.names = ["é˜³å…‰ç”µæº"]
    
    symbolsInfo = [parse_symbol(symbol) for symbol in args.symbols]
    index_symbolInfo = parse_symbol(args.index)
    
    # åˆ›å»ºé›†ä¸­å­˜æ”¾åˆ†ææŠ¥å‘Šçš„ç›®å½•
    data_dir_path = resolve_base_dir(args.data_dir)
    transaction_package_dir = data_dir_path / "0_transaction_package"
    transaction_package_dir.mkdir(parents=True, exist_ok=True)
    logger.info(f"é›†ä¸­åˆ†ææŠ¥å‘Šå°†ä¿å­˜åˆ°: {transaction_package_dir}")
    
    try:
        results = stock_price_dynamics_summarizer(
            symbolsInfo=symbolsInfo,
            index_symbolInfo=index_symbolInfo,
            start_date=args.start_date,
            end_date=args.end_date,
            long_term_start_date=args.long_term_start_date,
            top_n_similar=args.similar,
            base_dir=data_dir_path,
            force_refresh=args.force_refresh,
            only_find_similar=args.only_find_similar,
            force_refresh_financials=args.force_refresh_financials
        )
    except DataValidationError as exc:
        logger.error("æ•°æ®æ ¡éªŒå¤±è´¥: %s", exc)
        sys.exit(1)
    except Exception as exc:
        logger.exception("åˆ†æè¿‡ç¨‹ä¸­å‡ºç°æœªé¢„æœŸé”™è¯¯: %s", exc)
        sys.exit(1)
    
    # æ‰“å°ç®€è¦ç»“æœ
    for symbol, result in results.items():
        logger.info(f"\nè‚¡ç¥¨ {parse_symbol(symbol).stock_name} ({symbol}) çš„åˆ†æç»“æœæ¦‚è§ˆ:")
        
        if args.only_find_similar:
            logger.info(f"ç›¸ä¼¼è‚¡ç¥¨: {result['similar_stocks']}")
            logger.info(f"ç›¸ä¼¼è‚¡ç¥¨åç§°: {result['similar_names']}")
        else:
            logger.info(result['summary'].head())
            logger.info(f"ç›¸ä¼¼è‚¡ç¥¨: {result['similar_stocks']}")
    
    if not args.only_find_similar:
        logger.info(f"\næ‰€æœ‰åˆ†ææŠ¥å‘Šå·²å¤åˆ¶åˆ°: {transaction_package_dir}")
    else:
        logger.info("\nåªæ‰§è¡Œäº†ç›¸ä¼¼è‚¡ç¥¨æŸ¥æ‰¾ï¼Œæœªç”Ÿæˆåˆ†ææŠ¥å‘Šã€‚")


if __name__ == "__main__":
    main()


    # --symbols 002236.SZ --names å¤§åè‚¡ä»½ --start-date 20250701 --long-term-start-date 20210101 --force-refresh
    # --symbols 003816.SZ --names ä¸­å›½å¹¿æ ¸ --start-date 20250701 --long-term-start-date 20210101 --force-refresh
    # --symbols 513050.SH --names ä¸­æ¦‚äº’è”ç½‘ETF --start-date 20250701 --long-term-start-date 20220101 --force-refresh
    # --symbols 01810.HK --names å°ç±³é›†å›¢-W --start-date 20250701 --long-term-start-date 20220101 --force-refresh
    # --symbols 00700.HK --names è…¾è®¯æ§è‚¡ --start-date 20250701 --long-term-start-date 20220101 --force-refresh
    # --symbols 09988.HK --names é˜¿é‡Œå·´å·´-W --start-date 20250810 --long-term-start-date 20220101 --force-refresh
    # --symbols 03690.HK --names ç¾å›¢-W --start-date 20250701 --long-term-start-date 20220101 --force-refresh
    # --symbols PDD.US --names æ‹¼å¤šå¤š --start-date 20250701 --long-term-start-date 20220101 --force-refresh
    # --symbols 518800.SH --names é»„é‡‘ETF --start-date 20250901 --long-term-start-date 20230101 --force-refresh
    # --symbols 605117.SH --names å¾·ä¸šè‚¡ä»½ --start-date 20250701 --long-term-start-date 20220101 --force-refresh
    # --symbols 01810.HK 00700.HK 09988.HK 03690.HK 513050.SH --names å°ç±³é›†å›¢-W è…¾è®¯æ§è‚¡ é˜¿é‡Œå·´å·´-W ç¾å›¢-W ä¸­æ¦‚äº’è”ç½‘ETF --start-date 20250801 --long-term-start-date 20240301 --force-refresh
    # --symbols 002714 600598 002311 002415 000792 600989 600426 600019 601899 603993 002371 603501 002049 000895 603195 600276 300760 600900 601888 600258 300750 002594 600760 600941 601398 601100 600938 000063 601318 600030 002352 300274 002027 003816 518800 000895 000300 600460 600584 688017 002475 688981 601877 --names ç‰§åŸè‚¡ä»½ åŒ—å¤§è’ æµ·å¤§é›†å›¢ æµ·åº·å¨è§† ç›æ¹–è‚¡ä»½ å®ä¸°èƒ½æº åé²æ’å‡ å®é’¢è‚¡ä»½ ç´«é‡‘çŸ¿ä¸š æ´›é˜³é’¼ä¸š åŒ—æ–¹ååˆ› éŸ¦å°”è‚¡ä»½ ç´«å…‰å›½å¾® åŒæ±‡å‘å±• å…¬ç‰›é›†å›¢ æ’ç‘åŒ»è¯ è¿ˆç‘åŒ»ç–— é•¿æ±Ÿç”µåŠ› ä¸­å›½ä¸­å… é¦–æ—…é…’åº— å®å¾·æ—¶ä»£ æ¯”äºšè¿ª ä¸­èˆªæ²ˆé£ ä¸­å›½ç§»åŠ¨ å·¥å•†é“¶è¡Œ æ’ç«‹æ¶²å‹ ä¸­å›½æµ·æ²¹ ä¸­å…´é€šè®¯ ä¸­å›½å¹³å®‰ ä¸­ä¿¡è¯åˆ¸ é¡ºä¸°æ§è‚¡ é˜³å…‰ç”µæº åˆ†ä¼—ä¼ åª’ ä¸­å›½å¹¿æ ¸ é»„é‡‘ETF åŒæ±‡é›†å›¢ æ²ªæ·±300ETF å£«å…°å¾® é•¿ç”µç§‘æŠ€ ç»¿çš„è°æ³¢ ç«‹è®¯ç²¾å¯† ä¸­èŠ¯å›½é™… æ­£æ³°ç”µå™¨ --start-date 20250901 --long-term-start-date 20220101 --force-refresh
    # --symbols 518800.SH 09988.HK 513050.SH 601877.SH 603501.SH 002352.SZ 002714.SZ --names é»„é‡‘ETF é˜¿é‡Œå·´å·´-W ä¸­æ¦‚äº’è”ç½‘ETF æ­£æ³°ç”µå™¨ è±ªå¨é›†å›¢ é¡ºä¸°æ§è‚¡ ç‰§åŸè‚¡ä»½ --start-date 20250901 --long-term-start-date 20230101 --force-refresh
